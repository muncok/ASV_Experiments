{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnn.si_train import set_seed\n",
    "from dnn.train.model import init_speechnet\n",
    "from dnn.data.dataloader import init_sv_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secToSample(sec):\n",
    "    return int(16000 * sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "def compute_eer(pos_scores, neg_scores):\n",
    "    score_vector = np.concatenate([pos_scores, neg_scores])\n",
    "    label_vector = np.concatenate([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])\n",
    "    fpr, tpr, thres = roc_curve(label_vector, score_vector, pos_label=1)\n",
    "    eer = np.min([fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))], \n",
    "                 1-tpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]])\n",
    "    thres = thres[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "    print(\"eer:{:.2f}, thres:{:.4f}\".format(eer*100, thres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnn.parser import get_sv_parser\n",
    "options = get_sv_parser().parse_args(args=[])\n",
    "options.n_dct_filters = 40\n",
    "options.n_mels = 40\n",
    "options.timeshift_ms = 100\n",
    "options.data_folder = \"/home/muncok/DL/dataset/SV_sets\"\n",
    "options.window_size= 0.025\n",
    "options.window_stride= 0.010\n",
    "options.cache_size = 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.input_format = 'fbank'\n",
    "options.input_clip = False\n",
    "# options.input_length = secToSample(1)\n",
    "options.splice_frames = secToSample(0.1)//160+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from dnn.data.dataloader import init_embed_loaders\n",
    "options.data_folder= \"/home/muncok/DL/dataset/SV_sets/speech_commands/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SI_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/voxc/si_train/full_train/si_voxc_res15_0.1s_full_fbank.pt is loaded\n"
     ]
    }
   ],
   "source": [
    "options.input = \"models/voxc/si_train/full_train/si_voxc_res15_0.1s_full_fbank.pt\"\n",
    "options.model = \"res15\"\n",
    "model = init_speechnet(options)\n",
    "lda = None# None means not using lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/commands/equal_num_102spk_dot1.pt is loaded\n"
     ]
    }
   ],
   "source": [
    "options.input = \"models/commands/equal_num_102spk_dot1.pt\"\n",
    "options.model = \"SimpleCNN\"\n",
    "model = init_speechnet(options)\n",
    "lda = pickle.load(open(\"models/lda/equal_num_102spk_dot1_splice_lda.pkl\", \"rb\"))\n",
    "# lda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/reddots/simplecnn/si_reddots_0.2s_random_2.pt is loaded\n"
     ]
    }
   ],
   "source": [
    "options.input = \"models/reddots/simplecnn/si_reddots_0.2s_random_2.pt\"\n",
    "model = init_speechnet(options)\n",
    "# lda = pickle.load(open(\"models/lda/word_aligned_lda.pkl\", \"rb\"))\n",
    "lda = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def embeds(opt, val_dataloader, model, lda=None):\n",
    "    val_iter = iter(val_dataloader)\n",
    "    model.eval()\n",
    "    splice_dim = opt.splice_frames\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    if lda is not None:\n",
    "        print(\"LDA is loaded\")\n",
    "    for batch in (val_iter):\n",
    "        x, y = batch\n",
    "        time_dim = x.size(2)\n",
    "        split_points = range(0, time_dim-splice_dim+1, splice_dim)\n",
    "        model_outputs = []\n",
    "        for point in split_points:\n",
    "            x_in = Variable(x.narrow(2, point, splice_dim))\n",
    "            if opt.cuda:\n",
    "                x_in = x_in.cuda()\n",
    "            model_outputs.append(model.embed(x_in).cpu().data)\n",
    "        model_output = torch.stack(model_outputs, dim=0)\n",
    "        model_output = model_output.mean(0)\n",
    "        if lda is not None:\n",
    "            model_output = torch.from_numpy(lda.transform(model_output.numpy()).astype(np.float32))\n",
    "        embeddings.append(model_output)\n",
    "        labels.append(y.numpy())\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    labels = np.hstack(labels)\n",
    "    return embeddings, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_df = pd.read_pickle('trials/commands/final/equal_num_102spk_enroll.pkl')\n",
    "pos_test_df = pd.read_pickle('trials/commands/final/equal_num_102spk_pos_test.pkl')\n",
    "neg_test_df = pd.read_pickle('trials/commands/final/equal_num_102spk_neg_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down',\n",
       " 'eight',\n",
       " 'five',\n",
       " 'four',\n",
       " 'go',\n",
       " 'left',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'off',\n",
       " 'on',\n",
       " 'one',\n",
       " 'right',\n",
       " 'seven',\n",
       " 'six',\n",
       " 'stop',\n",
       " 'three',\n",
       " 'two',\n",
       " 'up',\n",
       " 'yes',\n",
       " 'zero']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = enroll_df.sent.unique().tolist()\n",
    "common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marvin',\n",
       " 'happy',\n",
       " 'sheila',\n",
       " 'cat',\n",
       " 'wow',\n",
       " 'bird',\n",
       " 'tree',\n",
       " 'bed',\n",
       " 'house',\n",
       " 'dog']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_words = list(set(pos_test_df.sent.unique().tolist()) - set(common_words))\n",
    "aux_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_spks = enroll_df.spk.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_df['file'] = enroll_df.apply(lambda x: os.path.join(x.sent, x.file), axis=1)\n",
    "pos_test_df['file'] = pos_test_df.apply(lambda x: os.path.join(x.sent, x.file), axis=1)\n",
    "neg_test_df['file'] = neg_test_df.apply(lambda x: os.path.join(x.sent, x.file), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = init_embed_loaders(options, enroll_df)\n",
    "enroll_embeddings, _ = embeds(options, val_dataloader, model, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = init_embed_loaders(options, pos_test_df)\n",
    "pos_embedding, _ = embeds(options, val_dataloader, model, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = init_embed_loaders(options, neg_test_df)\n",
    "imposter_embeddings, _ = embeds(options, val_dataloader, model, lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_average = 1\n",
    "n_enroll_uttrs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_models = dict()\n",
    "for spk in enroll_spks:\n",
    "    enroll_idx = enroll_df[enroll_df.spk == spk].index\n",
    "#     enroll_idx = enroll_df[(enroll_df.spk == spk)].iloc[[i for i in range(0, 40, 1)]].index\n",
    "    spk_models[spk] = enroll_embeddings[enroll_df.index.get_indexer_for(enroll_idx),].mean(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_words_in_uttr: 1\n",
      "eer:21.10, thres:0.9954\n"
     ]
    }
   ],
   "source": [
    "for n_words_in_uttr in range(1,2):\n",
    "    if n_words_in_uttr > 1:\n",
    "        pos_concat_df = pd.read_pickle('trials/commands/final/equal_num_102spk_concat{}_pos.pkl'.format(n_words_in_uttr))\n",
    "        neg_concat_df = pd.read_pickle('trials/commands/final/equal_num_102spk_concat{}_neg.pkl'.format(n_words_in_uttr))\n",
    "    else:\n",
    "        pos_concat_df = pd.read_pickle('trials/commands/final/equal_num_102spk_pos_test.pkl')\n",
    "        neg_concat_df = pd.read_pickle('trials/commands/final/equal_num_102spk_neg_test.pkl')\n",
    "\n",
    "    pos_concat_dict = dict()\n",
    "\n",
    "    for spk in enroll_spks:\n",
    "        pos_concat_dict[spk] = pos_concat_df[pos_concat_df.spk == spk]\n",
    "\n",
    "    pos_embs_per_uttr = dict()\n",
    "\n",
    "    for spk in enroll_spks:\n",
    "        pos_concat_spk_df = pos_concat_dict[spk]\n",
    "  \n",
    "        pos_embs_per_uttr[spk] = []\n",
    "\n",
    "        for uniqID, _ in pos_concat_spk_df.iterrows():\n",
    "            idxs = uniqID.split('_')\n",
    "            pos_test_idx = pos_test_df[pos_test_df.index.isin(idxs)].index\n",
    "            pos_embs_per_uttr[spk].append(pos_embedding[pos_test_df.index.get_indexer_for(pos_test_idx),])\n",
    "\n",
    "#     Neg embedding preloading\n",
    "\n",
    "    neg_embs_per_uttr = []\n",
    "\n",
    "    for uniqID, _ in neg_concat_df.iterrows():\n",
    "        idxs = uniqID.split('_')\n",
    "        neg_test_idx = neg_test_df[neg_test_df.index.isin(idxs)].index\n",
    "        neg_embs_per_uttr.append(imposter_embeddings[neg_test_df.index.get_indexer_for(neg_test_idx),])\n",
    "\n",
    "    neg_embs_per_uttr_emb = torch.stack([torch.mean(x, dim=0) for x in neg_embs_per_uttr])\n",
    "\n",
    "    neg_embs_per_uttr_scr = torch.stack(neg_embs_per_uttr)\n",
    "\n",
    "    # average embedding --> scoring\n",
    "\n",
    "    pos_scores = dict()\n",
    "    neg_scores = dict()\n",
    "\n",
    "    for spk in enroll_spks:\n",
    "\n",
    "        pos_embs_per_uttr_emb = torch.stack([torch.mean(x, dim=0) for x in pos_embs_per_uttr[spk]])\n",
    "\n",
    "        pos_scores[spk] = F.cosine_similarity(pos_embs_per_uttr_emb, spk_models[spk])\n",
    "\n",
    "        neg_scores[spk] = F.cosine_similarity(neg_embs_per_uttr_emb, spk_models[spk])\n",
    "\n",
    "    print(\"n_words_in_uttr: {}\".format(n_words_in_uttr))\n",
    "    uni_pos_scores = np.concatenate([v for v in pos_scores.values()])\n",
    "    uni_neg_scores = np.concatenate([v for v in neg_scores.values()])\n",
    "    compute_eer(uni_pos_scores, uni_neg_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
