{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDA (Deep Discriminant Analysis)\n",
    "\n",
    "기존의 DDA 학습은 classification base였다면 이번에는 Metric learning을 이용하여 DDA를 수행해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "# datasets\n",
    "# voxc1_fbank_xvector\n",
    "# gcommand_fbank_xvector\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\",\n",
    "                              input_frames=100, splice_frames=[50, 100], stride_frames=1, input_format='fbank',\n",
    "                              cuda=True,\n",
    "                              lrs=[0.01, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False,\n",
    "                              batch_size=128,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                              n_epochs=50\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class embedDataset(data.Dataset):\n",
    "    def __init__(self, embeds, labels):\n",
    "        super().__init__()\n",
    "        self.embeds = embeds\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.embeds[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.embeds.shape[0]\n",
    "        \n",
    "\n",
    "def embedToDataset(embed_dict):\n",
    "    embeds = np.array([v for v in embed_dict.values()])\n",
    "    spks = [k.split(\"/\")[0] for k in embed_dict.keys()]\n",
    "    spk2label = pd.Series(spks).unique().tolist()\n",
    "    labels = np.array([spk2label.index(spk) for spk in spks])\n",
    "    \n",
    "    dataset = embedDataset(embeds, labels)\n",
    "    \n",
    "    return dataset, embeds.shape[1], len(spk2label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = pd.read_pickle(\"../dataset/dataframes/voxc1/voxc_trial.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_embeds = pickle.load(open(\"../best_models/voxc1/ResNet34_v4_softmax/voxc_train_dvectors.pkl\", \"rb\"))\n",
    "sv_embeds = pickle.load(open(\"../best_models/voxc1/ResNet34_v4_softmax/voxc_test_dvectors.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_dataset, embed_dim, n_labels = embedToDataset(si_embeds)\n",
    "sv_dataset, _, _ = embedToDataset(sv_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def index_dataset(dataset):\n",
    "    return {c : [example_idx for example_idx, (_, class_label_ind) in \\\n",
    "                 enumerate(zip(dataset.embeds, dataset.labels)) if class_label_ind == c] for c in set(dataset.labels)}\n",
    "\n",
    "def sample_from_class(images_by_class, class_label_ind):\n",
    "    return images_by_class[class_label_ind][random.randrange(len(images_by_class[class_label_ind]))]\n",
    "\n",
    "def simple(batch_size, dataset, prob_other = 0.5):\n",
    "    '''lazy sampling, not like in lifted_struct. they add to the pool all postiive combinations, then compute the average number of positive pairs per image, then sample for every image the same number of negative pairs'''\n",
    "    images_by_class = index_dataset(dataset)\n",
    "    for batch_idx in range(int(math.ceil(len(dataset) * 1.0 / batch_size))):\n",
    "        example_indices = []\n",
    "        for i in range(0, batch_size, 2):\n",
    "            perm = random.sample(images_by_class.keys(), 2)\n",
    "            example_indices += [sample_from_class(images_by_class, perm[0]), sample_from_class(images_by_class, perm[0 if i == 0 or random.random() > prob_other else 1])]\n",
    "        yield example_indices[:batch_size]\n",
    "\n",
    "def triplet(batch_size, dataset, class2img=None):\n",
    "    if class2img is not None:\n",
    "        images_by_class = class2img\n",
    "    else:\n",
    "        images_by_class = index_dataset(dataset)\n",
    "    for batch_idx in range(int(math.ceil(len(dataset) * 1.0 / batch_size))):\n",
    "        example_indices = []\n",
    "        for i in range(0, batch_size, 3):\n",
    "            perm = random.sample(images_by_class.keys(), 2)\n",
    "            example_indices += [sample_from_class(images_by_class, perm[0]), sample_from_class(images_by_class, perm[0]), sample_from_class(images_by_class, perm[1])]\n",
    "        yield example_indices[:batch_size]\n",
    "\n",
    "def npairs(batch_size, dataset, K = 4):\n",
    "    images_by_class = index_dataset(dataset)\n",
    "    for batch_idx in range(int(math.ceil(len(dataset) * 1.0 / batch_size))):\n",
    "        example_indices = [sample_from_class(images_by_class, class_label_ind) for k in range(int(math.ceil(batch_size * 1.0 / K))) for class_label_ind in [random.choice(images_by_class.keys())] for i in range(K)]\n",
    "        yield example_indices[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_sampler = lambda batch, dataset, sampler, **kwargs: \\\n",
    "type('', (torch.utils.data.sampler.Sampler,), \n",
    "     dict(__len__ = dataset.__len__, __iter__ = \\\n",
    "          lambda _: itertools.chain.from_iterable(sampler(batch, dataset, **kwargs))))(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class2idx = index_dataset(si_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# si_loader = DataLoader(si_dataset, batch_size=128, num_workers=4, shuffle=True)\n",
    "n_pairs_per_batch = 42\n",
    "si_loader = torch.utils.data.DataLoader(si_dataset, \n",
    "                                       sampler = adapt_sampler(n_pairs_per_batch*3, si_dataset, triplet, class2img=train_class2idx), \n",
    "                                       num_workers = 4, batch_size = n_pairs_per_batch*3, \n",
    "                                       drop_last = True, pin_memory = True)\n",
    "sv_loader = DataLoader(sv_dataset, batch_size=128, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class dda_model(nn.Module):\n",
    "    def __init__(self, in_dims, n_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(in_dims, 2*in_dims),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(2*in_dims, 2*in_dims),\n",
    "            nn.PReLU()\n",
    "        )    \n",
    "        self.hidden_batch = nn.BatchNorm1d(2*in_dims)\n",
    "    \n",
    "        self.embedding_layer = nn.Linear(2*in_dims, n_labels)\n",
    "        \n",
    "    def embed(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_batch(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):           \n",
    "        x = self.embed(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dda_model(embed_dim, n_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dda_model(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (hidden_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (hidden_batch): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (embedding_layer): Linear(in_features=256, out_features=1211, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.train.train_utils import set_seed, find_optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "\n",
    "_, optimizer = find_optimizer(config, model)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "step_scheduler = MultiStepLR(optimizer, [30], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def triplet_train(model, loader):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(loader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeds = model(X)\n",
    "        embeds = embeds / embeds.norm(dim=1,keepdim=True)\n",
    "        anchor = embeds[0:63*2:3]\n",
    "        pos_egs = embeds[1:63*2:3]\n",
    "        neg_egs = embeds[2:63*2:3]\n",
    "        loss = criterion(anchor, pos_egs, neg_egs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        total += y.size(0)\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "                  .format(batch_idx+1, len(loader), loss_sum / total))\n",
    "    return loss_sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def embeds_utterance(config, val_dataloader, model):\n",
    "    val_iter = iter(val_dataloader)\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            X, y = batch\n",
    "            if not config['no_cuda']:\n",
    "                X = X.cuda()\n",
    "                \n",
    "            model_output = model.embed(X).cpu().data\n",
    "            embeddings.append(model_output)\n",
    "            labels.append(y.numpy())\n",
    "        embeddings = torch.cat(embeddings)\n",
    "        labels = np.hstack(labels)\n",
    "    return embeddings, labels \n",
    "\n",
    "def sv_test(config, sv_loader, model, trial):\n",
    "        embeddings, _ = embeds_utterance(config, sv_loader, model)\n",
    "        sim_matrix = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)\n",
    "        cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]\n",
    "        score_vector = sim_matrix[cord].numpy()\n",
    "        label_vector = np.array(trial.label)\n",
    "        fpr, tpr, thres = roc_curve(\n",
    "                label_vector, score_vector, pos_label=1)\n",
    "        eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "\n",
    "        return eer, label_vector, score_vector\n",
    "    \n",
    "def sv_euc_test(config, sv_loader, model, trial):\n",
    "        embeddings, _ = embeds_utterance(config, sv_loader, model)\n",
    "#         embeddings /= embeddings.norm(dim=1,keepdim=True)\n",
    "        a = embeddings.unsqueeze(1)\n",
    "        b = embeddings.unsqueeze(0)\n",
    "        dist = a - b\n",
    "        sim_matrix = -dist.norm(dim=2)\n",
    "        cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]\n",
    "        score_vector = sim_matrix[cord].numpy()\n",
    "        label_vector = np.array(trial.label)\n",
    "        fpr, tpr, thres = roc_curve(\n",
    "                label_vector, score_vector, pos_label=1)\n",
    "        eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "\n",
    "        return eer, label_vector, score_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.002708\n",
      "Batch 200/1174\t Loss 0.002422\n",
      "Batch 300/1174\t Loss 0.002304\n",
      "Batch 400/1174\t Loss 0.002221\n",
      "Batch 500/1174\t Loss 0.002174\n",
      "Batch 600/1174\t Loss 0.002142\n",
      "Batch 700/1174\t Loss 0.002104\n",
      "Batch 800/1174\t Loss 0.002082\n",
      "Batch 900/1174\t Loss 0.002075\n",
      "Batch 1000/1174\t Loss 0.002055\n",
      "Batch 1100/1174\t Loss 0.002040\n",
      "epoch #0, train loss: 300.3295560851693\n",
      "epoch #0, sv eer: 0.10057501863486316\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001867\n",
      "Batch 200/1174\t Loss 0.001866\n",
      "Batch 300/1174\t Loss 0.001846\n",
      "Batch 400/1174\t Loss 0.001844\n",
      "Batch 500/1174\t Loss 0.001852\n",
      "Batch 600/1174\t Loss 0.001855\n",
      "Batch 700/1174\t Loss 0.001847\n",
      "Batch 800/1174\t Loss 0.001846\n",
      "Batch 900/1174\t Loss 0.001838\n",
      "Batch 1000/1174\t Loss 0.001841\n",
      "Batch 1100/1174\t Loss 0.001839\n",
      "epoch #1, train loss: 271.75236892700195\n",
      "epoch #1, sv eer: 0.09791289532531147\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001844\n",
      "Batch 200/1174\t Loss 0.001835\n",
      "Batch 300/1174\t Loss 0.001822\n",
      "Batch 400/1174\t Loss 0.001813\n",
      "Batch 500/1174\t Loss 0.001813\n",
      "Batch 600/1174\t Loss 0.001803\n",
      "Batch 700/1174\t Loss 0.001796\n",
      "Batch 800/1174\t Loss 0.001784\n",
      "Batch 900/1174\t Loss 0.001782\n",
      "Batch 1000/1174\t Loss 0.001780\n",
      "Batch 1100/1174\t Loss 0.001778\n",
      "epoch #2, train loss: 263.59994750097394\n",
      "epoch #2, sv eer: 0.09668831860291768\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001771\n",
      "Batch 200/1174\t Loss 0.001769\n",
      "Batch 300/1174\t Loss 0.001744\n",
      "Batch 400/1174\t Loss 0.001741\n",
      "Batch 500/1174\t Loss 0.001746\n",
      "Batch 600/1174\t Loss 0.001749\n",
      "Batch 700/1174\t Loss 0.001742\n",
      "Batch 800/1174\t Loss 0.001735\n",
      "Batch 900/1174\t Loss 0.001732\n",
      "Batch 1000/1174\t Loss 0.001733\n",
      "Batch 1100/1174\t Loss 0.001726\n",
      "epoch #3, train loss: 254.98056297004223\n",
      "epoch #3, sv eer: 0.09727398573101906\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001698\n",
      "Batch 200/1174\t Loss 0.001688\n",
      "Batch 300/1174\t Loss 0.001679\n",
      "Batch 400/1174\t Loss 0.001687\n",
      "Batch 500/1174\t Loss 0.001695\n",
      "Batch 600/1174\t Loss 0.001695\n",
      "Batch 700/1174\t Loss 0.001683\n",
      "Batch 800/1174\t Loss 0.001692\n",
      "Batch 900/1174\t Loss 0.001685\n",
      "Batch 1000/1174\t Loss 0.001687\n",
      "Batch 1100/1174\t Loss 0.001686\n",
      "epoch #4, train loss: 248.45316333323717\n",
      "epoch #4, sv eer: 0.103450111809179\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001679\n",
      "Batch 200/1174\t Loss 0.001701\n",
      "Batch 300/1174\t Loss 0.001698\n",
      "Batch 400/1174\t Loss 0.001706\n",
      "Batch 500/1174\t Loss 0.001696\n",
      "Batch 600/1174\t Loss 0.001696\n",
      "Batch 700/1174\t Loss 0.001682\n",
      "Batch 800/1174\t Loss 0.001686\n",
      "Batch 900/1174\t Loss 0.001684\n",
      "Batch 1000/1174\t Loss 0.001687\n",
      "Batch 1100/1174\t Loss 0.001691\n",
      "epoch #5, train loss: 249.7563144788146\n",
      "epoch #5, sv eer: 0.09913747204770525\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001642\n",
      "Batch 200/1174\t Loss 0.001633\n",
      "Batch 300/1174\t Loss 0.001654\n",
      "Batch 400/1174\t Loss 0.001648\n",
      "Batch 500/1174\t Loss 0.001641\n",
      "Batch 600/1174\t Loss 0.001639\n",
      "Batch 700/1174\t Loss 0.001635\n",
      "Batch 800/1174\t Loss 0.001638\n",
      "Batch 900/1174\t Loss 0.001638\n",
      "Batch 1000/1174\t Loss 0.001636\n",
      "Batch 1100/1174\t Loss 0.001634\n",
      "epoch #6, train loss: 242.4463633671403\n",
      "epoch #6, sv eer: 0.10297092961345969\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001694\n",
      "Batch 200/1174\t Loss 0.001663\n",
      "Batch 300/1174\t Loss 0.001653\n",
      "Batch 400/1174\t Loss 0.001653\n",
      "Batch 500/1174\t Loss 0.001647\n",
      "Batch 600/1174\t Loss 0.001645\n",
      "Batch 700/1174\t Loss 0.001645\n",
      "Batch 800/1174\t Loss 0.001647\n",
      "Batch 900/1174\t Loss 0.001650\n",
      "Batch 1000/1174\t Loss 0.001648\n",
      "Batch 1100/1174\t Loss 0.001644\n",
      "epoch #7, train loss: 242.79963269084692\n",
      "epoch #7, sv eer: 0.10132041316153764\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001680\n",
      "Batch 200/1174\t Loss 0.001673\n",
      "Batch 300/1174\t Loss 0.001660\n",
      "Batch 400/1174\t Loss 0.001660\n",
      "Batch 500/1174\t Loss 0.001649\n",
      "Batch 600/1174\t Loss 0.001637\n",
      "Batch 700/1174\t Loss 0.001632\n",
      "Batch 800/1174\t Loss 0.001631\n",
      "Batch 900/1174\t Loss 0.001629\n",
      "Batch 1000/1174\t Loss 0.001626\n",
      "Batch 1100/1174\t Loss 0.001628\n",
      "epoch #8, train loss: 240.55938009172678\n",
      "epoch #8, sv eer: 0.10036204877009904\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001624\n",
      "Batch 200/1174\t Loss 0.001602\n",
      "Batch 300/1174\t Loss 0.001591\n",
      "Batch 400/1174\t Loss 0.001597\n",
      "Batch 500/1174\t Loss 0.001594\n",
      "Batch 600/1174\t Loss 0.001601\n",
      "Batch 700/1174\t Loss 0.001603\n",
      "Batch 800/1174\t Loss 0.001607\n",
      "Batch 900/1174\t Loss 0.001604\n",
      "Batch 1000/1174\t Loss 0.001604\n",
      "Batch 1100/1174\t Loss 0.001601\n",
      "epoch #9, train loss: 236.59662082791328\n",
      "epoch #9, sv eer: 0.1037695666063252\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001648\n",
      "Batch 200/1174\t Loss 0.001621\n",
      "Batch 300/1174\t Loss 0.001599\n",
      "Batch 400/1174\t Loss 0.001587\n",
      "Batch 500/1174\t Loss 0.001570\n",
      "Batch 600/1174\t Loss 0.001555\n",
      "Batch 700/1174\t Loss 0.001552\n",
      "Batch 800/1174\t Loss 0.001552\n",
      "Batch 900/1174\t Loss 0.001551\n",
      "Batch 1000/1174\t Loss 0.001559\n",
      "Batch 1100/1174\t Loss 0.001562\n",
      "epoch #10, train loss: 231.13835937529802\n",
      "epoch #10, sv eer: 0.09972313917580662\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001531\n",
      "Batch 200/1174\t Loss 0.001532\n",
      "Batch 300/1174\t Loss 0.001548\n",
      "Batch 400/1174\t Loss 0.001553\n",
      "Batch 500/1174\t Loss 0.001553\n",
      "Batch 600/1174\t Loss 0.001561\n",
      "Batch 700/1174\t Loss 0.001554\n",
      "Batch 800/1174\t Loss 0.001554\n",
      "Batch 900/1174\t Loss 0.001552\n",
      "Batch 1000/1174\t Loss 0.001551\n",
      "Batch 1100/1174\t Loss 0.001548\n",
      "epoch #11, train loss: 228.99824734777212\n",
      "epoch #11, sv eer: 0.10286444468107762\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001571\n",
      "Batch 200/1174\t Loss 0.001564\n",
      "Batch 300/1174\t Loss 0.001565\n",
      "Batch 400/1174\t Loss 0.001577\n",
      "Batch 500/1174\t Loss 0.001575\n",
      "Batch 600/1174\t Loss 0.001580\n",
      "Batch 700/1174\t Loss 0.001573\n",
      "Batch 800/1174\t Loss 0.001565\n",
      "Batch 900/1174\t Loss 0.001560\n",
      "Batch 1000/1174\t Loss 0.001558\n",
      "Batch 1100/1174\t Loss 0.001559\n",
      "epoch #12, train loss: 231.11585007607937\n",
      "epoch #12, sv eer: 0.10313065701203279\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001583\n",
      "Batch 200/1174\t Loss 0.001577\n",
      "Batch 300/1174\t Loss 0.001572\n",
      "Batch 400/1174\t Loss 0.001571\n",
      "Batch 500/1174\t Loss 0.001577\n",
      "Batch 600/1174\t Loss 0.001575\n",
      "Batch 700/1174\t Loss 0.001569\n",
      "Batch 800/1174\t Loss 0.001559\n",
      "Batch 900/1174\t Loss 0.001552\n",
      "Batch 1000/1174\t Loss 0.001555\n",
      "Batch 1100/1174\t Loss 0.001553\n",
      "epoch #13, train loss: 230.13104327768087\n",
      "epoch #13, sv eer: 0.1067511447130231\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001604\n",
      "Batch 200/1174\t Loss 0.001578\n",
      "Batch 300/1174\t Loss 0.001556\n",
      "Batch 400/1174\t Loss 0.001550\n",
      "Batch 500/1174\t Loss 0.001542\n",
      "Batch 600/1174\t Loss 0.001539\n",
      "Batch 700/1174\t Loss 0.001537\n",
      "Batch 800/1174\t Loss 0.001534\n",
      "Batch 900/1174\t Loss 0.001532\n",
      "Batch 1000/1174\t Loss 0.001537\n",
      "Batch 1100/1174\t Loss 0.001543\n",
      "epoch #14, train loss: 228.00356254726648\n",
      "epoch #14, sv eer: 0.10637844744968587\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001435\n",
      "Batch 200/1174\t Loss 0.001482\n",
      "Batch 300/1174\t Loss 0.001487\n",
      "Batch 400/1174\t Loss 0.001493\n",
      "Batch 500/1174\t Loss 0.001497\n",
      "Batch 600/1174\t Loss 0.001496\n",
      "Batch 700/1174\t Loss 0.001496\n",
      "Batch 800/1174\t Loss 0.001505\n",
      "Batch 900/1174\t Loss 0.001504\n",
      "Batch 1000/1174\t Loss 0.001502\n",
      "Batch 1100/1174\t Loss 0.001506\n",
      "epoch #15, train loss: 223.09770169109106\n",
      "epoch #15, sv eer: 0.10371632414013417\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001611\n",
      "Batch 200/1174\t Loss 0.001593\n",
      "Batch 300/1174\t Loss 0.001569\n",
      "Batch 400/1174\t Loss 0.001548\n",
      "Batch 500/1174\t Loss 0.001538\n",
      "Batch 600/1174\t Loss 0.001541\n",
      "Batch 700/1174\t Loss 0.001543\n",
      "Batch 800/1174\t Loss 0.001542\n",
      "Batch 900/1174\t Loss 0.001534\n",
      "Batch 1000/1174\t Loss 0.001529\n",
      "Batch 1100/1174\t Loss 0.001532\n",
      "epoch #16, train loss: 226.99610174447298\n",
      "epoch #16, sv eer: 0.10494090086252796\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001524\n",
      "Batch 200/1174\t Loss 0.001515\n",
      "Batch 300/1174\t Loss 0.001516\n",
      "Batch 400/1174\t Loss 0.001497\n",
      "Batch 500/1174\t Loss 0.001498\n",
      "Batch 600/1174\t Loss 0.001502\n",
      "Batch 700/1174\t Loss 0.001509\n",
      "Batch 800/1174\t Loss 0.001513\n",
      "Batch 900/1174\t Loss 0.001518\n",
      "Batch 1000/1174\t Loss 0.001514\n",
      "Batch 1100/1174\t Loss 0.001512\n",
      "epoch #17, train loss: 224.07564057409763\n",
      "epoch #17, sv eer: 0.10233202001916729\n",
      "------------------------------\n",
      "curr_lr: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/1174\t Loss 0.001525\n",
      "Batch 200/1174\t Loss 0.001518\n",
      "Batch 300/1174\t Loss 0.001525\n",
      "Batch 400/1174\t Loss 0.001529\n",
      "Batch 500/1174\t Loss 0.001525\n",
      "Batch 600/1174\t Loss 0.001521\n",
      "Batch 700/1174\t Loss 0.001522\n",
      "Batch 800/1174\t Loss 0.001520\n",
      "Batch 900/1174\t Loss 0.001513\n",
      "Batch 1000/1174\t Loss 0.001511\n",
      "Batch 1100/1174\t Loss 0.001511\n",
      "epoch #18, train loss: 224.02292791381478\n",
      "epoch #18, sv eer: 0.10765626663827069\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001542\n",
      "Batch 200/1174\t Loss 0.001549\n",
      "Batch 300/1174\t Loss 0.001545\n",
      "Batch 400/1174\t Loss 0.001536\n",
      "Batch 500/1174\t Loss 0.001533\n",
      "Batch 600/1174\t Loss 0.001523\n",
      "Batch 700/1174\t Loss 0.001520\n",
      "Batch 800/1174\t Loss 0.001519\n",
      "Batch 900/1174\t Loss 0.001521\n",
      "Batch 1000/1174\t Loss 0.001520\n",
      "Batch 1100/1174\t Loss 0.001517\n",
      "epoch #19, train loss: 223.69455706328154\n",
      "epoch #19, sv eer: 0.1067511447130231\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001538\n",
      "Batch 200/1174\t Loss 0.001509\n",
      "Batch 300/1174\t Loss 0.001511\n",
      "Batch 400/1174\t Loss 0.001516\n",
      "Batch 500/1174\t Loss 0.001503\n",
      "Batch 600/1174\t Loss 0.001517\n",
      "Batch 700/1174\t Loss 0.001503\n",
      "Batch 800/1174\t Loss 0.001501\n",
      "Batch 900/1174\t Loss 0.001499\n",
      "Batch 1000/1174\t Loss 0.001497\n",
      "Batch 1100/1174\t Loss 0.001495\n",
      "epoch #20, train loss: 221.35431026294827\n",
      "epoch #20, sv eer: 0.10621872005111277\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001470\n",
      "Batch 200/1174\t Loss 0.001481\n",
      "Batch 300/1174\t Loss 0.001481\n",
      "Batch 400/1174\t Loss 0.001475\n",
      "Batch 500/1174\t Loss 0.001462\n",
      "Batch 600/1174\t Loss 0.001462\n",
      "Batch 700/1174\t Loss 0.001463\n",
      "Batch 800/1174\t Loss 0.001472\n",
      "Batch 900/1174\t Loss 0.001473\n",
      "Batch 1000/1174\t Loss 0.001477\n",
      "Batch 1100/1174\t Loss 0.001478\n",
      "epoch #21, train loss: 218.75888340175152\n",
      "epoch #21, sv eer: 0.10605899265253967\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001536\n",
      "Batch 200/1174\t Loss 0.001518\n",
      "Batch 300/1174\t Loss 0.001518\n",
      "Batch 400/1174\t Loss 0.001516\n",
      "Batch 500/1174\t Loss 0.001503\n",
      "Batch 600/1174\t Loss 0.001496\n",
      "Batch 700/1174\t Loss 0.001491\n",
      "Batch 800/1174\t Loss 0.001495\n",
      "Batch 900/1174\t Loss 0.001493\n",
      "Batch 1000/1174\t Loss 0.001489\n",
      "Batch 1100/1174\t Loss 0.001491\n",
      "epoch #22, train loss: 220.32603619992733\n",
      "epoch #22, sv eer: 0.10739005430731552\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001437\n",
      "Batch 200/1174\t Loss 0.001435\n",
      "Batch 300/1174\t Loss 0.001424\n",
      "Batch 400/1174\t Loss 0.001435\n",
      "Batch 500/1174\t Loss 0.001438\n",
      "Batch 600/1174\t Loss 0.001449\n",
      "Batch 700/1174\t Loss 0.001446\n",
      "Batch 800/1174\t Loss 0.001451\n",
      "Batch 900/1174\t Loss 0.001456\n",
      "Batch 1000/1174\t Loss 0.001458\n",
      "Batch 1100/1174\t Loss 0.001461\n",
      "epoch #23, train loss: 216.26057899743319\n",
      "epoch #23, sv eer: 0.10898732829304654\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001525\n",
      "Batch 200/1174\t Loss 0.001470\n",
      "Batch 300/1174\t Loss 0.001446\n",
      "Batch 400/1174\t Loss 0.001444\n",
      "Batch 500/1174\t Loss 0.001446\n",
      "Batch 600/1174\t Loss 0.001447\n",
      "Batch 700/1174\t Loss 0.001447\n",
      "Batch 800/1174\t Loss 0.001446\n",
      "Batch 900/1174\t Loss 0.001450\n",
      "Batch 1000/1174\t Loss 0.001452\n",
      "Batch 1100/1174\t Loss 0.001452\n",
      "epoch #24, train loss: 215.31753532588482\n",
      "epoch #24, sv eer: 0.10946651048876584\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001459\n",
      "Batch 200/1174\t Loss 0.001457\n",
      "Batch 300/1174\t Loss 0.001462\n",
      "Batch 400/1174\t Loss 0.001463\n",
      "Batch 500/1174\t Loss 0.001472\n",
      "Batch 600/1174\t Loss 0.001471\n",
      "Batch 700/1174\t Loss 0.001478\n",
      "Batch 800/1174\t Loss 0.001469\n",
      "Batch 900/1174\t Loss 0.001467\n",
      "Batch 1000/1174\t Loss 0.001465\n",
      "Batch 1100/1174\t Loss 0.001468\n",
      "epoch #25, train loss: 217.16854270547628\n",
      "epoch #25, sv eer: 0.11292727079118305\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001519\n",
      "Batch 200/1174\t Loss 0.001498\n",
      "Batch 300/1174\t Loss 0.001498\n",
      "Batch 400/1174\t Loss 0.001475\n",
      "Batch 500/1174\t Loss 0.001473\n",
      "Batch 600/1174\t Loss 0.001464\n",
      "Batch 700/1174\t Loss 0.001465\n",
      "Batch 800/1174\t Loss 0.001464\n",
      "Batch 900/1174\t Loss 0.001459\n",
      "Batch 1000/1174\t Loss 0.001454\n",
      "Batch 1100/1174\t Loss 0.001458\n",
      "epoch #26, train loss: 215.55195012316108\n",
      "epoch #26, sv eer: 0.11026514748163135\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001463\n",
      "Batch 200/1174\t Loss 0.001440\n",
      "Batch 300/1174\t Loss 0.001455\n",
      "Batch 400/1174\t Loss 0.001456\n",
      "Batch 500/1174\t Loss 0.001450\n",
      "Batch 600/1174\t Loss 0.001456\n",
      "Batch 700/1174\t Loss 0.001450\n",
      "Batch 800/1174\t Loss 0.001461\n",
      "Batch 900/1174\t Loss 0.001464\n",
      "Batch 1000/1174\t Loss 0.001465\n",
      "Batch 1100/1174\t Loss 0.001464\n",
      "epoch #27, train loss: 216.8433621674776\n",
      "epoch #27, sv eer: 0.10850814609732723\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001502\n",
      "Batch 200/1174\t Loss 0.001478\n",
      "Batch 300/1174\t Loss 0.001462\n",
      "Batch 400/1174\t Loss 0.001470\n",
      "Batch 500/1174\t Loss 0.001469\n",
      "Batch 600/1174\t Loss 0.001457\n",
      "Batch 700/1174\t Loss 0.001461\n",
      "Batch 800/1174\t Loss 0.001461\n",
      "Batch 900/1174\t Loss 0.001457\n",
      "Batch 1000/1174\t Loss 0.001462\n",
      "Batch 1100/1174\t Loss 0.001460\n",
      "epoch #28, train loss: 216.58551132306457\n",
      "epoch #28, sv eer: 0.10286444468107762\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001421\n",
      "Batch 200/1174\t Loss 0.001439\n",
      "Batch 300/1174\t Loss 0.001445\n",
      "Batch 400/1174\t Loss 0.001446\n",
      "Batch 500/1174\t Loss 0.001460\n",
      "Batch 600/1174\t Loss 0.001459\n",
      "Batch 700/1174\t Loss 0.001455\n",
      "Batch 800/1174\t Loss 0.001453\n",
      "Batch 900/1174\t Loss 0.001448\n",
      "Batch 1000/1174\t Loss 0.001440\n",
      "Batch 1100/1174\t Loss 0.001438\n",
      "epoch #29, train loss: 213.0684996470809\n",
      "epoch #29, sv eer: 0.10510062826110106\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001468\n",
      "Batch 200/1174\t Loss 0.001437\n",
      "Batch 300/1174\t Loss 0.001443\n",
      "Batch 400/1174\t Loss 0.001435\n",
      "Batch 500/1174\t Loss 0.001440\n",
      "Batch 600/1174\t Loss 0.001434\n",
      "Batch 700/1174\t Loss 0.001447\n",
      "Batch 800/1174\t Loss 0.001450\n",
      "Batch 900/1174\t Loss 0.001443\n",
      "Batch 1000/1174\t Loss 0.001444\n",
      "Batch 1100/1174\t Loss 0.001447\n",
      "epoch #30, train loss: 214.15101578831673\n",
      "epoch #30, sv eer: 0.1090938132254286\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001442\n",
      "Batch 200/1174\t Loss 0.001436\n",
      "Batch 300/1174\t Loss 0.001466\n",
      "Batch 400/1174\t Loss 0.001455\n",
      "Batch 500/1174\t Loss 0.001446\n",
      "Batch 600/1174\t Loss 0.001446\n",
      "Batch 700/1174\t Loss 0.001445\n",
      "Batch 800/1174\t Loss 0.001441\n",
      "Batch 900/1174\t Loss 0.001443\n",
      "Batch 1000/1174\t Loss 0.001441\n",
      "Batch 1100/1174\t Loss 0.001439\n",
      "epoch #31, train loss: 213.33624318242073\n",
      "epoch #31, sv eer: 0.1064316899158769\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001422\n",
      "Batch 200/1174\t Loss 0.001444\n",
      "Batch 300/1174\t Loss 0.001447\n",
      "Batch 400/1174\t Loss 0.001445\n",
      "Batch 500/1174\t Loss 0.001443\n",
      "Batch 600/1174\t Loss 0.001440\n",
      "Batch 700/1174\t Loss 0.001444\n",
      "Batch 800/1174\t Loss 0.001442\n",
      "Batch 900/1174\t Loss 0.001440\n",
      "Batch 1000/1174\t Loss 0.001436\n",
      "Batch 1100/1174\t Loss 0.001432\n",
      "epoch #32, train loss: 212.64110420644283\n",
      "epoch #32, sv eer: 0.10898732829304654\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001470\n",
      "Batch 200/1174\t Loss 0.001438\n",
      "Batch 300/1174\t Loss 0.001434\n",
      "Batch 400/1174\t Loss 0.001444\n",
      "Batch 500/1174\t Loss 0.001431\n",
      "Batch 600/1174\t Loss 0.001418\n",
      "Batch 700/1174\t Loss 0.001419\n",
      "Batch 800/1174\t Loss 0.001418\n",
      "Batch 900/1174\t Loss 0.001419\n",
      "Batch 1000/1174\t Loss 0.001422\n",
      "Batch 1100/1174\t Loss 0.001423\n",
      "epoch #33, train loss: 210.34036089479923\n",
      "epoch #33, sv eer: 0.10770950910446171\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001435\n",
      "Batch 200/1174\t Loss 0.001428\n",
      "Batch 300/1174\t Loss 0.001434\n",
      "Batch 400/1174\t Loss 0.001435\n",
      "Batch 500/1174\t Loss 0.001438\n",
      "Batch 600/1174\t Loss 0.001435\n",
      "Batch 700/1174\t Loss 0.001436\n",
      "Batch 800/1174\t Loss 0.001429\n",
      "Batch 900/1174\t Loss 0.001428\n",
      "Batch 1000/1174\t Loss 0.001430\n",
      "Batch 1100/1174\t Loss 0.001432\n",
      "epoch #34, train loss: 211.37582644820213\n",
      "epoch #34, sv eer: 0.1090938132254286\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001375\n",
      "Batch 200/1174\t Loss 0.001412\n",
      "Batch 300/1174\t Loss 0.001424\n",
      "Batch 400/1174\t Loss 0.001424\n",
      "Batch 500/1174\t Loss 0.001415\n",
      "Batch 600/1174\t Loss 0.001428\n",
      "Batch 700/1174\t Loss 0.001426\n",
      "Batch 800/1174\t Loss 0.001424\n",
      "Batch 900/1174\t Loss 0.001430\n",
      "Batch 1000/1174\t Loss 0.001423\n",
      "Batch 1100/1174\t Loss 0.001426\n",
      "epoch #35, train loss: 211.20177997648716\n",
      "epoch #35, sv eer: 0.10866787349590033\n",
      "------------------------------\n",
      "curr_lr: 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/1174\t Loss 0.001442\n",
      "Batch 200/1174\t Loss 0.001422\n",
      "Batch 300/1174\t Loss 0.001421\n",
      "Batch 400/1174\t Loss 0.001418\n",
      "Batch 500/1174\t Loss 0.001413\n",
      "Batch 600/1174\t Loss 0.001410\n",
      "Batch 700/1174\t Loss 0.001408\n",
      "Batch 800/1174\t Loss 0.001416\n",
      "Batch 900/1174\t Loss 0.001415\n",
      "Batch 1000/1174\t Loss 0.001414\n",
      "Batch 1100/1174\t Loss 0.001419\n",
      "epoch #36, train loss: 210.4752187281847\n",
      "epoch #36, sv eer: 0.10951975295495688\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001328\n",
      "Batch 200/1174\t Loss 0.001354\n",
      "Batch 300/1174\t Loss 0.001368\n",
      "Batch 400/1174\t Loss 0.001382\n",
      "Batch 500/1174\t Loss 0.001390\n",
      "Batch 600/1174\t Loss 0.001393\n",
      "Batch 700/1174\t Loss 0.001406\n",
      "Batch 800/1174\t Loss 0.001410\n",
      "Batch 900/1174\t Loss 0.001414\n",
      "Batch 1000/1174\t Loss 0.001413\n",
      "Batch 1100/1174\t Loss 0.001412\n",
      "epoch #37, train loss: 209.3656529188156\n",
      "epoch #37, sv eer: 0.10872111596209136\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001440\n",
      "Batch 200/1174\t Loss 0.001416\n",
      "Batch 300/1174\t Loss 0.001424\n",
      "Batch 400/1174\t Loss 0.001429\n",
      "Batch 500/1174\t Loss 0.001434\n",
      "Batch 600/1174\t Loss 0.001427\n",
      "Batch 700/1174\t Loss 0.001426\n",
      "Batch 800/1174\t Loss 0.001427\n",
      "Batch 900/1174\t Loss 0.001433\n",
      "Batch 1000/1174\t Loss 0.001433\n",
      "Batch 1100/1174\t Loss 0.001431\n",
      "epoch #38, train loss: 211.90378158912063\n",
      "epoch #38, sv eer: 0.11095729954211479\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001441\n",
      "Batch 200/1174\t Loss 0.001442\n",
      "Batch 300/1174\t Loss 0.001451\n",
      "Batch 400/1174\t Loss 0.001453\n",
      "Batch 500/1174\t Loss 0.001445\n",
      "Batch 600/1174\t Loss 0.001438\n",
      "Batch 700/1174\t Loss 0.001448\n",
      "Batch 800/1174\t Loss 0.001443\n",
      "Batch 900/1174\t Loss 0.001438\n",
      "Batch 1000/1174\t Loss 0.001438\n",
      "Batch 1100/1174\t Loss 0.001433\n",
      "epoch #39, train loss: 212.341997474432\n",
      "epoch #39, sv eer: 0.1064316899158769\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001402\n",
      "Batch 200/1174\t Loss 0.001412\n",
      "Batch 300/1174\t Loss 0.001436\n",
      "Batch 400/1174\t Loss 0.001411\n",
      "Batch 500/1174\t Loss 0.001416\n",
      "Batch 600/1174\t Loss 0.001422\n",
      "Batch 700/1174\t Loss 0.001426\n",
      "Batch 800/1174\t Loss 0.001419\n",
      "Batch 900/1174\t Loss 0.001414\n",
      "Batch 1000/1174\t Loss 0.001417\n",
      "Batch 1100/1174\t Loss 0.001414\n",
      "epoch #40, train loss: 209.86588797718287\n",
      "epoch #40, sv eer: 0.11164945160259823\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001423\n",
      "Batch 200/1174\t Loss 0.001388\n",
      "Batch 300/1174\t Loss 0.001397\n",
      "Batch 400/1174\t Loss 0.001410\n",
      "Batch 500/1174\t Loss 0.001414\n",
      "Batch 600/1174\t Loss 0.001402\n",
      "Batch 700/1174\t Loss 0.001405\n",
      "Batch 800/1174\t Loss 0.001411\n",
      "Batch 900/1174\t Loss 0.001408\n",
      "Batch 1000/1174\t Loss 0.001410\n",
      "Batch 1100/1174\t Loss 0.001411\n",
      "epoch #41, train loss: 208.74585364758968\n",
      "epoch #41, sv eer: 0.11042487488020446\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001442\n",
      "Batch 200/1174\t Loss 0.001433\n",
      "Batch 300/1174\t Loss 0.001426\n",
      "Batch 400/1174\t Loss 0.001428\n",
      "Batch 500/1174\t Loss 0.001417\n",
      "Batch 600/1174\t Loss 0.001422\n",
      "Batch 700/1174\t Loss 0.001417\n",
      "Batch 800/1174\t Loss 0.001409\n",
      "Batch 900/1174\t Loss 0.001415\n",
      "Batch 1000/1174\t Loss 0.001415\n",
      "Batch 1100/1174\t Loss 0.001417\n",
      "epoch #42, train loss: 209.74032057449222\n",
      "epoch #42, sv eer: 0.11303375572356511\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001470\n",
      "Batch 200/1174\t Loss 0.001433\n",
      "Batch 300/1174\t Loss 0.001410\n",
      "Batch 400/1174\t Loss 0.001402\n",
      "Batch 500/1174\t Loss 0.001413\n",
      "Batch 600/1174\t Loss 0.001407\n",
      "Batch 700/1174\t Loss 0.001410\n",
      "Batch 800/1174\t Loss 0.001407\n",
      "Batch 900/1174\t Loss 0.001407\n",
      "Batch 1000/1174\t Loss 0.001409\n",
      "Batch 1100/1174\t Loss 0.001409\n",
      "epoch #43, train loss: 208.65811454504728\n",
      "epoch #43, sv eer: 0.11729315301884784\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001396\n",
      "Batch 200/1174\t Loss 0.001385\n",
      "Batch 300/1174\t Loss 0.001409\n",
      "Batch 400/1174\t Loss 0.001400\n",
      "Batch 500/1174\t Loss 0.001393\n",
      "Batch 600/1174\t Loss 0.001398\n",
      "Batch 700/1174\t Loss 0.001410\n",
      "Batch 800/1174\t Loss 0.001413\n",
      "Batch 900/1174\t Loss 0.001414\n",
      "Batch 1000/1174\t Loss 0.001410\n",
      "Batch 1100/1174\t Loss 0.001407\n",
      "epoch #44, train loss: 209.29007169604301\n",
      "epoch #44, sv eer: 0.10834841869875413\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001440\n",
      "Batch 200/1174\t Loss 0.001421\n",
      "Batch 300/1174\t Loss 0.001418\n",
      "Batch 400/1174\t Loss 0.001427\n",
      "Batch 500/1174\t Loss 0.001414\n",
      "Batch 600/1174\t Loss 0.001407\n",
      "Batch 700/1174\t Loss 0.001401\n",
      "Batch 800/1174\t Loss 0.001397\n",
      "Batch 900/1174\t Loss 0.001402\n",
      "Batch 1000/1174\t Loss 0.001404\n",
      "Batch 1100/1174\t Loss 0.001406\n",
      "epoch #45, train loss: 208.1766473390162\n",
      "epoch #45, sv eer: 0.11479075710786923\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001430\n",
      "Batch 200/1174\t Loss 0.001394\n",
      "Batch 300/1174\t Loss 0.001416\n",
      "Batch 400/1174\t Loss 0.001428\n",
      "Batch 500/1174\t Loss 0.001422\n",
      "Batch 600/1174\t Loss 0.001415\n",
      "Batch 700/1174\t Loss 0.001420\n",
      "Batch 800/1174\t Loss 0.001416\n",
      "Batch 900/1174\t Loss 0.001419\n",
      "Batch 1000/1174\t Loss 0.001423\n",
      "Batch 1100/1174\t Loss 0.001417\n",
      "epoch #46, train loss: 210.0869814939797\n",
      "epoch #46, sv eer: 0.11361942285166649\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001389\n",
      "Batch 200/1174\t Loss 0.001403\n",
      "Batch 300/1174\t Loss 0.001394\n",
      "Batch 400/1174\t Loss 0.001405\n",
      "Batch 500/1174\t Loss 0.001403\n",
      "Batch 600/1174\t Loss 0.001406\n",
      "Batch 700/1174\t Loss 0.001398\n",
      "Batch 800/1174\t Loss 0.001407\n",
      "Batch 900/1174\t Loss 0.001403\n",
      "Batch 1000/1174\t Loss 0.001401\n",
      "Batch 1100/1174\t Loss 0.001400\n",
      "epoch #47, train loss: 207.05668492242694\n",
      "epoch #47, sv eer: 0.11580236396549888\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001386\n",
      "Batch 200/1174\t Loss 0.001398\n",
      "Batch 300/1174\t Loss 0.001388\n",
      "Batch 400/1174\t Loss 0.001393\n",
      "Batch 500/1174\t Loss 0.001386\n",
      "Batch 600/1174\t Loss 0.001392\n",
      "Batch 700/1174\t Loss 0.001393\n",
      "Batch 800/1174\t Loss 0.001398\n",
      "Batch 900/1174\t Loss 0.001394\n",
      "Batch 1000/1174\t Loss 0.001395\n",
      "Batch 1100/1174\t Loss 0.001398\n",
      "epoch #48, train loss: 207.3143231421709\n",
      "epoch #48, sv eer: 0.11654775849217336\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/1174\t Loss 0.001340\n",
      "Batch 200/1174\t Loss 0.001378\n",
      "Batch 300/1174\t Loss 0.001371\n",
      "Batch 400/1174\t Loss 0.001375\n",
      "Batch 500/1174\t Loss 0.001373\n",
      "Batch 600/1174\t Loss 0.001381\n",
      "Batch 700/1174\t Loss 0.001381\n",
      "Batch 800/1174\t Loss 0.001376\n",
      "Batch 900/1174\t Loss 0.001378\n",
      "Batch 1000/1174\t Loss 0.001380\n",
      "Batch 1100/1174\t Loss 0.001383\n",
      "epoch #49, train loss: 205.7366864643991\n",
      "epoch #49, sv eer: 0.11910339686934299\n"
     ]
    }
   ],
   "source": [
    "from sv_system.train.si_train import val\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "\n",
    "#     step_scheduler.step()    \n",
    "    \n",
    "#     train code\n",
    "    train_loss = triplet_train(model, si_loader)\n",
    "    print(\"epoch #{}, train loss: {}\".format(epoch_idx, train_loss))\n",
    "\n",
    "#     evaluate best_metric\n",
    "    if not config['no_eer']:\n",
    "        # eer validation code\n",
    "        eer, label, score = sv_test(config, sv_loader, model, trial)\n",
    "        print(\"epoch #{}, sv eer: {}\".format(epoch_idx, eer))\n",
    "    \n",
    "    plateau_scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dda_net.state_dict(), open(\"temp_dda_net.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dda_net.load_state_dict(torch.load(\"temp_dda_net.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=64, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeds = []\n",
    "for (X, y) in test_dataloader:\n",
    "        if is_cuda:\n",
    "            X = X.cuda()\n",
    "        new_embed = dda_net.embed(X)\n",
    "        new_embeds += [new_embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embed_tensor = torch.cat(new_embeds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "sim_matrix = F.cosine_similarity(                                                                                                                                                                                                                                    \n",
    "     new_embed_tensor.cpu().unsqueeze(1), new_embed_tensor.cpu().unsqueeze(0), dim=2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]                                                                                                                                                                                                         \n",
    "score_vector = sim_matrix[cord].detach().numpy()                                                                                                                                                                                                                              \n",
    "label_vector = np.array(trial.label)                                                                                                                                                                                                                                 \n",
    "fpr, tpr, thres = roc_curve(                                                                                                                                                                                                                                         \n",
    "     label_vector, score_vector, pos_label=1)                                                                                                                                                                                                                     \n",
    "eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
