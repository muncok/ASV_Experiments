{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDA (Deep Discriminant Analysis)\n",
    "\n",
    "기존의 DDA 학습은 classification base였다면 이번에는 Metric learning을 이용하여 DDA를 수행해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/host/projects/sv_experiments/')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "# datasets\n",
    "# voxc1_fbank_xvector\n",
    "# gcommand_fbank_xvector\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\",\n",
    "                              input_frames=100, splice_frames=[50, 100], stride_frames=1, input_format='fbank',\n",
    "                              cuda=True,\n",
    "                              lrs=[0.01, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False,\n",
    "                              batch_size=128,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                              n_epochs=50\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class embedDataset(data.Dataset):\n",
    "    def __init__(self, embeds, labels):\n",
    "        super().__init__()\n",
    "        self.embeds = embeds\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.embeds[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.embeds.shape[0]\n",
    "\n",
    "def embedToDataset(embeds, key_df):\n",
    "    labels = key_df.label.tolist()\n",
    "    dataset = embedDataset(embeds, labels)\n",
    "    \n",
    "    return dataset, embeds.shape[1], len(key_df.label.unique())\n",
    "\n",
    "def key2df(keys):\n",
    "    key_df = pd.DataFrame(keys, columns=['key'])\n",
    "    key_df['spk'] = key_df.key.apply(lambda x: x.split(\"-\")[0])\n",
    "    key_df['label'] = key_df.groupby('spk').ngroup()\n",
    "    key_df['origin'] = key_df.spk.apply(lambda x: 'voxc2' if x.startswith('id') else 'voxc1')\n",
    "    \n",
    "    return key_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = pd.read_pickle(\"/dataset/SV_sets/dataframes/voxc1/voxc_trial.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_keys = pickle.load(open(\"../../embeddings/voxc12/xvectors/xvectors_tdnn6b/train_feat/key.pkl\", \"rb\"))\n",
    "si_embeds = np.load(\"../../embeddings/voxc12/xvectors/xvectors_tdnn6b/train_feat/feat.npy\")\n",
    "\n",
    "sv_keys = pickle.load(open(\"../../embeddings/voxc12/xvectors/xvectors_tdnn6b/test_feat/key.pkl\", \"rb\"))\n",
    "sv_embeds = np.load(\"../../embeddings/voxc12/xvectors/xvectors_tdnn6b/test_feat/feat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_key_df = key2df(si_keys)\n",
    "sv_key_df = key2df(sv_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_dataset, embed_dim, n_labels = embedToDataset(si_embeds, si_key_df)\n",
    "sv_dataset, _, _ = embedToDataset(sv_embeds, sv_key_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "si_loader = DataLoader(si_dataset, batch_size=128, num_workers=0, shuffle=True)\n",
    "sv_loader = DataLoader(sv_dataset, batch_size=128, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class dda_model(nn.Module):\n",
    "    def __init__(self, in_dims, n_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(in_dims, 2*in_dims),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(2*in_dims, 2*in_dims),\n",
    "            nn.PReLU()\n",
    "        )    \n",
    "        self.hidden_batch = nn.BatchNorm1d(2*in_dims)\n",
    "    \n",
    "        self.embedding_layer = nn.Linear(2*in_dims, n_labels)\n",
    "        \n",
    "    def embed(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = self.hidden_batch(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):           \n",
    "        x = self.embed(x)\n",
    "        x = self.embedding_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dda_model(embed_dim, n_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dda_model(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (hidden_layer): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (hidden_batch): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (embedding_layer): Linear(in_features=1024, out_features=7324, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.train.train_utils import set_seed, find_optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "\n",
    "criterion, optimizer = find_optimizer(config, model)\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "step_scheduler = MultiStepLR(optimizer, [10], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(si_loader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += torch.sum(torch.eq(torch.argmax(logit, dim=1), y)).item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "#         if (batch_idx+1) % 100 == 0:\n",
    "#             print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "#                   .format(batch_idx+1, len(si_loader), loss_sum /(batch_idx+1),)\n",
    "#                  )\n",
    "        acc = n_corrects / total\n",
    "\n",
    "    acc = n_corrects / total\n",
    "    return loss_sum, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def embeds_utterance(config, val_dataloader, model):\n",
    "    val_iter = iter(val_dataloader)\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iter:\n",
    "            X, y = batch\n",
    "            if not config['no_cuda']:\n",
    "                X = X.cuda()\n",
    "                \n",
    "            model_output = model.embed(X).cpu().data\n",
    "            embeddings.append(model_output)\n",
    "            labels.append(y.numpy())\n",
    "        embeddings = torch.cat(embeddings)\n",
    "        labels = np.hstack(labels)\n",
    "    return embeddings, labels \n",
    "\n",
    "def sv_test(config, sv_loader, model, trial):\n",
    "        embeddings, _ = embeds_utterance(config, sv_loader, model)\n",
    "        sim_matrix = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)\n",
    "        cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]\n",
    "        score_vector = sim_matrix[cord].numpy()\n",
    "        label_vector = np.array(trial.label)\n",
    "        fpr, tpr, thres = roc_curve(\n",
    "                label_vector, score_vector, pos_label=1)\n",
    "        eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "\n",
    "        return eer, label_vector, score_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #0, train accuracy: 0.8410005448806273\n",
      "epoch #0, sv eer: 0.07629645405175167\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #1, train accuracy: 0.9811867437432673\n",
      "epoch #1, sv eer: 0.0750718773293579\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #2, train accuracy: 0.9885606383245233\n",
      "epoch #2, sv eer: 0.0753913321265041\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #3, train accuracy: 0.9913257509331863\n",
      "epoch #3, sv eer: 0.07560430199126823\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #4, train accuracy: 0.9928539218879174\n",
      "epoch #4, sv eer: 0.07549781705888617\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #5, train accuracy: 0.9936955119372698\n",
      "epoch #5, sv eer: 0.07528484719412203\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #6, train accuracy: 0.9940078788486109\n",
      "epoch #6, sv eer: 0.07555105952507721\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #7, train accuracy: 0.9944047962021194\n",
      "epoch #7, sv eer: 0.07688212117985305\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #8, train accuracy: 0.9946146065586091\n",
      "epoch #8, sv eer: 0.07549781705888617\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #9, train accuracy: 0.9946897625072024\n",
      "epoch #9, sv eer: 0.07650942391651581\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #10, train accuracy: 0.9948800010020793\n",
      "epoch #10, sv eer: 0.07618996911936961\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #11, train accuracy: 0.9949473282060275\n",
      "epoch #11, sv eer: 0.07709509104461719\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #12, train accuracy: 0.994946545331563\n",
      "epoch #12, sv eer: 0.07624321158556065\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #13, train accuracy: 0.9950733709948142\n",
      "epoch #13, sv eer: 0.07528484719412203\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #14, train accuracy: 0.9951798419219882\n",
      "epoch #14, sv eer: 0.07496539239697583\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #15, train accuracy: 0.9951101660946464\n",
      "epoch #15, sv eer: 0.07650942391651581\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #16, train accuracy: 0.9951516584412656\n",
      "epoch #16, sv eer: 0.07501863486316686\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #17, train accuracy: 0.9952221171430718\n",
      "epoch #17, sv eer: 0.07533808966031307\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #18, train accuracy: 0.9952667409875492\n",
      "epoch #18, sv eer: 0.07512511979554894\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #19, train accuracy: 0.9952941415938071\n",
      "epoch #19, sv eer: 0.07517836226173996\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #20, train accuracy: 0.9953747776636521\n",
      "epoch #20, sv eer: 0.0749121499307848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #21, train accuracy: 0.9953356339404263\n",
      "epoch #21, sv eer: 0.07560430199126823\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #22, train accuracy: 0.9953144963298846\n",
      "epoch #22, sv eer: 0.07618996911936961\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #23, train accuracy: 0.9954468021143874\n",
      "epoch #23, sv eer: 0.07608348418698754\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #24, train accuracy: 0.9955000375779743\n",
      "epoch #24, sv eer: 0.07597699925460548\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #25, train accuracy: 0.9954076583911616\n",
      "epoch #25, sv eer: 0.07618996911936961\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #26, train accuracy: 0.9953857379061553\n",
      "epoch #26, sv eer: 0.07672239378127994\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #27, train accuracy: 0.995510214946013\n",
      "epoch #27, sv eer: 0.07597699925460548\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #28, train accuracy: 0.9955407470501291\n",
      "epoch #28, sv eer: 0.07618996911936961\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #29, train accuracy: 0.9955689305308515\n",
      "epoch #29, sv eer: 0.07709509104461719\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #30, train accuracy: 0.9955039519502968\n",
      "epoch #30, sv eer: 0.07736130337557236\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #31, train accuracy: 0.9956057256306836\n",
      "epoch #31, sv eer: 0.07768075817271856\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #32, train accuracy: 0.9955016033269033\n",
      "epoch #32, sv eer: 0.07736130337557236\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #33, train accuracy: 0.9955203923140516\n",
      "epoch #33, sv eer: 0.07533808966031307\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #34, train accuracy: 0.9955798907733547\n",
      "epoch #34, sv eer: 0.07581727185603238\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #35, train accuracy: 0.9955673647819225\n",
      "epoch #35, sv eer: 0.07677563624747098\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #36, train accuracy: 0.9955838051456772\n",
      "epoch #36, sv eer: 0.07618996911936961\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #37, train accuracy: 0.9956487837262319\n",
      "epoch #37, sv eer: 0.07640293898413375\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #38, train accuracy: 0.9956832302026706\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-68689b7b94cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no_eer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# eer validation code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0meer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msv_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch #{}, sv eer: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-00f90fcea543>\u001b[0m in \u001b[0;36msv_test\u001b[0;34m(config, sv_loader, model, trial)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msv_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeds_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msim_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mcord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menrolment_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mscore_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(x1, x2, dim, eps)\u001b[0m\n\u001b[1;32m   2215\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m     \"\"\"\n\u001b[0;32m-> 2217\u001b[0;31m     \u001b[0mw12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2218\u001b[0m     \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m     \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sv_system.train.si_train import val\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "\n",
    "#     step_scheduler.step()    \n",
    "    \n",
    "#     train code\n",
    "    train_loss, train_acc = train(model)\n",
    "    print(\"epoch #{}, train accuracy: {}\".format(epoch_idx, train_acc))\n",
    "\n",
    "#     evaluate best_metric\n",
    "    if not config['no_eer']:\n",
    "        # eer validation code\n",
    "        eer, label, score = sv_test(config, sv_loader, model, trial)\n",
    "        print(\"epoch #{}, sv eer: {}\".format(epoch_idx, eer))\n",
    "    \n",
    "    plateau_scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dda_net.state_dict(), open(\"temp_dda_net.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exatract & Save New embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds, _ = embeds_utterance(config, si_loader, model)\n",
    "train_keys = si_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeds, _ = embeds_utterance(config, sv_loader, model)\n",
    "test_keys = sv_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_key_embeds(save_dir, train_keys, train_embeds, test_keys, test_embeds):\n",
    "    if not os.path.isdir(os.path.join(save_dir, \"train_feat\")):\n",
    "        os.makedirs(os.path.join(save_dir, \"train_feat\"))    \n",
    "    if not os.path.isdir(os.path.join(save_dir, \"test_feat\")):\n",
    "        os.makedirs(os.path.join(save_dir, \"test_feat\"))\n",
    "        \n",
    "    pickle.dump(train_keys, open(os.path.join(save_dir, \"train_feat\", \"key.pkl\"), \"wb\"))\n",
    "    np.save(open(os.path.join(save_dir, \"train_feat\", \"feat.npy\"), \"wb\"), train_embeds.numpy())\n",
    "    pickle.dump(test_keys, open(os.path.join(save_dir, \"test_feat\", \"key.pkl\"), \"wb\"))\n",
    "    np.save(open(os.path.join(save_dir, \"test_feat\", \"feat.npy\"), \"wb\"), test_embeds.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_key_embeds(\"../../embeddings/voxc12/dda_xvector1/\", train_keys, train_embeds, test_keys, test_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orig_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-64a4bac3ca95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m sim_matrix = F.cosine_similarity(                                                                                                                                                                                                                                    \n\u001b[0;32m----> 3\u001b[0;31m      orig_embeddings.unsqueeze(1), orig_embeddings.unsqueeze(0), dim=2)       \n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'orig_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "sim_matrix = F.cosine_similarity(                                                                                                                                                                                                                                    \n",
    "     orig_embeddings.unsqueeze(1), orig_embeddings.unsqueeze(0), dim=2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]                                                                                                                                                                                                         \n",
    "score_vector = sim_matrix[cord].detach().numpy()                                                                                                                                                                                                                              \n",
    "label_vector = np.array(trial.label)                                                                                                                                                                                                                                 \n",
    "fpr, tpr, thres = roc_curve(                                                                                                                                                                                                                                         \n",
    "     label_vector, score_vector, pos_label=1)                                                                                                                                                                                                                     \n",
    "eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10549"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_keys.index('Betty_White-zqtRF50lhsQ-0000003')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_key_df['session'] = sv_key_df.key.apply(lambda x: x.split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trial_ids = []\n",
    "new_trial_keys = []\n",
    "for idx, row in sv_key_df.iterrows():\n",
    "    pos_pair = sv_key_df[(sv_key_df.spk == row.spk) & (sv_key_df.session != row.session)].sample(n=4)\n",
    "    neg_pair = sv_key_df[(sv_key_df.spk != row.spk)].sample(n=4)\n",
    "    \n",
    "    for pos_idx, neg_idx in zip(pos_pair.index.tolist(), neg_pair.index.tolist()):\n",
    "        new_trial_ids += [(idx, pos_idx, 1), (idx, neg_idx, 0)]\n",
    "    \n",
    "    for pos_key, neg_key in zip(pos_pair.key, neg_pair.key):\n",
    "        new_trial_keys += [(row.key, pos_key, 'target'), (row.key, neg_key, 'nontarget')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trial = pd.DataFrame(new_trial_ids, columns=['enrolment_id', 'test_id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "sv_embed_tensor = torch.from_numpy(sv_embeds)\n",
    "\n",
    "sim_matrix = F.cosine_similarity(                                                                                                                                                                                                                                    \n",
    "     sv_embed_tensor.unsqueeze(1), sv_embed_tensor.unsqueeze(0), dim=2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "cord = [new_trial.enrolment_id.tolist(), new_trial.test_id.tolist()]                                                                                                                                                                                                         \n",
    "score_vector = sim_matrix[cord].detach().numpy()                                                                                                                                                                                                                              \n",
    "label_vector = np.array(new_trial.label)                                                                                                                                                                                                                                 \n",
    "fpr, tpr, thres = roc_curve(                                                                                                                                                                                                                                         \n",
    "     label_vector, score_vector, pos_label=1)                                                                                                                                                                                                                     \n",
    "eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09478867459991794"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plda trial format으로 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trial_key = pd.DataFrame(new_trial_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trial_key.to_csv(\"voxc12_trial_sv\", sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trial['is_target'] = new_trial.label.apply(lambda x: 'target' if x == 1 else 'nontarget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrolment_id</th>\n",
       "      <th>test_id</th>\n",
       "      <th>label</th>\n",
       "      <th>is_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>target</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   enrolment_id  test_id  label is_target\n",
       "0             0      100      1    target"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_trial.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
