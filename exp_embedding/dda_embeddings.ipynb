{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDA (Deep Discriminant Analysis)\n",
    "\n",
    "putting dvectors through additional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "%pylab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = pd.read_pickle(\"../dataset/dataframes/voxc1/voxc_trial.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = pickle.load(open(\"../best_models/voxc1/ResNet34_v4_softmax/voxc_train_dvectors.pkl\", \n",
    "                                  \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = np.array([v for v in embedding_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spks = [k.split(\"/\")[0] for k in embedding_dict.keys()]\n",
    "spk2label = pd.Series(spks).unique().tolist()\n",
    "labels = [spk2label.index(spk) for spk in spks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_dict = pickle.load(open(\"../best_models/voxc1/ResNet34_v4_softmax/voxc_test_dvectors.pkl\", \n",
    "                                  \"rb\"))\n",
    "test_embeds = np.array([v for v in test_embedding_dict.values()])\n",
    "test_spks = [k.split(\"/\")[0] for k in test_embedding_dict.keys()]\n",
    "test_spk2label = pd.Series(test_spks).unique().tolist()\n",
    "test_labels = [test_spk2label.index(spk) for spk in test_spks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = embedDataset(embeds, labels)\n",
    "test_dataset = embedDataset(test_embeds, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                                                                                                                                                                                                                                                 \n",
    "import torch.nn as nn                                                                                                                                                                                                                                                        \n",
    "\n",
    "class CenterLoss(nn.Module):                                                                                                                                                                                                                                                 \n",
    "    \"\"\"Center loss.                                                                                                                                                                                                                                                          \n",
    "\n",
    "    Reference:                                                                                                                                                                                                                                                               \n",
    "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.                                                                                                                                                                              \n",
    "\n",
    "    Args:                                                                                                                                                                                                                                                                    \n",
    "     num_classes (int): number of classes.                                                                                                                                                                                                                                \n",
    "     feat_dim (int): feature dimension.                                                                                                                                                                                                                                   \n",
    "    \"\"\"                                                                                                                                                                                                                                                                      \n",
    "    def __init__(self, num_classes=10, feat_dim=2, use_gpu=True):                                                                                                                                                                                                            \n",
    "        super(CenterLoss, self).__init__()                                                                                                                                                                                                                                   \n",
    "        self.num_classes = num_classes                                                                                                                                                                                                                                       \n",
    "        self.feat_dim = feat_dim                                                                                                                                                                                                                                             \n",
    "        self.use_gpu = use_gpu                                                                                                                                                                                                                                               \n",
    "\n",
    "        if self.use_gpu:                                                                                                                                                                                                                                                     \n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())                                                                                                                                                                                 \n",
    "        else:                                                                                                                                                                                                                                                                \n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))                                                                                                                                                                                        \n",
    "\n",
    "    def forward(self, x, labels):                                                                                                                                                                                                                                            \n",
    "        \"\"\"                                                                                                                                                                                                                                                                  \n",
    "        Args:                                                                                                                                                                                                                                                                \n",
    "         x: feature matrix with shape (batch_size, feat_dim).                                                                                                                                                                                                             \n",
    "         labels: ground truth labels with shape (num_classes).                                                                                                                                                                                                            \n",
    "        \"\"\"                                                                                                                                                                                                                                                                  \n",
    "        batch_size = x.size(0)                                                                                                                                                                                                                                               \n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "            torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()                                                                                                                                                               \n",
    "        distmat.addmm_(1, -2, x, self.centers.t())                                                                                                                                                                                                                           \n",
    "\n",
    "        classes = torch.arange(self.num_classes).long()                                                                                                                                                                                                                      \n",
    "        if self.use_gpu: classes = classes.cuda()                                                                                                                                                                                                                            \n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)                                                                                                                                                                                                    \n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))                                                                                                                                                                                                       \n",
    "\n",
    "        dist = []                                                                                                                                                                                                                                                            \n",
    "        for i in range(batch_size):                                                                                                                                                                                                                                          \n",
    "            value = distmat[i][mask[i]]                                                                                                                                                                                                                                      \n",
    "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability                                                                                                                                                                                              \n",
    "            dist.append(value)                                                                                                                                                                                                                                               \n",
    "        dist = torch.cat(dist)                                                                                                                                                                                                                                               \n",
    "        loss = dist.mean()                                                                                                                                                                                                                                                   \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class dda_model(nn.Module):\n",
    "    def __init__(self, in_dims, n_labels):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(in_dims, 2*in_dims),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(2*in_dims, 2*in_dims),\n",
    "            nn.ReLU(),\n",
    "        )    \n",
    "#         self.hidden_batch = nn.BatchNorm2d(1)\n",
    "    \n",
    "        self.embedding_layer = nn.Linear(2*in_dims, n_labels)\n",
    "    \n",
    "    def forward(self, x):           \n",
    "        x = self.input_layer(x)\n",
    "        feat = self.hidden_layer(x)\n",
    "        out = self.embedding_layer(feat)\n",
    "        \n",
    "        return feat, out\n",
    "    \n",
    "    def embed(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_layer(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dda_net = dda_model(embeds.shape[1], len(spk2label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dda_model(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (hidden_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (embedding_layer): Linear(in_features=256, out_features=1211, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dda_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "is_cuda = True\n",
    "weight_cent = 0.01\n",
    "criterion_xent = nn.CrossEntropyLoss()\n",
    "criterion_cent = CenterLoss(num_classes=len(spk2label), feat_dim=256, use_gpu=is_cuda)\n",
    "optimizer_model = torch.optim.SGD(dda_net.parameters(), lr=0.001, weight_decay=5e-04, momentum=0.9)\n",
    "optimizer_centloss = torch.optim.SGD(criterion_cent.parameters(), lr=0.5)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=128, num_workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Batch 100/1156\t Loss 1.347935 XentLoss 0.669932 CenterLoss 0.678003\n",
      "Batch 200/1156\t Loss 1.356123 XentLoss 0.675990 CenterLoss 0.680133\n",
      "Batch 300/1156\t Loss 1.349205 XentLoss 0.671279 CenterLoss 0.677925\n",
      "Batch 400/1156\t Loss 1.348451 XentLoss 0.670388 CenterLoss 0.678062\n",
      "Batch 500/1156\t Loss 1.349134 XentLoss 0.670966 CenterLoss 0.678168\n",
      "Batch 600/1156\t Loss 1.350251 XentLoss 0.671806 CenterLoss 0.678444\n",
      "Batch 700/1156\t Loss 1.349512 XentLoss 0.671183 CenterLoss 0.678329\n",
      "Batch 800/1156\t Loss 1.347875 XentLoss 0.669868 CenterLoss 0.678006\n",
      "Batch 900/1156\t Loss 1.345107 XentLoss 0.667614 CenterLoss 0.677493\n",
      "Batch 1000/1156\t Loss 1.344864 XentLoss 0.667301 CenterLoss 0.677563\n",
      "Batch 1100/1156\t Loss 1.344678 XentLoss 0.667193 CenterLoss 0.677485\n",
      "loss: 1555.0148074626923, acc:0.9490789873931118\n",
      "epoch: 1\n",
      "Batch 100/1156\t Loss 1.323208 XentLoss 0.655143 CenterLoss 0.668064\n",
      "Batch 200/1156\t Loss 1.326369 XentLoss 0.657923 CenterLoss 0.668446\n",
      "Batch 300/1156\t Loss 1.324209 XentLoss 0.654893 CenterLoss 0.669316\n",
      "Batch 400/1156\t Loss 1.321957 XentLoss 0.653010 CenterLoss 0.668947\n",
      "Batch 500/1156\t Loss 1.322452 XentLoss 0.653108 CenterLoss 0.669345\n",
      "Batch 600/1156\t Loss 1.323121 XentLoss 0.653540 CenterLoss 0.669581\n",
      "Batch 700/1156\t Loss 1.321995 XentLoss 0.652598 CenterLoss 0.669398\n",
      "Batch 800/1156\t Loss 1.322246 XentLoss 0.652403 CenterLoss 0.669843\n",
      "Batch 900/1156\t Loss 1.323011 XentLoss 0.652812 CenterLoss 0.670199\n",
      "Batch 1000/1156\t Loss 1.322839 XentLoss 0.652717 CenterLoss 0.670122\n",
      "Batch 1100/1156\t Loss 1.322718 XentLoss 0.652410 CenterLoss 0.670309\n",
      "loss: 1527.5887163877487, acc:0.9508500354885592\n",
      "epoch: 2\n",
      "Batch 100/1156\t Loss 1.294643 XentLoss 0.634233 CenterLoss 0.660410\n",
      "Batch 200/1156\t Loss 1.298930 XentLoss 0.637481 CenterLoss 0.661450\n",
      "Batch 300/1156\t Loss 1.303067 XentLoss 0.639472 CenterLoss 0.663595\n",
      "Batch 400/1156\t Loss 1.299994 XentLoss 0.637041 CenterLoss 0.662952\n",
      "Batch 500/1156\t Loss 1.297560 XentLoss 0.635339 CenterLoss 0.662221\n",
      "Batch 600/1156\t Loss 1.297239 XentLoss 0.634580 CenterLoss 0.662659\n",
      "Batch 700/1156\t Loss 1.299067 XentLoss 0.636100 CenterLoss 0.662967\n",
      "Batch 800/1156\t Loss 1.298084 XentLoss 0.635126 CenterLoss 0.662958\n",
      "Batch 900/1156\t Loss 1.297398 XentLoss 0.634333 CenterLoss 0.663065\n",
      "Batch 1000/1156\t Loss 1.297727 XentLoss 0.635063 CenterLoss 0.662665\n",
      "Batch 1100/1156\t Loss 1.298751 XentLoss 0.635898 CenterLoss 0.662853\n",
      "loss: 1501.9140318632126, acc:0.9529387906850982\n",
      "epoch: 3\n",
      "Batch 100/1156\t Loss 1.282978 XentLoss 0.625711 CenterLoss 0.657268\n",
      "Batch 200/1156\t Loss 1.281597 XentLoss 0.623892 CenterLoss 0.657705\n",
      "Batch 300/1156\t Loss 1.281844 XentLoss 0.624414 CenterLoss 0.657430\n",
      "Batch 400/1156\t Loss 1.284538 XentLoss 0.626620 CenterLoss 0.657918\n",
      "Batch 500/1156\t Loss 1.284948 XentLoss 0.627447 CenterLoss 0.657501\n",
      "Batch 600/1156\t Loss 1.284011 XentLoss 0.626343 CenterLoss 0.657668\n",
      "Batch 700/1156\t Loss 1.282092 XentLoss 0.624626 CenterLoss 0.657466\n",
      "Batch 800/1156\t Loss 1.281343 XentLoss 0.624234 CenterLoss 0.657109\n",
      "Batch 900/1156\t Loss 1.280702 XentLoss 0.624011 CenterLoss 0.656691\n",
      "Batch 1000/1156\t Loss 1.279549 XentLoss 0.622921 CenterLoss 0.656628\n",
      "Batch 1100/1156\t Loss 1.279350 XentLoss 0.622886 CenterLoss 0.656464\n",
      "loss: 1477.9656349420547, acc:0.954162301010579\n",
      "epoch: 4\n",
      "Batch 100/1156\t Loss 1.269964 XentLoss 0.620307 CenterLoss 0.649657\n",
      "Batch 200/1156\t Loss 1.267681 XentLoss 0.617521 CenterLoss 0.650159\n",
      "Batch 300/1156\t Loss 1.263748 XentLoss 0.614308 CenterLoss 0.649440\n",
      "Batch 400/1156\t Loss 1.262012 XentLoss 0.612390 CenterLoss 0.649622\n",
      "Batch 500/1156\t Loss 1.260577 XentLoss 0.611234 CenterLoss 0.649342\n",
      "Batch 600/1156\t Loss 1.261707 XentLoss 0.612298 CenterLoss 0.649409\n",
      "Batch 700/1156\t Loss 1.262295 XentLoss 0.612683 CenterLoss 0.649612\n",
      "Batch 800/1156\t Loss 1.262162 XentLoss 0.612363 CenterLoss 0.649799\n",
      "Batch 900/1156\t Loss 1.262094 XentLoss 0.612293 CenterLoss 0.649802\n",
      "Batch 1000/1156\t Loss 1.260395 XentLoss 0.610755 CenterLoss 0.649640\n",
      "Batch 1100/1156\t Loss 1.258810 XentLoss 0.609473 CenterLoss 0.649337\n",
      "loss: 1455.531539440155, acc:0.9557575962415926\n",
      "epoch: 5\n",
      "Batch 100/1156\t Loss 1.245356 XentLoss 0.603793 CenterLoss 0.641563\n",
      "Batch 200/1156\t Loss 1.244325 XentLoss 0.600324 CenterLoss 0.644001\n",
      "Batch 300/1156\t Loss 1.240804 XentLoss 0.597908 CenterLoss 0.642896\n",
      "Batch 400/1156\t Loss 1.242610 XentLoss 0.599676 CenterLoss 0.642934\n",
      "Batch 500/1156\t Loss 1.244636 XentLoss 0.601157 CenterLoss 0.643480\n",
      "Batch 600/1156\t Loss 1.243483 XentLoss 0.600005 CenterLoss 0.643478\n",
      "Batch 700/1156\t Loss 1.242444 XentLoss 0.599211 CenterLoss 0.643234\n",
      "Batch 800/1156\t Loss 1.241720 XentLoss 0.598473 CenterLoss 0.643247\n",
      "Batch 900/1156\t Loss 1.240517 XentLoss 0.597527 CenterLoss 0.642990\n",
      "Batch 1000/1156\t Loss 1.240441 XentLoss 0.597712 CenterLoss 0.642728\n",
      "Batch 1100/1156\t Loss 1.240529 XentLoss 0.597798 CenterLoss 0.642731\n",
      "loss: 1434.3028507232666, acc:0.9569743468415183\n",
      "epoch: 6\n",
      "Batch 100/1156\t Loss 1.214840 XentLoss 0.579445 CenterLoss 0.635396\n",
      "Batch 200/1156\t Loss 1.213787 XentLoss 0.579375 CenterLoss 0.634412\n",
      "Batch 300/1156\t Loss 1.218256 XentLoss 0.583139 CenterLoss 0.635116\n",
      "Batch 400/1156\t Loss 1.217711 XentLoss 0.582501 CenterLoss 0.635210\n",
      "Batch 500/1156\t Loss 1.218510 XentLoss 0.582719 CenterLoss 0.635791\n",
      "Batch 600/1156\t Loss 1.220276 XentLoss 0.584619 CenterLoss 0.635657\n",
      "Batch 700/1156\t Loss 1.221697 XentLoss 0.585626 CenterLoss 0.636070\n",
      "Batch 800/1156\t Loss 1.222920 XentLoss 0.586444 CenterLoss 0.636475\n",
      "Batch 900/1156\t Loss 1.222706 XentLoss 0.586240 CenterLoss 0.636466\n",
      "Batch 1000/1156\t Loss 1.223763 XentLoss 0.587033 CenterLoss 0.636730\n",
      "Batch 1100/1156\t Loss 1.224096 XentLoss 0.587246 CenterLoss 0.636850\n",
      "loss: 1414.2414907217026, acc:0.9579815459492345\n",
      "epoch: 7\n",
      "Batch 100/1156\t Loss 1.215356 XentLoss 0.581979 CenterLoss 0.633377\n",
      "Batch 200/1156\t Loss 1.215056 XentLoss 0.583173 CenterLoss 0.631883\n",
      "Batch 300/1156\t Loss 1.213334 XentLoss 0.581233 CenterLoss 0.632100\n",
      "Batch 400/1156\t Loss 1.211245 XentLoss 0.580080 CenterLoss 0.631165\n",
      "Batch 500/1156\t Loss 1.211782 XentLoss 0.580302 CenterLoss 0.631480\n",
      "Batch 600/1156\t Loss 1.209617 XentLoss 0.578076 CenterLoss 0.631542\n",
      "Batch 700/1156\t Loss 1.209521 XentLoss 0.578047 CenterLoss 0.631474\n",
      "Batch 800/1156\t Loss 1.208473 XentLoss 0.577270 CenterLoss 0.631202\n",
      "Batch 900/1156\t Loss 1.207675 XentLoss 0.576755 CenterLoss 0.630920\n",
      "Batch 1000/1156\t Loss 1.207516 XentLoss 0.576434 CenterLoss 0.631082\n",
      "Batch 1100/1156\t Loss 1.207051 XentLoss 0.576140 CenterLoss 0.630911\n",
      "loss: 1395.4133804440498, acc:0.9591712576469396\n",
      "epoch: 8\n",
      "Batch 100/1156\t Loss 1.194973 XentLoss 0.568343 CenterLoss 0.626630\n",
      "Batch 200/1156\t Loss 1.199133 XentLoss 0.571316 CenterLoss 0.627817\n",
      "Batch 300/1156\t Loss 1.195972 XentLoss 0.568966 CenterLoss 0.627007\n",
      "Batch 400/1156\t Loss 1.196674 XentLoss 0.569841 CenterLoss 0.626833\n",
      "Batch 500/1156\t Loss 1.196763 XentLoss 0.570181 CenterLoss 0.626582\n",
      "Batch 600/1156\t Loss 1.196536 XentLoss 0.569739 CenterLoss 0.626797\n",
      "Batch 700/1156\t Loss 1.195282 XentLoss 0.568912 CenterLoss 0.626370\n",
      "Batch 800/1156\t Loss 1.193832 XentLoss 0.567946 CenterLoss 0.625886\n",
      "Batch 900/1156\t Loss 1.193191 XentLoss 0.567446 CenterLoss 0.625745\n",
      "Batch 1000/1156\t Loss 1.193271 XentLoss 0.567568 CenterLoss 0.625703\n",
      "Batch 1100/1156\t Loss 1.192262 XentLoss 0.566794 CenterLoss 0.625468\n",
      "loss: 1377.527314066887, acc:0.9600162233413323\n",
      "epoch: 9\n",
      "Batch 100/1156\t Loss 1.179590 XentLoss 0.559369 CenterLoss 0.620221\n",
      "Batch 200/1156\t Loss 1.175868 XentLoss 0.556270 CenterLoss 0.619598\n",
      "Batch 300/1156\t Loss 1.178075 XentLoss 0.558056 CenterLoss 0.620019\n",
      "Batch 400/1156\t Loss 1.176080 XentLoss 0.556910 CenterLoss 0.619171\n",
      "Batch 500/1156\t Loss 1.175065 XentLoss 0.556065 CenterLoss 0.619000\n",
      "Batch 600/1156\t Loss 1.175225 XentLoss 0.555968 CenterLoss 0.619256\n",
      "Batch 700/1156\t Loss 1.174992 XentLoss 0.555820 CenterLoss 0.619172\n",
      "Batch 800/1156\t Loss 1.174890 XentLoss 0.555997 CenterLoss 0.618893\n",
      "Batch 900/1156\t Loss 1.175609 XentLoss 0.556493 CenterLoss 0.619116\n",
      "Batch 1000/1156\t Loss 1.176601 XentLoss 0.557314 CenterLoss 0.619287\n",
      "Batch 1100/1156\t Loss 1.175964 XentLoss 0.556975 CenterLoss 0.618989\n",
      "loss: 1360.6160117387772, acc:0.9610775002534897\n",
      "epoch: 10\n",
      "Batch 100/1156\t Loss 1.163327 XentLoss 0.549143 CenterLoss 0.614184\n",
      "Batch 200/1156\t Loss 1.160034 XentLoss 0.547513 CenterLoss 0.612521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/1156\t Loss 1.160116 XentLoss 0.547176 CenterLoss 0.612940\n",
      "Batch 400/1156\t Loss 1.162422 XentLoss 0.549147 CenterLoss 0.613275\n",
      "Batch 500/1156\t Loss 1.164449 XentLoss 0.550701 CenterLoss 0.613748\n",
      "Batch 600/1156\t Loss 1.163393 XentLoss 0.549537 CenterLoss 0.613856\n",
      "Batch 700/1156\t Loss 1.162055 XentLoss 0.548262 CenterLoss 0.613793\n",
      "Batch 800/1156\t Loss 1.161496 XentLoss 0.547691 CenterLoss 0.613805\n",
      "Batch 900/1156\t Loss 1.162428 XentLoss 0.548287 CenterLoss 0.614141\n",
      "Batch 1000/1156\t Loss 1.163695 XentLoss 0.548994 CenterLoss 0.614701\n",
      "Batch 1100/1156\t Loss 1.163769 XentLoss 0.549248 CenterLoss 0.614521\n",
      "loss: 1344.3152918815613, acc:0.9619089464967723\n",
      "epoch: 11\n",
      "Batch 100/1156\t Loss 1.149263 XentLoss 0.540494 CenterLoss 0.608768\n",
      "Batch 200/1156\t Loss 1.150369 XentLoss 0.542444 CenterLoss 0.607924\n",
      "Batch 300/1156\t Loss 1.148086 XentLoss 0.540227 CenterLoss 0.607859\n",
      "Batch 400/1156\t Loss 1.148280 XentLoss 0.540011 CenterLoss 0.608269\n",
      "Batch 500/1156\t Loss 1.149690 XentLoss 0.541157 CenterLoss 0.608532\n",
      "Batch 600/1156\t Loss 1.150457 XentLoss 0.541555 CenterLoss 0.608902\n",
      "Batch 700/1156\t Loss 1.149811 XentLoss 0.540697 CenterLoss 0.609115\n",
      "Batch 800/1156\t Loss 1.149655 XentLoss 0.540571 CenterLoss 0.609084\n",
      "Batch 900/1156\t Loss 1.148920 XentLoss 0.540074 CenterLoss 0.608846\n",
      "Batch 1000/1156\t Loss 1.149662 XentLoss 0.540796 CenterLoss 0.608867\n",
      "Batch 1100/1156\t Loss 1.149379 XentLoss 0.540493 CenterLoss 0.608886\n",
      "loss: 1329.1800809502602, acc:0.9629567039578193\n",
      "epoch: 12\n",
      "Batch 100/1156\t Loss 1.137485 XentLoss 0.534562 CenterLoss 0.602922\n",
      "Batch 200/1156\t Loss 1.137007 XentLoss 0.533040 CenterLoss 0.603967\n",
      "Batch 300/1156\t Loss 1.135789 XentLoss 0.532474 CenterLoss 0.603315\n",
      "Batch 400/1156\t Loss 1.139475 XentLoss 0.535424 CenterLoss 0.604051\n",
      "Batch 500/1156\t Loss 1.139196 XentLoss 0.535099 CenterLoss 0.604097\n",
      "Batch 600/1156\t Loss 1.139006 XentLoss 0.535119 CenterLoss 0.603887\n",
      "Batch 700/1156\t Loss 1.139879 XentLoss 0.535712 CenterLoss 0.604167\n",
      "Batch 800/1156\t Loss 1.138319 XentLoss 0.534392 CenterLoss 0.603926\n",
      "Batch 900/1156\t Loss 1.137557 XentLoss 0.533785 CenterLoss 0.603772\n",
      "Batch 1000/1156\t Loss 1.137413 XentLoss 0.533612 CenterLoss 0.603800\n",
      "Batch 1100/1156\t Loss 1.136285 XentLoss 0.532621 CenterLoss 0.603664\n",
      "loss: 1314.4930192232132, acc:0.9637746307499916\n",
      "epoch: 13\n",
      "Batch 100/1156\t Loss 1.118635 XentLoss 0.522697 CenterLoss 0.595938\n",
      "Batch 200/1156\t Loss 1.125374 XentLoss 0.528739 CenterLoss 0.596635\n",
      "Batch 300/1156\t Loss 1.127665 XentLoss 0.530278 CenterLoss 0.597388\n",
      "Batch 400/1156\t Loss 1.130706 XentLoss 0.531765 CenterLoss 0.598941\n",
      "Batch 500/1156\t Loss 1.128166 XentLoss 0.529031 CenterLoss 0.599135\n",
      "Batch 600/1156\t Loss 1.130817 XentLoss 0.530952 CenterLoss 0.599865\n",
      "Batch 700/1156\t Loss 1.126645 XentLoss 0.527674 CenterLoss 0.598971\n",
      "Batch 800/1156\t Loss 1.126064 XentLoss 0.527153 CenterLoss 0.598911\n",
      "Batch 900/1156\t Loss 1.126224 XentLoss 0.526674 CenterLoss 0.599550\n",
      "Batch 1000/1156\t Loss 1.125448 XentLoss 0.526225 CenterLoss 0.599223\n",
      "Batch 1100/1156\t Loss 1.126397 XentLoss 0.527109 CenterLoss 0.599288\n",
      "loss: 1300.5109972953796, acc:0.9643424476966235\n",
      "epoch: 14\n",
      "Batch 100/1156\t Loss 1.116943 XentLoss 0.524803 CenterLoss 0.592140\n",
      "Batch 200/1156\t Loss 1.119616 XentLoss 0.523979 CenterLoss 0.595637\n",
      "Batch 300/1156\t Loss 1.117597 XentLoss 0.522457 CenterLoss 0.595140\n",
      "Batch 400/1156\t Loss 1.115977 XentLoss 0.521055 CenterLoss 0.594922\n",
      "Batch 500/1156\t Loss 1.114312 XentLoss 0.520127 CenterLoss 0.594184\n",
      "Batch 600/1156\t Loss 1.114548 XentLoss 0.520358 CenterLoss 0.594190\n",
      "Batch 700/1156\t Loss 1.113322 XentLoss 0.519020 CenterLoss 0.594302\n",
      "Batch 800/1156\t Loss 1.113664 XentLoss 0.519323 CenterLoss 0.594341\n",
      "Batch 900/1156\t Loss 1.113009 XentLoss 0.518909 CenterLoss 0.594101\n",
      "Batch 1000/1156\t Loss 1.112847 XentLoss 0.518944 CenterLoss 0.593903\n",
      "Batch 1100/1156\t Loss 1.112740 XentLoss 0.518679 CenterLoss 0.594061\n",
      "loss: 1287.321841955185, acc:0.9651468550376855\n",
      "epoch: 15\n",
      "Batch 100/1156\t Loss 1.102190 XentLoss 0.514635 CenterLoss 0.587555\n",
      "Batch 200/1156\t Loss 1.099240 XentLoss 0.511521 CenterLoss 0.587719\n",
      "Batch 300/1156\t Loss 1.102443 XentLoss 0.513805 CenterLoss 0.588639\n",
      "Batch 400/1156\t Loss 1.103418 XentLoss 0.514221 CenterLoss 0.589197\n",
      "Batch 500/1156\t Loss 1.101308 XentLoss 0.512242 CenterLoss 0.589066\n",
      "Batch 600/1156\t Loss 1.103635 XentLoss 0.513725 CenterLoss 0.589910\n",
      "Batch 700/1156\t Loss 1.103460 XentLoss 0.513482 CenterLoss 0.589977\n",
      "Batch 800/1156\t Loss 1.103809 XentLoss 0.513590 CenterLoss 0.590219\n",
      "Batch 900/1156\t Loss 1.102957 XentLoss 0.512848 CenterLoss 0.590109\n",
      "Batch 1000/1156\t Loss 1.102484 XentLoss 0.512582 CenterLoss 0.589902\n",
      "Batch 1100/1156\t Loss 1.103299 XentLoss 0.513023 CenterLoss 0.590277\n",
      "loss: 1274.7080852389336, acc:0.9661810930476222\n",
      "epoch: 16\n",
      "Batch 100/1156\t Loss 1.083784 XentLoss 0.500321 CenterLoss 0.583463\n",
      "Batch 200/1156\t Loss 1.086938 XentLoss 0.502307 CenterLoss 0.584631\n",
      "Batch 300/1156\t Loss 1.093269 XentLoss 0.507312 CenterLoss 0.585957\n",
      "Batch 400/1156\t Loss 1.093547 XentLoss 0.507948 CenterLoss 0.585599\n",
      "Batch 500/1156\t Loss 1.093841 XentLoss 0.508361 CenterLoss 0.585480\n",
      "Batch 600/1156\t Loss 1.093842 XentLoss 0.508481 CenterLoss 0.585361\n",
      "Batch 700/1156\t Loss 1.093334 XentLoss 0.507617 CenterLoss 0.585717\n",
      "Batch 800/1156\t Loss 1.091871 XentLoss 0.506512 CenterLoss 0.585359\n",
      "Batch 900/1156\t Loss 1.092146 XentLoss 0.506596 CenterLoss 0.585550\n",
      "Batch 1000/1156\t Loss 1.092133 XentLoss 0.506325 CenterLoss 0.585807\n",
      "Batch 1100/1156\t Loss 1.092252 XentLoss 0.506586 CenterLoss 0.585666\n",
      "loss: 1262.3847715258598, acc:0.9666272349342616\n",
      "epoch: 17\n",
      "Batch 100/1156\t Loss 1.083824 XentLoss 0.503646 CenterLoss 0.580179\n",
      "Batch 200/1156\t Loss 1.077987 XentLoss 0.497544 CenterLoss 0.580443\n",
      "Batch 300/1156\t Loss 1.077157 XentLoss 0.497080 CenterLoss 0.580077\n",
      "Batch 400/1156\t Loss 1.079076 XentLoss 0.499017 CenterLoss 0.580059\n",
      "Batch 500/1156\t Loss 1.080394 XentLoss 0.499539 CenterLoss 0.580855\n",
      "Batch 600/1156\t Loss 1.081004 XentLoss 0.500187 CenterLoss 0.580818\n",
      "Batch 700/1156\t Loss 1.080447 XentLoss 0.500102 CenterLoss 0.580345\n",
      "Batch 800/1156\t Loss 1.080279 XentLoss 0.499846 CenterLoss 0.580433\n",
      "Batch 900/1156\t Loss 1.081016 XentLoss 0.500188 CenterLoss 0.580827\n",
      "Batch 1000/1156\t Loss 1.081599 XentLoss 0.500312 CenterLoss 0.581287\n",
      "Batch 1100/1156\t Loss 1.081525 XentLoss 0.499983 CenterLoss 0.581542\n",
      "loss: 1250.9027342796326, acc:0.9672558894108899\n",
      "epoch: 18\n",
      "Batch 100/1156\t Loss 1.066598 XentLoss 0.490734 CenterLoss 0.575864\n",
      "Batch 200/1156\t Loss 1.067135 XentLoss 0.491750 CenterLoss 0.575385\n",
      "Batch 300/1156\t Loss 1.070366 XentLoss 0.494565 CenterLoss 0.575802\n",
      "Batch 400/1156\t Loss 1.070468 XentLoss 0.494186 CenterLoss 0.576282\n",
      "Batch 500/1156\t Loss 1.071889 XentLoss 0.495195 CenterLoss 0.576694\n",
      "Batch 600/1156\t Loss 1.072225 XentLoss 0.495330 CenterLoss 0.576895\n",
      "Batch 700/1156\t Loss 1.072066 XentLoss 0.494915 CenterLoss 0.577151\n",
      "Batch 800/1156\t Loss 1.073133 XentLoss 0.495710 CenterLoss 0.577423\n",
      "Batch 900/1156\t Loss 1.072738 XentLoss 0.494981 CenterLoss 0.577757\n",
      "Batch 1000/1156\t Loss 1.071398 XentLoss 0.493984 CenterLoss 0.577415\n",
      "Batch 1100/1156\t Loss 1.072722 XentLoss 0.494946 CenterLoss 0.577776\n",
      "loss: 1239.4699820876122, acc:0.9677155507486396\n",
      "epoch: 19\n",
      "Batch 100/1156\t Loss 1.061613 XentLoss 0.489969 CenterLoss 0.571644\n",
      "Batch 200/1156\t Loss 1.058968 XentLoss 0.488516 CenterLoss 0.570452\n",
      "Batch 300/1156\t Loss 1.055968 XentLoss 0.484704 CenterLoss 0.571264\n",
      "Batch 400/1156\t Loss 1.058396 XentLoss 0.486132 CenterLoss 0.572264\n",
      "Batch 500/1156\t Loss 1.059700 XentLoss 0.487241 CenterLoss 0.572460\n",
      "Batch 600/1156\t Loss 1.060020 XentLoss 0.487238 CenterLoss 0.572782\n",
      "Batch 700/1156\t Loss 1.061817 XentLoss 0.488695 CenterLoss 0.573122\n",
      "Batch 800/1156\t Loss 1.063374 XentLoss 0.489784 CenterLoss 0.573591\n",
      "Batch 900/1156\t Loss 1.062585 XentLoss 0.488972 CenterLoss 0.573612\n",
      "Batch 1000/1156\t Loss 1.062511 XentLoss 0.489077 CenterLoss 0.573434\n",
      "Batch 1100/1156\t Loss 1.063059 XentLoss 0.489382 CenterLoss 0.573677\n",
      "loss: 1228.8548638820648, acc:0.9686145942474735\n"
     ]
    }
   ],
   "source": [
    "dda_net.train()\n",
    "\n",
    "if is_cuda:\n",
    "    dda_net = dda_net.cuda()\n",
    "\n",
    "for epoch_idx in range(20):\n",
    "    print(f\"epoch: {epoch_idx}\")\n",
    "    loss_sum = 0\n",
    "    xent_loss_sum = 0\n",
    "    cent_loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    for batch_idx, (X, y) in enumerate(dataloader):\n",
    "        if is_cuda:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        feats, outs  = dda_net(X)\n",
    "        loss_xent = criterion_xent(outs, y)\n",
    "        loss_cent = criterion_cent(feats, y)\n",
    "        loss_cent *= weight_cent\n",
    "        loss = loss_xent + loss_cent\n",
    "\n",
    "        optimizer_model.zero_grad()\n",
    "        optimizer_centloss.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_model.step()\n",
    "        for param in criterion_cent.parameters():                                                                                 \n",
    "            param.grad.data *= (1. / weight_cent)                                                                            \n",
    "        optimizer_centloss.step()                                                                                                 \n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        xent_loss_sum += loss_xent.item()\n",
    "        cent_loss_sum += loss_cent.item()\n",
    "        n_corrects += torch.sum(torch.eq(torch.argmax(outs, dim=1), y))\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Batch {}/{}\\t Loss {:.6f} XentLoss {:.6f} CenterLoss {:.6f}\" \\\n",
    "                  .format(batch_idx+1, len(dataloader), loss_sum /(batch_idx+1), \n",
    "                        xent_loss_sum/(batch_idx+1), \n",
    "                        cent_loss_sum/(batch_idx+1))\n",
    "                 )\n",
    "\n",
    "    acc = n_corrects.item() / embeds.shape[0]\n",
    "    print(f\"loss: {loss_sum}, acc:{acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dda_net.state_dict(), open(\"temp_dda_net.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting new embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dda_net.load_state_dict(torch.load(\"temp_dda_net.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=64, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeds = []\n",
    "for (X, y) in test_dataloader:\n",
    "        if is_cuda:\n",
    "            X = X.cuda()\n",
    "        new_embed = dda_net.embed(X)\n",
    "        new_embeds += [new_embed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embed_tensor = torch.cat(new_embeds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "sim_matrix = F.cosine_similarity(                                                                                                                                                                                                                                    \n",
    "     new_embed_tensor.cpu().unsqueeze(1), new_embed_tensor.cpu().unsqueeze(0), dim=2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]                                                                                                                                                                                                         \n",
    "score_vector = sim_matrix[cord].detach().numpy()                                                                                                                                                                                                                              \n",
    "label_vector = np.array(trial.label)                                                                                                                                                                                                                                 \n",
    "fpr, tpr, thres = roc_curve(                                                                                                                                                                                                                                         \n",
    "     label_vector, score_vector, pos_label=1)                                                                                                                                                                                                                     \n",
    "eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.064263656692578"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
