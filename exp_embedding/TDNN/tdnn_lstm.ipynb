{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\", \n",
    "                              data_folder=\"/dataset/SV_sets/voxceleb12/feats/fbank64_vad/\",\n",
    "                              input_frames=400, splice_frames=[200, 400], stride_frames=1, \n",
    "                              input_format='fbank', input_dim=65, random_clip=True,\n",
    "                              n_epochs=200, lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False, batch_size=128,\n",
    "                              gpu_no=[0], cuda=True, num_workers=4,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.feat_dataset import FeatDataset\n",
    "dev_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_dev.csv\")\n",
    "dev_df['label'] = dev_df.groupby(\"spk\").ngroup()\n",
    "dev_train_df = dev_df[dev_df.set == 'train']\n",
    "dev_val_df = dev_df[dev_df.set == 'val']\n",
    "eval_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_eval.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train_dataset = FeatDataset.read_df(config, dev_train_df, 'train')\n",
    "dev_val_dataset = FeatDataset.read_df(config, dev_val_df, 'test')\n",
    "eval_dataset = FeatDataset.read_df(config, eval_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_default_loader \n",
    "dev_train_dataloader = init_default_loader(config, dev_train_dataset, shuffle=True, var_len=False) \n",
    "dev_val_dataloader = init_default_loader(config, dev_val_dataset, shuffle=False, var_len=False) \n",
    "eval_dataloader = init_default_loader(config, eval_dataset, shuffle=False, var_len=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnn_models import tdnn_xvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = tdnn_xvector(config, 512, n_labels=len(dev_df.label.unique()))\n",
    "# saved_model = torch.load(\"trained_models/vox1_dev_tdnn_xvector_held_out.pt\")\n",
    "# model.load_state_dict(saved_model)\n",
    "\n",
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(\"logs/self_paced\")\n",
    "model_path = \"trained_models/voxc1_dev_tdnn_xvector_spl.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/549\t Loss 0.009126\n",
      "Batch 200/549\t Loss 0.009036\n",
      "Batch 300/549\t Loss 0.008867\n",
      "Batch 400/549\t Loss 0.008558\n",
      "Batch 500/549\t Loss 0.008340\n",
      "epoch #0, train loss: 0.0082, train acc: 0.5083\n",
      "sel_samples/total: 46719/70229\n",
      "epoch #0, val loss: 0.0612, val acc: 0.1934\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.004962\n",
      "Batch 200/549\t Loss 0.004873\n",
      "Batch 300/549\t Loss 0.004801\n",
      "Batch 400/549\t Loss 0.004759\n",
      "Batch 500/549\t Loss 0.004725\n",
      "epoch #1, train loss: 0.0047, train acc: 0.5794\n",
      "sel_samples/total: 46829/70229\n",
      "epoch #1, val loss: 0.0599, val acc: 0.2251\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.002890\n",
      "Batch 200/549\t Loss 0.002799\n",
      "Batch 300/549\t Loss 0.002743\n",
      "Batch 400/549\t Loss 0.002743\n",
      "Batch 500/549\t Loss 0.002752\n",
      "epoch #2, train loss: 0.0027, train acc: 0.6207\n",
      "sel_samples/total: 46871/70229\n",
      "epoch #2, val loss: 0.0593, val acc: 0.2817\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.001574\n",
      "Batch 200/549\t Loss 0.001629\n",
      "Batch 300/549\t Loss 0.001632\n",
      "Batch 400/549\t Loss 0.001623\n",
      "Batch 500/549\t Loss 0.001625\n",
      "epoch #3, train loss: 0.0016, train acc: 0.6441\n",
      "sel_samples/total: 46900/70229\n",
      "epoch #3, val loss: 0.0630, val acc: 0.2698\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.001045\n",
      "Batch 200/549\t Loss 0.000986\n",
      "Batch 300/549\t Loss 0.001007\n",
      "Batch 400/549\t Loss 0.001014\n",
      "Batch 500/549\t Loss 0.001017\n",
      "epoch #4, train loss: 0.0010, train acc: 0.6551\n",
      "sel_samples/total: 46916/70229\n",
      "epoch #4, val loss: 0.0612, val acc: 0.3251\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000677\n",
      "Batch 200/549\t Loss 0.000668\n",
      "Batch 300/549\t Loss 0.000659\n",
      "Batch 400/549\t Loss 0.000671\n",
      "Batch 500/549\t Loss 0.000688\n",
      "epoch #5, train loss: 0.0007, train acc: 0.6602\n",
      "sel_samples/total: 46927/70229\n",
      "epoch #5, val loss: 0.0628, val acc: 0.3289\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000480\n",
      "Batch 200/549\t Loss 0.000471\n",
      "Batch 300/549\t Loss 0.000479\n",
      "Batch 400/549\t Loss 0.000477\n",
      "Batch 500/549\t Loss 0.000471\n",
      "epoch #6, train loss: 0.0005, train acc: 0.6636\n",
      "sel_samples/total: 46935/70229\n",
      "epoch #6, val loss: 0.0592, val acc: 0.3540\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000359\n",
      "Batch 200/549\t Loss 0.000357\n",
      "Batch 300/549\t Loss 0.000353\n",
      "Batch 400/549\t Loss 0.000353\n",
      "Batch 500/549\t Loss 0.000350\n",
      "epoch #7, train loss: 0.0004, train acc: 0.6656\n",
      "sel_samples/total: 46937/70229\n",
      "epoch #7, val loss: 0.0582, val acc: 0.3787\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000261\n",
      "Batch 200/549\t Loss 0.000260\n",
      "Batch 300/549\t Loss 0.000255\n",
      "Batch 400/549\t Loss 0.000255\n",
      "Batch 500/549\t Loss 0.000259\n",
      "epoch #8, train loss: 0.0003, train acc: 0.6665\n",
      "sel_samples/total: 46940/70229\n",
      "epoch #8, val loss: 0.0594, val acc: 0.3769\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000204\n",
      "Batch 200/549\t Loss 0.000200\n",
      "Batch 300/549\t Loss 0.000200\n",
      "Batch 400/549\t Loss 0.000202\n",
      "Batch 500/549\t Loss 0.000205\n",
      "epoch #9, train loss: 0.0002, train acc: 0.6671\n",
      "sel_samples/total: 46944/70229\n",
      "epoch #9, val loss: 0.0592, val acc: 0.3773\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000181\n",
      "Batch 200/549\t Loss 0.000178\n",
      "Batch 300/549\t Loss 0.000179\n",
      "Batch 400/549\t Loss 0.000178\n",
      "Batch 500/549\t Loss 0.000180\n",
      "epoch #10, train loss: 0.0002, train acc: 0.6673\n",
      "sel_samples/total: 46946/70229\n",
      "epoch #10, val loss: 0.0587, val acc: 0.3838\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000157\n",
      "Batch 200/549\t Loss 0.000155\n",
      "Batch 300/549\t Loss 0.000155\n",
      "Batch 400/549\t Loss 0.000149\n",
      "Batch 500/549\t Loss 0.000151\n",
      "epoch #11, train loss: 0.0002, train acc: 0.6678\n",
      "sel_samples/total: 46952/70229\n",
      "epoch #11, val loss: 0.0580, val acc: 0.3932\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000127\n",
      "Batch 200/549\t Loss 0.000119\n",
      "Batch 300/549\t Loss 0.000117\n",
      "Batch 400/549\t Loss 0.000120\n",
      "Batch 500/549\t Loss 0.000125\n",
      "epoch #12, train loss: 0.0001, train acc: 0.6680\n",
      "sel_samples/total: 46952/70229\n",
      "epoch #12, val loss: 0.0576, val acc: 0.3951\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "Batch 100/549\t Loss 0.000124\n",
      "Batch 200/549\t Loss 0.000116\n",
      "Batch 300/549\t Loss 0.000123\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "from sklearn.metrics import roc_curve\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "step_scheduler = MultiStepLR(optimizer, [30], 0.1)\n",
    "\n",
    "k = 7\n",
    "\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    \n",
    "    n_sel_samples = 0\n",
    "# =============== train code #===============\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        ### spl masking\n",
    "        spl_mask = loss.lt(k)\n",
    "        masked_loss = torch.masked_select(loss, spl_mask).mean()\n",
    "        n_sel_samples += spl_mask.sum().item()\n",
    "        masked_loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += masked_loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "                  .format(batch_idx+1, len(dev_train_dataloader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "    plateau_scheduler.step(train_loss)\n",
    "    \n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))\n",
    "    writer.add_scalar(\"train/loss\", train_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"train/acc\", train_acc, epoch_idx+1)\n",
    "    \n",
    "    k = k/1.01\n",
    "    print(\"sel_samples/total: {}/{}\".format(n_sel_samples, total))\n",
    "\n",
    "#=============== dev_val code #===============\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss_sum += loss.mean().item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    val_loss = loss_sum / total\n",
    "    val_acc = n_corrects / total\n",
    "    \n",
    "    \n",
    "    print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))\n",
    "    writer.add_scalar(\"val/loss\", val_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"val/acc\", val_acc, epoch_idx+1)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, Fr features\n",
    "fr_feats = []\n",
    "model.eval()\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    fr_feat = model.fr_feat(X).cpu().detach()\n",
    "    fr_feats.append(fr_feat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Freeze Model & FineTurne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "freezed_params = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param_name not in ['classifier.2.weight', 'classifier.2.bias']:\n",
    "        freezed_params.append(param)\n",
    "        \n",
    "for param in freezed_params:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #0, train loss: 0.0219, train acc: 0.9818\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #1, train loss: 0.0217, train acc: 0.9818\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #2, train loss: 0.0214, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #3, train loss: 0.0210, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #4, train loss: 0.0206, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #5, train loss: 0.0202, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #6, train loss: 0.0198, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #7, train loss: 0.0194, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #8, train loss: 0.0190, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #9, train loss: 0.0186, train acc: 0.9848\n"
     ]
    }
   ],
   "source": [
    "# =============== fine_tune code #===============\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "for epoch_idx in range(0, 10):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(held_out_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    #         if (batch_idx+1) % 1000 == 0:\n",
    "    #             print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "    #                   .format(batch_idx+1, len(si_loader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "\n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73438e0851f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#=============== dev_val code #===============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#=============== dev_val code #===============\n",
    "model.eval()\n",
    "loss_sum = 0\n",
    "n_corrects = 0\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    logit = model(X)\n",
    "    loss = criterion(logit, y)\n",
    "    loss_sum += loss.item()\n",
    "    n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "    total += y.size(0)\n",
    "val_loss = loss_sum / total\n",
    "val_acc = n_corrects / total\n",
    "print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "global_mean = si_embeds.mean(0)\n",
    "clf = LDA(solver='svd', n_components=200)\n",
    "clf.fit(si_embeds - global_mean, si_key_df.label)\n",
    "\n",
    "si_embeds = clf.transform(si_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "sv_embeds = clf.transform(sv_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "si_dataset, embed_dim, n_labels = embedToDataset(si_embeds.reshape(-1,200), si_key_df)\n",
    "sv_dataset, _, _ = embedToDataset(sv_embeds, sv_key_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
