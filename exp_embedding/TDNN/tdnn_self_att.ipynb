{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\", \n",
    "                              data_folder=\"/dataset/SV_sets/voxceleb12/feats/fbank64_vad/\",\n",
    "                              input_frames=400, splice_frames=[200, 400], stride_frames=1, \n",
    "                              input_format='fbank', input_dim=65, random_clip=True,\n",
    "                              n_epochs=120, lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False, batch_size=64,\n",
    "                              gpu_no=[0], cuda=True, num_workers=4,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_dev.csv\")\n",
    "spk_counts = dev_df.spk.value_counts()\n",
    "small_set_spks = spk_counts[:300].index\n",
    "dev_df = dev_df[dev_df.spk.isin(small_set_spks)]\n",
    "dev_df['label'] = dev_df.groupby('spk').ngroup()\n",
    "dev_train_df = dev_df[dev_df.set == 'train']\n",
    "dev_val_df = dev_df[dev_df.set == 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_eval.csv\")\n",
    "eval_df = eval_df[eval_df.spk.isin(small_set_spks)]\n",
    "eval_df['label'] = eval_df.groupby('spk').ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.feat_dataset import FeatDataset\n",
    "\n",
    "dev_train_dataset = FeatDataset.read_df(config, dev_train_df, 'train')\n",
    "dev_val_dataset = FeatDataset.read_df(config, dev_val_df, 'test')\n",
    "eval_dataset = FeatDataset.read_df(config, eval_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_default_loader \n",
    "dev_train_dataloader = init_default_loader(config, dev_train_dataset, shuffle=True, var_len=False) \n",
    "dev_val_dataloader = init_default_loader(config, dev_val_dataset, shuffle=False, var_len=False) \n",
    "eval_dataloader = init_default_loader(config, eval_dataset, shuffle=False, var_len=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tdnn_models import tdnn_xvector_se\n",
    "from tdnn_models import tdnn_xvector\n",
    "\n",
    "model = tdnn_xvector_se(config, 512, n_labels=len(dev_df.label.unique()))\n",
    "# saved_model = torch.load(\"trained_models/voxc1_small_dev_time_reduction.pt\")\n",
    "# model.load_state_dict(saved_model)\n",
    "\n",
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #0, train loss: 0.0780, train acc: 0.0585\n",
      "epoch #0, val loss: 0.0713, val acc: 0.0634\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #1, train loss: 0.0553, train acc: 0.2131\n",
      "epoch #1, val loss: 0.0566, val acc: 0.1740\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #2, train loss: 0.0381, train acc: 0.4275\n",
      "epoch #2, val loss: 0.0450, val acc: 0.3078\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #3, train loss: 0.0252, train acc: 0.6113\n",
      "epoch #3, val loss: 0.0381, val acc: 0.4025\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #4, train loss: 0.0165, train acc: 0.7445\n",
      "epoch #4, val loss: 0.0328, val acc: 0.4844\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #5, train loss: 0.0107, train acc: 0.8340\n",
      "epoch #5, val loss: 0.0260, val acc: 0.5890\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #6, train loss: 0.0072, train acc: 0.8890\n",
      "epoch #6, val loss: 0.0229, val acc: 0.6399\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #7, train loss: 0.0051, train acc: 0.9252\n",
      "epoch #7, val loss: 0.0224, val acc: 0.6410\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #8, train loss: 0.0035, train acc: 0.9498\n",
      "epoch #8, val loss: 0.0196, val acc: 0.6831\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #9, train loss: 0.0025, train acc: 0.9655\n",
      "epoch #9, val loss: 0.0195, val acc: 0.6927\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #10, train loss: 0.0020, train acc: 0.9740\n",
      "epoch #10, val loss: 0.0176, val acc: 0.7300\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #11, train loss: 0.0014, train acc: 0.9823\n",
      "epoch #11, val loss: 0.0165, val acc: 0.7379\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #12, train loss: 0.0012, train acc: 0.9859\n",
      "epoch #12, val loss: 0.0175, val acc: 0.7246\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #13, train loss: 0.0010, train acc: 0.9895\n",
      "epoch #13, val loss: 0.0165, val acc: 0.7399\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #14, train loss: 0.0008, train acc: 0.9914\n",
      "epoch #14, val loss: 0.0152, val acc: 0.7587\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #15, train loss: 0.0006, train acc: 0.9937\n",
      "epoch #15, val loss: 0.0145, val acc: 0.7695\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #16, train loss: 0.0006, train acc: 0.9941\n",
      "epoch #16, val loss: 0.0144, val acc: 0.7666\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #17, train loss: 0.0005, train acc: 0.9957\n",
      "epoch #17, val loss: 0.0146, val acc: 0.7709\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #18, train loss: 0.0005, train acc: 0.9961\n",
      "epoch #18, val loss: 0.0144, val acc: 0.7771\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #19, train loss: 0.0004, train acc: 0.9974\n",
      "epoch #19, val loss: 0.0133, val acc: 0.7899\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #20, train loss: 0.0003, train acc: 0.9977\n",
      "epoch #20, val loss: 0.0136, val acc: 0.7823\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #21, train loss: 0.0003, train acc: 0.9986\n",
      "epoch #21, val loss: 0.0132, val acc: 0.7959\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #22, train loss: 0.0003, train acc: 0.9983\n",
      "epoch #22, val loss: 0.0129, val acc: 0.7936\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #23, train loss: 0.0002, train acc: 0.9988\n",
      "epoch #23, val loss: 0.0128, val acc: 0.7973\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #24, train loss: 0.0002, train acc: 0.9990\n",
      "epoch #24, val loss: 0.0127, val acc: 0.8064\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #25, train loss: 0.0002, train acc: 0.9989\n",
      "epoch #25, val loss: 0.0135, val acc: 0.7879\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #26, train loss: 0.0002, train acc: 0.9989\n",
      "epoch #26, val loss: 0.0128, val acc: 0.7945\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #27, train loss: 0.0002, train acc: 0.9994\n",
      "epoch #27, val loss: 0.0129, val acc: 0.7976\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #28, train loss: 0.0002, train acc: 0.9993\n",
      "epoch #28, val loss: 0.0125, val acc: 0.8024\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #29, train loss: 0.0001, train acc: 0.9996\n",
      "epoch #29, val loss: 0.0121, val acc: 0.8104\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #30, train loss: 0.0001, train acc: 0.9997\n",
      "epoch #30, val loss: 0.0120, val acc: 0.8050\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #31, train loss: 0.0001, train acc: 0.9993\n",
      "epoch #31, val loss: 0.0119, val acc: 0.8084\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #32, train loss: 0.0001, train acc: 0.9995\n",
      "epoch #32, val loss: 0.0119, val acc: 0.8124\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #33, train loss: 0.0001, train acc: 0.9997\n",
      "epoch #33, val loss: 0.0119, val acc: 0.8070\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #34, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #34, val loss: 0.0118, val acc: 0.8050\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #35, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #35, val loss: 0.0116, val acc: 0.8118\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #36, train loss: 0.0001, train acc: 0.9997\n",
      "epoch #36, val loss: 0.0116, val acc: 0.8132\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #37, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #37, val loss: 0.0117, val acc: 0.8135\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #38, train loss: 0.0001, train acc: 0.9997\n",
      "epoch #38, val loss: 0.0122, val acc: 0.8081\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #39, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #39, val loss: 0.0113, val acc: 0.8235\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #40, train loss: 0.0001, train acc: 0.9997\n",
      "epoch #40, val loss: 0.0116, val acc: 0.8201\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #41, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #41, val loss: 0.0116, val acc: 0.8147\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #42, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #42, val loss: 0.0114, val acc: 0.8147\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #43, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #43, val loss: 0.0110, val acc: 0.8223\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #44, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #44, val loss: 0.0112, val acc: 0.8172\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #45, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #45, val loss: 0.0113, val acc: 0.8178\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #46, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #46, val loss: 0.0111, val acc: 0.8189\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #47, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #47, val loss: 0.0108, val acc: 0.8283\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #48, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #48, val loss: 0.0108, val acc: 0.8286\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #49, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #49, val loss: 0.0110, val acc: 0.8240\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #50, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #50, val loss: 0.0108, val acc: 0.8263\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #51, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #51, val loss: 0.0108, val acc: 0.8269\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #52, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #52, val loss: 0.0107, val acc: 0.8320\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #53, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #53, val loss: 0.0107, val acc: 0.8303\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #54, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #54, val loss: 0.0109, val acc: 0.8269\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #55, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #55, val loss: 0.0108, val acc: 0.8275\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #56, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #56, val loss: 0.0108, val acc: 0.8294\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #57, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #57, val loss: 0.0106, val acc: 0.8297\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #58, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #58, val loss: 0.0108, val acc: 0.8326\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #59, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #59, val loss: 0.0106, val acc: 0.8312\n",
      "------------------------------\n",
      "curr_lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-687:\n",
      "Process Process-688:\n",
      "Process Process-685:\n",
      "Process Process-686:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"../sv_system/data/feat_dataset.py\", line 78, in __getitem__\n",
      "    return (self.preprocess(os.path.join(self.data_folder, self.files[index])),\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"../sv_system/data/feat_dataset.py\", line 59, in preprocess\n",
      "    data = torch.from_numpy(data).unsqueeze(0).float()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9cf568745aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mn_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "from sklearn.metrics import roc_curve\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "from fine_tune_utils import class_weight\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weight(config, dev_train_df))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "# step_scheduler = MultiStepLR(optimizer, [30], 0.1)\n",
    "\n",
    "writer = SummaryWriter(\"logs/tdnn_xvector_vox1_small_baseline\")\n",
    "model_path = \"trained_models/voxc1_small_dev_tdnn_xvector.pt\"\n",
    "\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    \n",
    "# =============== train code #===============\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "#         if (batch_idx+1) % 1000 == 0:\n",
    "#             print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "#                   .format(batch_idx+1, len(si_loader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "    plateau_scheduler.step(train_loss)\n",
    "    \n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))\n",
    "    writer.add_scalar(\"train/loss\", train_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"train/acc\", train_acc, epoch_idx+1)\n",
    "\n",
    "#=============== dev_val code #===============\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    val_loss = loss_sum / total\n",
    "    val_acc = n_corrects / total\n",
    "    \n",
    "    print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))\n",
    "    writer.add_scalar(\"val/loss\", val_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"val/acc\", val_acc, epoch_idx+1)\n",
    "    \n",
    "#=============== model save #===============\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"../sv_system/saved_models/best_models/voxc2/voxc2_fbank64_untied_model.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c92aa2d2d8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mn_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "## =============== dev_val code #===============\n",
    "model.eval()\n",
    "loss_sum = 0\n",
    "n_corrects = 0\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    logit = model(X)\n",
    "    loss = criterion(logit, y)\n",
    "    loss_sum += loss.item()\n",
    "    n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "    total += y.size(0)\n",
    "val_loss = loss_sum / total\n",
    "val_acc = n_corrects / total\n",
    "\n",
    "print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, Attention map\n",
    "att_maps = []\n",
    "model.eval()\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_train_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    att_map = model.att_map(X).cpu().detach().numpy()\n",
    "    att_maps.append(att_map)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, Fr features\n",
    "fr_feats = []\n",
    "model.eval()\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    fr_feat = model.fr_feat(X).cpu().detach()\n",
    "    fr_feats.append(fr_feat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4115, 0.4619, 0.2880, 0.1456, 0.2449, 0.2377, 0.1846, 0.2711, 0.3090,\n",
       "        0.2152, 0.3659, 0.2667, 0.3660, 0.3011, 0.2163, 0.4169, 0.4812, 0.4101,\n",
       "        0.3309, 0.3308, 0.3580, 0.2597, 0.3758, 0.0898, 0.4742, 0.2708, 0.1194,\n",
       "        0.3323, 0.1286, 0.2033, 0.1993, 0.2460, 0.2657, 0.3245, 0.3804, 0.3026,\n",
       "        0.1827, 0.2627, 0.2961, 0.2626, 0.3103, 0.3536, 0.5638, 0.2204, 0.3099,\n",
       "        0.2129])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity as cosine\n",
    "cosine(fr_feats[0][0,:,0:1], fr_feats[0][0,:,1:], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 47])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_feats[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  1.,  0.,  0.,  0.,  0.,  0.,  0., 18., 26.]),\n",
       " array([0.00847906, 0.00992662, 0.01137418, 0.01282174, 0.0142693 ,\n",
       "        0.01571687, 0.01716443, 0.01861199, 0.02005955, 0.02150711,\n",
       "        0.02295467], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADbRJREFUeJzt3X+sZPVdxvH3UxZoC61S9nazUOilBE1WE5d6Q9D2DypFKY1CrTGQ2GKsWaqSFO0/29ZEtCRSU1rTYDDbQEpNS0sthDVgLVkxiFZ0F1dYfv/aVtYteylJgTa2Ah//mLPkdr1379w7Z3Zmv75fyeSeOfM9c57MnfPcs+ecmU1VIUk6/L1q0gEkSf2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNWHMoV7Z27dqanZ09lKuUpMPejh07nqmqmeXGHdJCn52dZfv27YdylZJ02EvyzWHGechFkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIacUg/KSpJkzS7+daJrXv3le8e+zrcQ5ekRljoktQIC12SGrFsoSc5KckdSR5Icn+SD3XzL0+yJ8nO7nbe+ONKkpYyzEnRF4EPV9U9SV4H7Ehye/fYp6vqk+OLJ0ka1rKFXlV7gb3d9PNJHgROHHcwSdLKrOgYepJZ4HTg7m7WpUnuTXJdkuN6ziZJWoGhCz3JscBXgcuq6jngGuBUYCODPfirllhuU5LtSbbPz8/3EFmStJihCj3JkQzK/AtVdRNAVT1dVS9V1cvAZ4EzFlu2qrZU1VxVzc3MLPtf4kmSVmmYq1wCXAs8WFWfWjB//YJh7wF29R9PkjSsYa5yeRvwPuC+JDu7eR8FLkqyEShgN3DJWBJKkoYyzFUudwFZ5KHb+o8jSVotPykqSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1Ijli30JCcluSPJA0nuT/Khbv4bktye5NHu53HjjytJWsowe+gvAh+uqg3AmcDvJdkAbAa2VdVpwLbuviRpQpYt9KraW1X3dNPPAw8CJwLnA9d3w64HLhhXSEnS8lZ0DD3JLHA6cDewrqr2dg99G1jXazJJ0ooMXehJjgW+ClxWVc8tfKyqCqglltuUZHuS7fPz8yOFlSQtbahCT3IkgzL/QlXd1M1+Osn67vH1wL7Flq2qLVU1V1VzMzMzfWSWJC1imKtcAlwLPFhVn1rw0Fbg4m76YuCW/uNJkoa1ZogxbwPeB9yXZGc376PAlcCNST4AfBP49fFElCQNY9lCr6q7gCzx8Nn9xpEkrZafFJWkRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUiGG+y0WSejW7+dZJR2iSe+iS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWrEsoWe5Lok+5LsWjDv8iR7kuzsbueNN6YkaTnD7KF/Djh3kfmfrqqN3e22fmNJklZq2UKvqjuBZw9BFknSCEY5hn5pknu7QzLHLTUoyaYk25Nsn5+fH2F1kqSDWW2hXwOcCmwE9gJXLTWwqrZU1VxVzc3MzKxydZKk5ayq0Kvq6ap6qapeBj4LnNFvLEnSSq2q0JOsX3D3PcCupcZKkg6NNcsNSHIDcBawNslTwB8BZyXZCBSwG7hkjBklSUNYttCr6qJFZl87hiySpBH4SVFJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUiGULPcl1SfYl2bVg3huS3J7k0e7nceONKUlazjB76J8Dzj1g3mZgW1WdBmzr7kuSJmjZQq+qO4FnD5h9PnB9N309cEHPuSRJK7TaY+jrqmpvN/1tYF1PeSRJqzTySdGqKqCWejzJpiTbk2yfn58fdXWSpCWsttCfTrIeoPu5b6mBVbWlquaqam5mZmaVq5MkLWe1hb4VuLibvhi4pZ84kqTVGuayxRuAbwA/meSpJB8ArgTOSfIo8M7uviRpgtYsN6CqLlriobN7ziJJGoGfFJWkRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqRFrRlk4yW7geeAl4MWqmusjlCRp5UYq9M47quqZHp5HkjQCD7lIUiNGLfQCvp5kR5JNfQSSJK3OqIdc3l5Ve5K8Ebg9yUNVdefCAV3RbwI4+eSTR1ydJGkpI+2hV9We7uc+4GbgjEXGbKmquaqam5mZGWV1kqSDWHWhJzkmyev2TwO/COzqK5gkaWVGOeSyDrg5yf7n+WJVfa2XVJKkFVt1oVfVE8DP9JhFkjQCL1uUpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGjHKfxJ9SM1uvnVi69595bsntm5JGpZ76JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNeKwuQ59kiZ1DbzXv0taCffQJakRFrokNcJCl6RGjFToSc5N8nCSx5Js7iuUJGnlVl3oSY4A/gJ4F7ABuCjJhr6CSZJWZpQ99DOAx6rqiar6IfAl4Px+YkmSVmqUQj8R+M8F95/q5kmSJmDs16En2QRs6u6+kOThA4asBZ4Zd44eHPKc+cSqFvP17Jc5+/X/Nucqt+f93jzMoFEKfQ9w0oL7b+rm/Yiq2gJsWepJkmyvqrkRchwS5uyXOftlzn4dLjkPNMohl38DTktySpKjgAuBrf3EkiSt1Kr30KvqxSSXAn8HHAFcV1X395ZMkrQiIx1Dr6rbgNtGzLDk4ZgpY85+mbNf5uzX4ZLzR6SqJp1BktQDP/ovSY3ovdCX+zqAJEcn+XL3+N1JZhc89pFu/sNJfmnB/N9Pcn+SXUluSPLqSeVMcnySO5K8kOTqA5b52ST3dct8JkmmLWeS1ya5NclD3Wt65agZx5HzgGW3Jtk1rTmTHJVkS5JHutf1vVOa86Lu/Xlvkq8lWTvBnOck2dHl2ZHkFxYsM03b0aI5x7UdjayqersxODn6OPAW4CjgP4ANB4z5XeAvu+kLgS930xu68UcDp3TPcwSDDys9CbymG3cj8JsTzHkM8Hbgg8DVByzzr8CZQIC/Bd41bTmB1wLv6KaPAv5xGnMuWO5XgS8Cuyb8/jzY7/2PgSu66VcBa6ctJ4PzZfv2ZwP+DLh8gjlPB07opn8a2DOl29GiOcexHfVx63sPfZivAzgfuL6b/mvg7O4v8PnAl6rqB1X1JPBY93wweDO+Jsma7oX8r0nlrKrvVdVdwH8vHJxkPfD6qvqXGvyWPw9cMG05q+r7VXVHN/1D4B4GnyGYqpwASY4F/gC4YsR8Y80J/BbwpwBV9XJVjfqBlHHkTHc7ptveXs9kt6N/r6r967+fwfZ99BRuR4vmHNN2NLK+C32YrwN4ZUxVvQh8Fzh+qWWrag/wSeBbwF7gu1X19QnmPNhzPrXMc05Dzlck+XHgl4FtU5rz48BVwPdHzPd/MnRGztm9hgAfT3JPkq8kWTdtOavqf4DfAe5jUOQbgGunJOd7gXuq6gdM93a0MOcretyORjb1J0WTHMfgr+cpwAkM9jB+Y7KpDn/dv3ZuAD5TVU9MOs+BkmwETq2qmyedZRlrGOyZ/XNVvRX4BoMdkKmS5EgGhX46g+3oXuAjEw0FJPkp4BPAJZPOcjBL5Zy27ajvQh/m6wBeGdO9GD8GfOcgy74TeLKq5ru9jJuAn59gzoM958J/ci36VQhTkHO/LcCjVfXnI2YcV86fA+aS7AbuAn4iyT9MYc7vMPgXxE3d/a8Ab53CnBsBqurx7lDGjUx4O0ryJuBm4P1V9fiC8VO1HS2Rc78+t6OR9V3ow3wdwFbg4m7614C/795gW4ELu+NopwCnMTg58i3gzO6scoCzgQcnmHNRVbUXeC7JmV3O9wO3TFtOgCRXMHjDXjZivrHlrKprquqEqpplcJLvkao6awpzFvA3wP5sZwMPTFtOBoW1IclMd/8cJrgddYcpbgU2V9U/7R88bdvRUjlhLNvR6Po+ywqcBzzC4Kzyx7p5fwL8Sjf9agZ7MY8xKOy3LFj2Y91yD7PgjDGDqwgeAnYBfwUcPeGcu4FngRcYHI/b0M2f6zI+DlxN98GtacrJYO+kGGzMO7vbb09bzgOee5YernIZ4+/9zcCdDA5jbANOntKcH+x+7/cy+CN0/KRyAn8IfG/Be3An8MZp246WysmYtqNRb35SVJIaMfUnRSVJw7HQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqxP8CbsDarkdvCSAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(att_maps[0][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02295467, 0.02295467, 0.02223028, 0.02295467, 0.02295467,\n",
       "       0.02295467, 0.02113651, 0.02113651, 0.02295467, 0.02295467,\n",
       "       0.00886847, 0.01112725, 0.02295467, 0.02295467, 0.02113651,\n",
       "       0.02182527, 0.02295467, 0.02193193, 0.02295467, 0.02113651,\n",
       "       0.02113651, 0.02295467, 0.02113651, 0.02113651, 0.02113651,\n",
       "       0.02295467, 0.02295467, 0.02223028, 0.02113651, 0.02113651,\n",
       "       0.02113651, 0.02113651, 0.02182527, 0.02295467, 0.00847906,\n",
       "       0.02295467, 0.02295467, 0.02193193, 0.02295467, 0.02295467,\n",
       "       0.02295467, 0.02113651, 0.02113651, 0.02113651, 0.02113651,\n",
       "       0.02113651, 0.02113651], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_maps[0][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "global_mean = si_embeds.mean(0)\n",
    "clf = LDA(solver='svd', n_components=200)\n",
    "clf.fit(si_embeds - global_mean, si_key_df.label)\n",
    "\n",
    "si_embeds = clf.transform(si_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "sv_embeds = clf.transform(sv_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "si_dataset, embed_dim, n_labels = embedToDataset(si_embeds.reshape(-1,200), si_key_df)\n",
    "sv_dataset, _, _ = embedToDataset(sv_embeds, sv_key_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
