{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\", \n",
    "                              data_folder=\"/dataset/SV_sets/voxceleb12/feats/fbank64_vad/\",\n",
    "                              input_frames=400, splice_frames=[200, 400], stride_frames=1, \n",
    "                              input_format='fbank', input_dim=65, random_clip=True,\n",
    "                              n_epochs=200, lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False, batch_size=64,\n",
    "                              gpu_no=[0], cuda=True, num_workers=4,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "held_out_spks = dev_df.spk.value_counts()[-10:].index\n",
    "held_out_df = dev_df[dev_df.spk.isin(held_out_spks)]\n",
    "held_out_train_df = held_out_df[held_out_df.set == 'train']\n",
    "held_out_val_df = held_out_df[held_out_df.set == 'val']\n",
    "held_out_train_dataset = FeatDataset.read_df(config, held_out_train_df, 'train')\n",
    "held_out_val_dataset = FeatDataset.read_df(config, held_out_val_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = dev_df[~dev_df.spk.isin(held_out_spks)]\n",
    "dev_train_df = dev_df[dev_df.set == 'train']\n",
    "dev_val_df = dev_df[dev_df.set == 'val']\n",
    "eval_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_eval.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.feat_dataset import FeatDataset\n",
    "\n",
    "dev_train_dataset = FeatDataset.read_df(config, dev_train_df, 'train')\n",
    "dev_val_dataset = FeatDataset.read_df(config, dev_val_df, 'test')\n",
    "eval_dataset = FeatDataset.read_df(config, eval_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_default_loader \n",
    "dev_train_dataloader = init_default_loader(config, dev_train_dataset, shuffle=True, var_len=False) \n",
    "dev_val_dataloader = init_default_loader(config, dev_val_dataset, shuffle=False, var_len=True) \n",
    "eval_dataloader = init_default_loader(config, eval_dataset, shuffle=False, var_len=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "held_out_train_dataloader = init_default_loader(config, held_out_train_dataset, shuffle=False, var_len=False)\n",
    "held_out_val_dataloader = init_default_loader(config, held_out_val_dataset, shuffle=False, var_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnn_models import tdnn_xvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fintune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = tdnn_xvector(config, 512, n_labels=len(dev_df.label.unique())+10)\n",
    "# saved_model = torch.load(\"trained_models/vox1_dev_tdnn_xvector_held_out.pt\")\n",
    "# model.load_state_dict(saved_model)\n",
    "\n",
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "from sklearn.metrics import roc_curve\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "# criterion = nn.CrossEntropyLoss(weight=label_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "# step_scheduler = MultiStepLR(optimizer, [30], 0.1)\n",
    "\n",
    "writer = SummaryWriter(\"logs/held_out\")\n",
    "model_path = \"trained_models/vox1_dev_tdnn_xvector_held_out.pt\"\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch_idx in range(start_epoch, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    \n",
    "# =============== train code #===============\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "#         if (batch_idx+1) % 1000 == 0:\n",
    "#             print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "#                   .format(batch_idx+1, len(si_loader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "    plateau_scheduler.step(train_loss)\n",
    "    \n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))\n",
    "    writer.add_scalar(\"train/loss\", train_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"train/acc\", train_acc, epoch_idx+1)\n",
    "\n",
    "#=============== dev_val code #===============\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    val_loss = loss_sum / total\n",
    "    val_acc = n_corrects / total\n",
    "    \n",
    "    print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))\n",
    "    writer.add_scalar(\"val/loss\", val_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"val/acc\", val_acc, epoch_idx+1)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, Fr features\n",
    "fr_feats = []\n",
    "model.eval()\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    fr_feat = model.fr_feat(X).cpu().detach()\n",
    "    fr_feats.append(fr_feat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9226, 0.7637, 0.5905, 0.4650, 0.3965, 0.3624, 0.3237, 0.2873, 0.2718,\n",
       "        0.2553, 0.2583, 0.2505, 0.2527, 0.2539, 0.2557, 0.2563, 0.2401, 0.2268,\n",
       "        0.2261, 0.2295, 0.2399, 0.2526, 0.2675, 0.2725, 0.2617, 0.2381, 0.2301,\n",
       "        0.2367, 0.2417, 0.2536, 0.2603, 0.2381, 0.2207, 0.1969, 0.1837, 0.1872,\n",
       "        0.1942, 0.2218, 0.2566, 0.2421, 0.2378, 0.2361, 0.2238, 0.2398, 0.2526,\n",
       "        0.2569, 0.2780, 0.2980, 0.2748, 0.2692, 0.2281, 0.1951, 0.1903, 0.1980,\n",
       "        0.2044, 0.2165, 0.2160, 0.1929, 0.1900, 0.2037, 0.2059, 0.1945, 0.2040,\n",
       "        0.2211, 0.2484, 0.2909, 0.3278, 0.3465, 0.3434, 0.3335, 0.3265, 0.3066,\n",
       "        0.2900, 0.2866, 0.3043, 0.3135, 0.3199, 0.3052, 0.2881, 0.2666, 0.2464,\n",
       "        0.2296, 0.2084, 0.1941, 0.1960, 0.1970, 0.2088, 0.2268, 0.2424, 0.2605,\n",
       "        0.2513, 0.2701, 0.2541, 0.2128, 0.1948, 0.2012, 0.2227, 0.2383, 0.2599,\n",
       "        0.2907, 0.3295, 0.3709, 0.4564, 0.4619, 0.3714, 0.3026, 0.2717, 0.2558,\n",
       "        0.2204, 0.2095, 0.1857, 0.1725, 0.1850, 0.1884, 0.2057, 0.2360, 0.2675,\n",
       "        0.2936, 0.3356, 0.3574, 0.3655, 0.3615, 0.3339, 0.3196, 0.3456, 0.3412,\n",
       "        0.3175, 0.2808, 0.2633, 0.2542, 0.2350, 0.2195, 0.2093, 0.2098, 0.2084,\n",
       "        0.2103, 0.2071, 0.2029, 0.2065, 0.2414, 0.2878, 0.3334, 0.3287, 0.3102,\n",
       "        0.2745, 0.2545, 0.2402, 0.2363, 0.2396, 0.2297, 0.2251, 0.2180, 0.2398,\n",
       "        0.2742, 0.3011, 0.3486, 0.4207, 0.4491, 0.4264, 0.3943, 0.3364, 0.2776,\n",
       "        0.2567, 0.2696, 0.2523, 0.2223, 0.2278, 0.2599, 0.2943, 0.3365, 0.3846,\n",
       "        0.4347, 0.4642, 0.4634, 0.3952, 0.3268, 0.2815, 0.2736, 0.2699, 0.2443,\n",
       "        0.2580, 0.2457, 0.2183, 0.1993, 0.1737, 0.1762, 0.1618, 0.1334, 0.1267,\n",
       "        0.1188, 0.1180, 0.1498, 0.1775, 0.2061, 0.2422, 0.2763, 0.3416, 0.3734,\n",
       "        0.3879, 0.3539, 0.2782, 0.2411, 0.2473, 0.2543, 0.2389, 0.2561, 0.2911,\n",
       "        0.3110, 0.2795, 0.2643, 0.2608, 0.2623, 0.2513, 0.2438, 0.2432, 0.2373,\n",
       "        0.2271, 0.2400, 0.2466, 0.2310, 0.2463, 0.2837, 0.2860, 0.2893, 0.3091,\n",
       "        0.3451, 0.3772, 0.4125, 0.4285, 0.4017, 0.3590, 0.3359, 0.2974, 0.2446,\n",
       "        0.2087, 0.2183, 0.2320, 0.2335, 0.2415, 0.2370, 0.2375, 0.2920, 0.3210,\n",
       "        0.3154, 0.2908, 0.2945, 0.2932, 0.2878, 0.2781, 0.2435, 0.2381, 0.2346,\n",
       "        0.2307, 0.2308, 0.2482, 0.2544, 0.2435, 0.2481, 0.2588, 0.2773, 0.2917,\n",
       "        0.3045, 0.3165, 0.3209, 0.3313, 0.3347, 0.3357, 0.3421, 0.3741, 0.4007,\n",
       "        0.4151, 0.4180, 0.4230, 0.4346, 0.4352, 0.4247, 0.4357, 0.4600, 0.4547,\n",
       "        0.4402, 0.4193, 0.4162, 0.4373, 0.4550, 0.4187, 0.3815, 0.3500, 0.3111,\n",
       "        0.2734, 0.2425, 0.2270, 0.2021, 0.1754, 0.1707, 0.1849, 0.2066, 0.2269,\n",
       "        0.2481, 0.2748, 0.2789, 0.2931, 0.2745, 0.2708, 0.2451, 0.2271, 0.2611,\n",
       "        0.3081, 0.3119, 0.3052, 0.2650, 0.2338, 0.2403, 0.2446, 0.2406, 0.2503,\n",
       "        0.2466, 0.2321, 0.2629, 0.3356, 0.4269, 0.3935, 0.3609, 0.3626, 0.3278,\n",
       "        0.2633, 0.2454, 0.2409, 0.2315, 0.2504, 0.2495, 0.3143, 0.4315, 0.5246,\n",
       "        0.5419, 0.5083, 0.4576, 0.3634, 0.2972, 0.2531, 0.2420, 0.2239, 0.2219,\n",
       "        0.2412, 0.2540, 0.2935, 0.3519, 0.3982, 0.4290, 0.4359, 0.3957, 0.3360,\n",
       "        0.2984, 0.2879, 0.2897, 0.2758, 0.2632, 0.2448, 0.2320, 0.2191, 0.2306,\n",
       "        0.2412, 0.2223, 0.2191, 0.2389, 0.2590, 0.2832, 0.2909, 0.2881, 0.3054,\n",
       "        0.3347, 0.3717, 0.4088, 0.4597, 0.4336, 0.3745, 0.3419, 0.3012, 0.2843,\n",
       "        0.2696, 0.2531, 0.2190])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity as cosine\n",
    "cosine(fr_feats[0][0,:,0:1], fr_feats[0][0,:,1:], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Freeze Model & FineTurne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "freezed_params = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param_name not in ['classifier.2.weight', 'classifier.2.bias']:\n",
    "        freezed_params.append(param)\n",
    "        \n",
    "for param in freezed_params:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #0, train loss: 0.0219, train acc: 0.9818\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #1, train loss: 0.0217, train acc: 0.9818\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #2, train loss: 0.0214, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #3, train loss: 0.0210, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #4, train loss: 0.0206, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #5, train loss: 0.0202, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #6, train loss: 0.0198, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #7, train loss: 0.0194, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #8, train loss: 0.0190, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #9, train loss: 0.0186, train acc: 0.9848\n"
     ]
    }
   ],
   "source": [
    "# =============== fine_tune code #===============\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "for epoch_idx in range(0, 10):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(held_out_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    #         if (batch_idx+1) % 1000 == 0:\n",
    "    #             print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "    #                   .format(batch_idx+1, len(si_loader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "\n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #9, val loss: 0.0389, val acc: 0.8519\n"
     ]
    }
   ],
   "source": [
    "#=============== dev_val code #===============\n",
    "model.eval()\n",
    "loss_sum = 0\n",
    "n_corrects = 0\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(held_out_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    logit = model(X)\n",
    "    loss = criterion(logit, y)\n",
    "    loss_sum += loss.item()\n",
    "    n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "    total += y.size(0)\n",
    "val_loss = loss_sum / total\n",
    "val_acc = n_corrects / total\n",
    "print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "global_mean = si_embeds.mean(0)\n",
    "clf = LDA(solver='svd', n_components=200)\n",
    "clf.fit(si_embeds - global_mean, si_key_df.label)\n",
    "\n",
    "si_embeds = clf.transform(si_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "sv_embeds = clf.transform(sv_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "si_dataset, embed_dim, n_labels = embedToDataset(si_embeds.reshape(-1,200), si_key_df)\n",
    "sv_dataset, _, _ = embedToDataset(sv_embeds, sv_key_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
