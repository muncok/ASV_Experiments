{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system_utils import compute_eer\n",
    "from batch_sv_system_utils import compute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79424325, 0.19043324, 0.20102014, 0.72356485, 0.81917568]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3,5)[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_scores(data, size):\n",
    "    c = 0\n",
    "    for i, x in enumerate(data):\n",
    "        if x.shape[0] < size:\n",
    "            c += 1\n",
    "            res = size - x.shape[0]\n",
    "            x = np.concatenate([x, np.repeat(x[-1:], res, 0)])\n",
    "        data[i] = np.sort(x[:size], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = eval_plda_score_list[:5000], eval_plda_labels[:5000]\n",
    "test_x, test_y = eval_plda_score_list[5000:], eval_plda_labels[5000:]\n",
    "\n",
    "prep_scores(train_x, 20)\n",
    "prep_scores(test_x, 20)\n",
    "\n",
    "train_x = np.concatenate(train_x, 1).T\n",
    "test_x = np.concatenate(test_x, 1).T\n",
    "train_y = np.concatenate(train_y, 0)\n",
    "test_y = np.concatenate(test_y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(x, n_feat=5):\n",
    "    min_ = x.min(0)\n",
    "    max_ = x.max(0)\n",
    "    median_ = np.median(x, axis=0)\n",
    "    avg_ = x.mean(0)\n",
    "    std_ = x.std(0)\n",
    "   \n",
    "    if n_feat==2:\n",
    "        return np.stack([avg_, std_], axis=0).T\n",
    "    elif n_feat==3:\n",
    "        return np.stack([max_, avg_, std_], axis=0).T\n",
    "    elif n_feat==4:\n",
    "        return np.stack([min_, max_, avg_, std_], axis=0).T\n",
    "    elif n_feat==5:\n",
    "        return np.stack([min_, max_, median_, avg_, std_], axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_plda_trial_output = pickle.load(\n",
    "    open(\"trials/dev940_eval311/hard_enr3xsess_ntar9/proxy_eT5_fNorm.pkl\", \"rb\"))\n",
    "eval_plda_score_list = [x[0] for x in eval_plda_trial_output]\n",
    "eval_plda_adapt_labels = [x[3] for x in eval_plda_trial_output]\n",
    "eval_plda_labels = [x[2] for x in eval_plda_trial_output]\n",
    "eval_plda_proxy_score_list = [x[1] for x in eval_plda_trial_output]\n",
    "eval_plda_proxy_idx = [x[4][:n_adapt] for x in eval_plda_trial_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_plda_trial_output = pickle.load(\n",
    "    open(\"trials/dev940_eval311/random_enr3xsess_ntar9/proxy_eT5.pkl\", \"rb\"))\n",
    "eval_plda_score_list = [x[0] for x in eval_plda_trial_output]\n",
    "eval_plda_labels = [x[2] for x in eval_plda_trial_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_plda_feats = [get_features(x, 4) for x in eval_plda_score_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = eval_plda_score_list, eval_plda_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = eval_plda_feats[:5000], eval_plda_labels[:5000]\n",
    "test_x, test_y = eval_plda_feats[5000:], eval_plda_labels[5000:]\n",
    "\n",
    "train_x = np.concatenate(train_x, 0)\n",
    "test_x = np.concatenate(test_x, 0)\n",
    "train_y = np.concatenate(train_y, 0)\n",
    "test_y = np.concatenate(test_y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# train_x = torch.from_numpy(train_x).float()\n",
    "# train_y = torch.from_numpy(train_y).float()\n",
    "test_x = torch.from_numpy(test_x).float()\n",
    "test_y = torch.from_numpy(test_y).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class ScoreDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_set = ScoreDataset(train_x, train_y)\n",
    "train_dataloader = DataLoader(train_set, batch_size=512, shuffle=True)\n",
    "test_set = ScoreDataset(test_x, test_y)\n",
    "test_dataloader = DataLoader(test_set, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00256443431135267\n",
      "0.0005374724511458611\n",
      "0.0005023196709140337\n",
      "0.00048698804728573077\n",
      "0.0004970548034179956\n",
      "0.0004432937361527655\n",
      "0.0004416808811451873\n",
      "0.0004400846161665346\n",
      "0.0004091158916708082\n",
      "0.0004310969483095086\n",
      "0.00042914065005194364\n",
      "0.00042882006220599345\n",
      "0.0006994093710090965\n",
      "0.0004253945279670399\n",
      "0.00042417484236732106\n",
      "0.00042395997534008854\n",
      "0.0005710579280275851\n",
      "0.000419642891272645\n",
      "0.0004200523550941033\n",
      "0.00042103232962374134\n",
      "0.0003552038542693481\n",
      "0.00041931306005818904\n",
      "0.00041868793682679994\n",
      "0.0004193486394669664\n",
      "0.00039658238529227674\n",
      "0.00041799595550785155\n",
      "0.0004178625329307089\n",
      "0.0004185890567460893\n",
      "0.0005420547386165708\n",
      "0.0004165800311383879\n",
      "0.0004171398888723662\n",
      "0.00041761283725664155\n",
      "0.0003124524373561144\n",
      "0.0004163402785716604\n",
      "0.0004164229441461136\n",
      "0.0004170103435184378\n",
      "0.00035661413858179003\n",
      "0.00041900254146370316\n",
      "0.00041701416306000615\n",
      "0.0004168971160006393\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = model.cuda()\n",
    "pos_weight_ = (1-train_y.mean())/train_y.mean()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_.cuda())\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5, nesterov=True)\n",
    "\n",
    "model.train()\n",
    "for eporch_i in range(10):\n",
    "    loss_sum = 0\n",
    "    total = 0\n",
    "    for b_idx, (x, y) in enumerate(train_dataloader):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)\n",
    "        loss = criterion(logit.squeeze(), y)\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        total += len(y)\n",
    "        optimizer.step()\n",
    "        if b_idx % 5000 == 1:\n",
    "            print(loss_sum / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_logits = []\n",
    "test_labels = []\n",
    "for b_idx, (x, y) in enumerate(test_dataloader):\n",
    "    x = x.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    logit = model(x)\n",
    "    test_logits.append(logit.cpu().detach().numpy())\n",
    "    test_labels.append(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = np.concatenate(test_logits).reshape(-1)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5653894639840891, 0.5653878862326372, 0.5653910417355409, 139.39949)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_eer(test_scores, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.009668225364954465,\n",
       " 0.009668744621026583,\n",
       " 0.009667706108882346,\n",
       " -3.0254157051025654)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_x, test_y = eval_plda_score_list[5000:], eval_plda_labels[5000:]\n",
    "test_scores = np.concatenate([x.mean(0) for x in test_x])\n",
    "test_y = np.concatenate(test_y, 0)\n",
    "compute_eer(test_scores, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
