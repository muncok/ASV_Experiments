{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system_utils import get_embeds, cosine_sim, compute_error\n",
    "from batch_sv_system_utils import compute_eer\n",
    "from utils import key2df, df2dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id2idx(keys):\n",
    "    key_df = key2df(keys)\n",
    "    id2idx, idx2id = df2dict(key_df) \n",
    "    return id2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(x, n_feat=4):\n",
    "    min_ = x.min(0)\n",
    "    max_ = x.max(0)\n",
    "    median_ = np.median(x, axis=0)\n",
    "    avg_ = x.mean(0)\n",
    "    std_ = x.std(0)\n",
    "   \n",
    "    if n_feat==2:\n",
    "        return np.stack([avg_, std_], axis=0).T\n",
    "    elif n_feat==3:\n",
    "        return np.stack([max_, avg_, std_], axis=0).T\n",
    "    elif n_feat==4:\n",
    "        return np.stack([min_, max_, avg_, std_], axis=0).T\n",
    "    elif n_feat==5:\n",
    "        return np.stack([min_, max_, median_, avg_, std_], axis=0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Adaptation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_embeds/\"\n",
    "# embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_300f_embeds/\"\n",
    "sv_embeds = np.load(embed_dir + \"/sv_embeds.npy\")\n",
    "keys = pickle.load(open(embed_dir + \"/sv_keys.pkl\", \"rb\"))\n",
    "id2idx = get_id2idx(keys)\n",
    "\n",
    "plda_embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_xvector/\"\n",
    "# plda_embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_300f_xvector/\"\n",
    "plda_sv_embeds = np.load(plda_embed_dir + \"/sv_embeds.npy\")\n",
    "plda_model_dir = plda_embed_dir + \"plda_train/\"\n",
    "plda_keys = pickle.load(open(plda_embed_dir + \"/sv_keys.pkl\", \"rb\"))\n",
    "plda_id2idx = get_id2idx(plda_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine + Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online PLDA\n",
    "from batch_sv_system_utils import run_trial, plot_score \n",
    "\n",
    "trial = pickle.load(open(\"trials/dev317_eval934/eval_random_enr10_tar50_n4950_gender/trials.pkl\", \"rb\"))\n",
    "cohort_embeds = np.load(\"trials/dev317_eval934/cohort_embeds.npy\")\n",
    "\n",
    "eval_score_list = []\n",
    "eval_labels = []\n",
    "eval_adapt_labels = []\n",
    "eval_preds = []\n",
    "\n",
    "for t_idx in range(0,len(trial)):\n",
    "    enr_spk, enr_ids, test_trial_df = trial[t_idx] \n",
    "    n_target= test_trial_df.label.value_counts()[1]\n",
    "    n_nonTarget = n_target\n",
    "    test_trial_df = pd.concat([test_trial_df[test_trial_df.label==1], \n",
    "                               test_trial_df[test_trial_df.label==0].sample(n=n_nonTarget)])\n",
    "    test_trial_df = test_trial_df.sample(frac=1.0)\n",
    "    test_trial = (np.array(test_trial_df.id), np.array(test_trial_df.label))\n",
    "    \n",
    "    ### get embeds\n",
    "    init_enr_embeds = get_embeds(enr_ids[:10], sv_embeds, id2idx, norm=True)\n",
    "    test_embeds = get_embeds(test_trial[0], sv_embeds, id2idx, norm=True)\n",
    "\n",
    "    ### adapt trial\n",
    "    eT = 15.0\n",
    "    adapt_scores = cosine_sim(init_enr_embeds, test_embeds)\n",
    "    \n",
    "    enr_cohort_scores = cosine_sim(init_enr_embeds, cohort_embeds)\n",
    "    enr_mean = enr_cohort_scores.mean(1, keepdims=True)\n",
    "    enr_std = enr_cohort_scores.std(1, keepdims=True)\n",
    "    test_cohort_scores = cosine_sim(test_embeds, cohort_embeds)\n",
    "    test_mean = test_cohort_scores.mean(1, keepdims=True).T\n",
    "    test_std = test_cohort_scores.std(1, keepdims=True).T\n",
    "    adapt_norm_scores = ((adapt_scores - enr_mean)/enr_std + (adapt_scores - test_mean)/test_std)/2\n",
    "    \n",
    "    adapt_enr_idx = np.nonzero(adapt_norm_scores.mean(0) > eT)[0]\n",
    "    adapt_enr_acc = np.array(test_trial[1])[adapt_enr_idx].mean()\n",
    "    print(\"[eT:{}] n_adapted:{}(of {}), adapt_acc:{:.3f}\".format(eT, len(adapt_enr_idx), test_trial[1].sum(), \n",
    "                                                                        adapt_enr_acc))\n",
    "    enr_embeds = np.concatenate([init_enr_embeds, test_embeds[adapt_enr_idx]], axis=0)\n",
    "    \n",
    "    ### test trial\n",
    "    test_scores = cosine_sim(enr_embeds, test_embeds)\n",
    "    enr_cohort_scores = cosine_sim(enr_embeds, cohort_embeds)\n",
    "    enr_mean = enr_cohort_scores.mean(1, keepdims=True)\n",
    "    enr_std = enr_cohort_scores.std(1, keepdims=True)\n",
    "    test_cohort_scores = cosine_sim(test_embeds, cohort_embeds)\n",
    "    test_mean = test_cohort_scores.mean(1, keepdims=True).T\n",
    "    test_std = test_cohort_scores.std(1, keepdims=True).T\n",
    "    test_norm_scores = ((test_scores - enr_mean)/enr_std + (test_scores - test_mean)/test_std)/2\n",
    "    \n",
    "    ### reduce to online sv scores\n",
    "    online_scores = []\n",
    "    prev_t = 0\n",
    "    adapt_times = np.append(adapt_enr_idx, len(test_trial[1]))\n",
    "    for n_enr, adapt_t in enumerate(adapt_times):\n",
    "        online_scores.append(test_norm_scores[:n_enr+1, prev_t:adapt_t+1])\n",
    "        prev_t = adapt_t+1   \n",
    "        \n",
    "    online_score_fusion = np.concatenate([x.mean(0) for x in online_scores], axis=0)\n",
    "    print(\"adapt eer:{:.4f}\".format(compute_eer(online_score_fusion, test_trial[1])[0]))\n",
    "    \n",
    "    eval_score_list.append(online_scores)\n",
    "    eval_labels.append(test_trial[1])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR train\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "test_adapt_scores = np.concatenate([reduce_scores(x) for x in eval_score_list], axis=0)\n",
    "test_feat = np.concatenate([reduce_scores(x, get_features) for x in eval_score_list], axis=0)\n",
    "test_labels = np.concatenate(eval_labels)\n",
    "lr_clf = pickle.load(open(\"trials/dev317_eval934/dev_random_enr20_spk10_gender/plda_nf4_bal_lr_clf.pkl\", \"rb\"))\n",
    "svm_clf = pickle.load(open(\"trials/dev317_eval934/dev_random_enr20_spk10_gender/plda_nf4_bal_svm_clf.pkl\", \"rb\"))\n",
    "\n",
    "print(compute_eer(test_adapt_scores, test_labels))\n",
    "print(compute_eer(lr_clf.decision_function(test_feat), test_labels))\n",
    "print(compute_eer(svm_clf.decision_function(test_feat), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fusion_eers = []\n",
    "feat_eers = []\n",
    "for scores, label in zip(eval_plda_score_list, eval_plda_labels):\n",
    "    score_fusion = reduce_scores(scores)\n",
    "    feat_score = svm_clf.decision_function(reduce_scores(scores, get_features))\n",
    "    score_fusion_eers.append(compute_eer(score_fusion, label)[0])\n",
    "    feat_eers.append(compute_eer(feat_score, label)[0])\n",
    "\n",
    "eer_diff = np.array(feat_eers) - np.array(score_fusion_eers)\n",
    "\n",
    "n_better = np.count_nonzero(eer_diff < 0)\n",
    "n_worse = np.count_nonzero(eer_diff > 0)\n",
    "n_equal = np.count_nonzero(eer_diff == 0)\n",
    "print(\"n_better:{}, n_worse:{}, n_equal:{}\".format(n_better, n_worse,  n_equal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[eT:15] n_adapted:46(of 60), adapt_acc:1.000\n",
      "adapt eer:0.0083\n"
     ]
    }
   ],
   "source": [
    "# Online PLDA\n",
    "from batch_sv_system_utils import run_trial, plot_score \n",
    "\n",
    "trial = pickle.load(open(\"trials/dev317_eval934/eval_random_enr10_tar50_spk10_gender/trials.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "t_idx = 10\n",
    "enr_spk, enr_ids, test_trial_df = trial[t_idx] \n",
    "n_target= test_trial_df.label.value_counts()[1]\n",
    "n_nonTarget = n_target\n",
    "test_trial_df = pd.concat([test_trial_df[test_trial_df.label==1], \n",
    "                           test_trial_df[test_trial_df.label==0].sample(n=n_nonTarget)])\n",
    "test_trial_df = test_trial_df.sample(frac=1.0)\n",
    "test_trial = (np.array(test_trial_df.id), np.array(test_trial_df.label))\n",
    "\n",
    "### get embeds\n",
    "plda_init_enr_embeds = get_embeds(enr_ids[:3], plda_sv_embeds, plda_id2idx, norm=False)\n",
    "plda_test_embeds = get_embeds(test_trial[0], plda_sv_embeds, plda_id2idx, norm=False)\n",
    "\n",
    "### adapt trial\n",
    "plda_adapt_fusion, plda_adapt_scores = run_trial(plda_init_enr_embeds, plda_test_embeds, test_trial[1],\n",
    "                         plda_dir=plda_model_dir, \n",
    "                         plot=False, title=\"adapt_score_fusion\")\n",
    "# adapt threshold\n",
    "eT = 15\n",
    "\n",
    "adapt_enr_idx = np.nonzero(plda_adapt_fusion > eT)[0]\n",
    "adapt_enr_acc = np.array(test_trial[1])[adapt_enr_idx].mean()\n",
    "print(\"[eT:{}] n_adapted:{}(of {}), adapt_acc:{:.3f}\".format(eT, len(adapt_enr_idx), test_trial[1].sum(), \n",
    "                                                                    adapt_enr_acc))\n",
    "plda_enr_embeds = np.concatenate([plda_init_enr_embeds, plda_test_embeds[adapt_enr_idx]], axis=0)\n",
    "plda_test_score_fusion, plda_test_scores = run_trial(plda_enr_embeds, plda_test_embeds, test_trial[1],\n",
    "                                       plda_dir=plda_model_dir, neg_embeds=None,\n",
    "                                       plot=False, title=\"score_fusion(plda, adapt)\",\n",
    "                                       verbose=False)\n",
    "plda_online_scores = []\n",
    "prev_t = 0\n",
    "adapt_times = np.append(adapt_enr_idx, len(test_trial[1]))\n",
    "for n_enr, adapt_t in enumerate(adapt_times):\n",
    "    plda_online_scores.append(plda_test_scores[:n_enr+1, prev_t:adapt_t+1])\n",
    "    prev_t = adapt_t+1   \n",
    "\n",
    "plda_online_score_fusion = np.concatenate([x.mean(0) for x in plda_online_scores], axis=0)\n",
    "print(\"adapt eer:{:.4f}\".format(compute_eer(plda_online_score_fusion, test_trial[1])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_confids = plda_adapt_scores.mean(0)[adapt_enr_idx]\n",
    "sorted_adapt_confid_idx = np.argsort(adapt_confids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n",
      "0.00833333333333336\n"
     ]
    }
   ],
   "source": [
    "step = 3\n",
    "for idx in range(3, len(sorted_adapt_confid_idx), step):\n",
    "    print(compute_eer(plda_test_scores[sorted_adapt_confid_idx[:idx+3]].mean(0), test_trial[1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_plda_trial_output = pickle.load(open(\"tmp/eT15_enr10_score.pkl\", \"rb\"))\n",
    "eval_plda_score_list = [x[0] for x in eval_plda_trial_output]\n",
    "eval_plda_labels = [x[1] for x in eval_plda_trial_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_scores(score_list, fusion_type=\"avg\", feat_fn=None):\n",
    "    if not feat_fn:\n",
    "        if fusion_type ==\"avg\":\n",
    "            return np.concatenate([score.mean(0) for score in score_list], axis=0)\n",
    "        elif fusion_type == \"avg_std\":\n",
    "            return np.concatenate([score.mean(0)+score.std(0) for score in score_list], axis=0)\n",
    "        elif fusion_type == \"avg_max\":\n",
    "            return np.concatenate([score.mean(0)+score.max(0) for score in score_list], axis=0)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        return np.concatenate([feat_fn(score) for score in score_list], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adapt_scores = np.concatenate([reduce_scores(x, fusion_type=\"avg_max\") for x in eval_plda_score_list], axis=0)\n",
    "test_labels = np.concatenate(eval_plda_labels)\n",
    "print(compute_eer(test_adapt_scores, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR train\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "test_adapt_scores = np.concatenate([reduce_scores(x) for x in eval_plda_score_list], axis=0)\n",
    "test_feat = np.concatenate([reduce_scores(x, feat_fn=get_features) for x in eval_plda_score_list], axis=0)\n",
    "test_labels = np.concatenate(eval_plda_labels)\n",
    "lr_clf = pickle.load(open(\"trials/dev317_eval934/dev_random_enr20_spk10_gender/plda_nf4_bal_lr_clf.pkl\", \"rb\"))\n",
    "svm_clf = pickle.load(open(\"trials/dev317_eval934/dev_random_enr20_spk10_gender/plda_nf4_bal_svm_clf.pkl\", \"rb\"))\n",
    "\n",
    "print(compute_eer(test_adapt_scores, test_labels))\n",
    "print(compute_eer(lr_clf.decision_function(test_feat), test_labels))\n",
    "print(compute_eer(svm_clf.decision_function(test_feat), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fusion_eers = []\n",
    "feat_eers = []\n",
    "for scores, label in zip(eval_plda_score_list, eval_plda_labels):\n",
    "    score_fusion = reduce_scores(scores)\n",
    "    feat_score = lr_clf.decision_function(reduce_scores(scores, get_features))\n",
    "    score_fusion_eers.append(compute_eer(score_fusion, label)[0])\n",
    "    feat_eers.append(compute_eer(feat_score, label)[0])\n",
    "\n",
    "eer_diff = np.array(feat_eers) - np.array(score_fusion_eers)\n",
    "avg_feat_eer = np.mean(feat_eers)\n",
    "avg_fusion_eer = np.mean(score_fusion_eers)\n",
    "\n",
    "n_better = np.count_nonzero(eer_diff < 0)\n",
    "n_worse = np.count_nonzero(eer_diff > 0)\n",
    "n_equal = np.count_nonzero(eer_diff == 0)\n",
    "print(\"n_better:{}, n_worse:{}, n_equal:{}\".format(n_better, n_worse,  n_equal))\n",
    "print(\"avg_fusion_eer:{:.5f}, avg_feat_eer:{:.5f}\".format(avg_fusion_eer, avg_feat_eer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_trial_idx = np.nonzero(eer_diff < 0)[0]\n",
    "\n",
    "better_n_adapt = []\n",
    "better_adapt_time = []\n",
    "for idx in better_trial_idx:\n",
    "    adapt_rec = eval_plda_trial_output[idx][-1]\n",
    "    n_adapt = len(adapt_rec)\n",
    "    avg_adapt_time = adapt_rec.mean()\n",
    "    better_n_adapt.append(n_adapt)\n",
    "    if n_adapt > 0:\n",
    "        better_adapt_time.append(avg_adapt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(better_n_adapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(better_adapt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_trial_idx = np.nonzero(eer_diff > 0)[0]\n",
    "\n",
    "worse_n_adapt = []\n",
    "worse_adapt_time = []\n",
    "for idx in worse_trial_idx:\n",
    "    adapt_rec = eval_plda_trial_output[idx][-1]\n",
    "    n_adapt = len(adapt_rec)\n",
    "    avg_adapt_time = adapt_rec.mean()\n",
    "    worse_n_adapt.append(n_adapt)\n",
    "    if n_adapt > 0:\n",
    "        worse_adapt_time.append(avg_adapt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(worse_n_adapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(worse_adapt_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
