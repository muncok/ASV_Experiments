{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Ernollment Trial (Session Matching)\n",
    "---------\n",
    "\n",
    "여러개의 초기 enrollment의 case도 고려해서 만드려고 한다.\n",
    "\n",
    "그리고 session을 고려한 case를 만들고싶다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from utils import key2df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-31755eb20dca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/host/projects/sv_experiments/sv_system'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "sys.path.append('/host/projects/sv_experiments/sv_system')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes & embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = pickle.load(open(\"./xvector_embeds/sv_keys.pkl\", \"rb\"))\n",
    "keys = np.array(keys)\n",
    "embeds = np.load(\"./xvector_embeds/sv_embeds.npy\")\n",
    "embeds = embeds / np.linalg.norm(embeds, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_df = key2df(keys)\n",
    "key2index = {k:v for v, k in enumerate(keys)}  # it used for mapping from key to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_uttr_stat = key_df.spk.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Held-out validation set\n",
    "\n",
    "utterance가 150개 넘는 것들을 trial spks로 빼고  \n",
    "나머지는 validation spk로 빼었다.  \n",
    "그리고 validation spk 음성을 이용해서 threshold를 정하기 위한 trial을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trial_enr_spks = spk_uttr_stat[spk_uttr_stat > 150].index.tolist()\n",
    "val_spks = spk_uttr_stat[spk_uttr_stat <= 150].index.tolist()\n",
    "val_embed_df = key_df[key_df.spk.isin(val_spks)]\n",
    "trial_embed_df = key_df[key_df.spk.isin(trial_enr_spks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxc2_trial = pd.read_pickle(\"trials/voxc2_1211_trials.pkl\")\n",
    "trial_for_thresh = voxc2_trial[(voxc2_trial.enroll_spk.isin(val_spks)) & (voxc2_trial.test_spk.isin(val_spks))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design trials for each enr_spk\n",
    "\n",
    "session을 하나 뽑아서 개 중에서 enr_uttr을 뽑고 target_trial_uttr를 뽑아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session을 매칭시킬지 그렇지 않을지 선택한다.\n",
    "same_session = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cases = []\n",
    "n_enroll_utters = 1\n",
    "n_target_session = 1\n",
    "\n",
    "\n",
    "for enr_spk in trial_enr_spks:\n",
    "    target_sessions = trial_embed_df[trial_embed_df.spk == enr_spk].session.unique()\n",
    "    for i, sess in enumerate(target_sessions):\n",
    "        target_uttrs = trial_embed_df[(trial_embed_df.spk == enr_spk) & (trial_embed_df.session == sess)] \n",
    "        enr_uttr_pool = target_uttrs.sample(n=1*n_enroll_utters) \n",
    "        if not same_session:           \n",
    "            target_sessions_set = set(target_sessions)\n",
    "            target_sessions_set.remove(sess)\n",
    "            target_session = random.choice(list(target_sessions_set))\n",
    "            target_uttrs = trial_embed_df[(trial_embed_df.spk == enr_spk) & (trial_embed_df.session == target_session)]\n",
    "        if len(target_uttrs) < 5:\n",
    "            continue\n",
    "        nonTarget_uttrs = trial_embed_df[trial_embed_df.spk != enr_spk]\n",
    "        # we have target ratios [0.9, 0.5, 0.1, 0.01]\n",
    "        nonTarget_n_uttrs = (np.array(len(target_uttrs)-n_enroll_utters)*np.array([1/9, 1, 9])).astype(np.int64)\n",
    "        # for p_ratio is 0.01 \n",
    "        nonTarget_n_uttrs = np.append(nonTarget_n_uttrs, min(len(nonTarget_uttrs), 99*len(target_uttrs)))\n",
    "        for i in range(0, len(enr_uttr_pool), n_enroll_utters):\n",
    "            enr_uttrs = enr_uttr_pool[i:i+n_enroll_utters]\n",
    "            for n_imp_uttr in nonTarget_n_uttrs: # neg_trial ratio \n",
    "                if same_session:\n",
    "                    target_trial_uttrs = target_uttrs.drop(index=enr_uttrs.index)\n",
    "                else:\n",
    "                    target_trial_uttrs = target_uttrs\n",
    "                nonTarget_trial_uttrs = nonTarget_uttrs.sample(n=n_imp_uttr)\n",
    "                cases += [[enr_spk, \n",
    "                           enr_uttrs.index.tolist(), \n",
    "                           target_trial_uttrs.index.tolist(),\n",
    "                           nonTarget_trial_uttrs.index.tolist()]]\n",
    "\n",
    "sorted_cases = sorted(cases, key=lambda x: len(x[3])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split trials according to pos_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_legnth = list(map(lambda x: len(x[3]), cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ratio_0 = cases[0:len(cases):4] # 0.9\n",
    "pos_ratio_1 = cases[1:len(cases):4] # 0.5\n",
    "pos_ratio_2 = cases[2:len(cases):4] # 0.1\n",
    "pos_ratio_3 = cases[3:len(cases):4] # 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./trials/enr306_session_diff/\"\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "trial_for_thresh.to_pickle(save_dir + \"/trial_for_thresh.pkl\")\n",
    "val_embed_df.to_pickle(save_dir + \"/valdiation_df.pkl\")\n",
    "pickle.dump(pos_ratio_0, open(save_dir + \"/trials_ratio_0.9.pkl\", \"wb\"))\n",
    "pickle.dump(pos_ratio_1, open(save_dir + \"/trials_ratio_0.5.pkl\", \"wb\"))\n",
    "pickle.dump(pos_ratio_2, open(save_dir + \"/trials_ratio_0.1.pkl\", \"wb\"))\n",
    "pickle.dump(pos_ratio_3, open(save_dir + \"/trials_ratio_0.01.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id10986',\n",
       " ['id10986-33NaEPsiFEA-00002'],\n",
       " ['id10986-s2077pq_vdY-00001',\n",
       "  'id10986-s2077pq_vdY-00002',\n",
       "  'id10986-s2077pq_vdY-00003',\n",
       "  'id10986-s2077pq_vdY-00004',\n",
       "  'id10986-s2077pq_vdY-00005',\n",
       "  'id10986-s2077pq_vdY-00006'],\n",
       " ['id10399-JL9kpRAbMno-00010',\n",
       "  'id11075-X1Kwj4fIAbo-00020',\n",
       "  'id11027-y0qy4sifQRY-00002',\n",
       "  'id11211-suSnxN1Wm8c-00031',\n",
       "  'id11181-XsiRQ8XiWOk-00009']]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cases[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
