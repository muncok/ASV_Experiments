{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\", \n",
    "                              data_folder=\"/dataset/SV_sets/voxceleb12/feats/fbank64_vad/\",\n",
    "                              input_frames=400, splice_frames=[200, 400], stride_frames=1, \n",
    "                              input_format='fbank', input_dim=65, random_clip=True,\n",
    "                              n_epochs=200, lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False, batch_size=64,\n",
    "                              gpu_no=[0], cuda=True, num_workers=4,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_dev.csv\")\n",
    "# dev_df['label'] = dev_df.groupby(\"spk\").ngroup()\n",
    "dev_train_df = dev_df[dev_df.set == 'train']\n",
    "dev_val_df = dev_df[dev_df.set == 'val']\n",
    "eval_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_eval.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.feat_dataset import FeatDataset\n",
    "\n",
    "dev_train_dataset = FeatDataset.read_df(config, dev_train_df, 'train')\n",
    "dev_val_dataset = FeatDataset.read_df(config, dev_val_df, 'test')\n",
    "eval_dataset = FeatDataset.read_df(config, eval_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_default_loader \n",
    "dev_train_dataloader = init_default_loader(config, dev_train_dataset, shuffle=True, var_len=False) \n",
    "dev_val_dataloader = init_default_loader(config, dev_val_dataset, shuffle=False, var_len=True) \n",
    "eval_dataloader = init_default_loader(config, eval_dataset, shuffle=False, var_len=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnn_models import tdnn_xvector_res_v0\n",
    "import torch\n",
    "\n",
    "model = tdnn_xvector_res_v0(config, 512, n_labels=len(dev_train_df.label.unique()))\n",
    "saved_model = torch.load(\"trained_models/voxc1_dev_tdnn_res_v0.pt\")\n",
    "model.load_state_dict(saved_model)\n",
    "\n",
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "from sklearn.metrics import roc_curve\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "from fine_tune_utils import class_weight\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weight(config, dev_train_df))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)\n",
    "# step_scheduler = MultiStepLR(optimizer, [30], 0.1)\n",
    "\n",
    "writer = SummaryWriter(\"logs/tdnn_res_v0\")\n",
    "model_path = \"voxc1_dev_tdnn_res_v0.pt\"\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch_idx in range(start_epoch, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    \n",
    "# =============== train code #===============\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "#         if (batch_idx+1) % 1000 == 0:\n",
    "#             print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "#                   .format(batch_idx+1, len(si_loader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "    plateau_scheduler.step(train_loss)\n",
    "    \n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))\n",
    "    writer.add_scalar(\"train/loss\", train_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"train/acc\", train_acc, epoch_idx+1)\n",
    "\n",
    "#=============== dev_val code #===============\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    val_loss = loss_sum / total\n",
    "    val_acc = n_corrects / total\n",
    "    \n",
    "    print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))\n",
    "    writer.add_scalar(\"val/loss\", val_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"val/acc\", val_acc, epoch_idx+1)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, Fr features\n",
    "fr_feats = []\n",
    "model.eval()\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    fr_feat = model.fr_feat(X).cpu().detach()\n",
    "    fr_feats.append(fr_feat)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9731,  0.9194,  0.8460,  0.7652,  0.6475,  0.5775,  0.5488,  0.5367,\n",
       "         0.5829,  0.5654,  0.4195,  0.2916,  0.2122,  0.1197,  0.0676,  0.0322,\n",
       "        -0.0061, -0.0410, -0.0531, -0.0531, -0.0590, -0.0614, -0.0328, -0.0150,\n",
       "        -0.0346, -0.0619, -0.0721, -0.0648, -0.0658, -0.0635, -0.0456, -0.0339,\n",
       "        -0.0354, -0.0256, -0.0266, -0.0331, -0.0207,  0.0241,  0.0486,  0.0436,\n",
       "         0.0121, -0.0017, -0.0279, -0.0312, -0.0458, -0.0290, -0.0084,  0.0147,\n",
       "         0.0084,  0.0343,  0.0646,  0.0471, -0.0223, -0.0798,  0.0046,  0.2005,\n",
       "         0.3268,  0.3368,  0.2863,  0.2927,  0.3569,  0.3999,  0.4128,  0.3561,\n",
       "         0.1756, -0.0672, -0.1856, -0.2118, -0.2193, -0.2026, -0.1163,  0.0897,\n",
       "         0.2522,  0.3210,  0.3290,  0.2939,  0.2097,  0.0964,  0.0285,  0.0154,\n",
       "         0.0167,  0.0372,  0.0647,  0.0906,  0.1198,  0.1151,  0.0794,  0.0454,\n",
       "         0.0297,  0.0120, -0.0102, -0.0400, -0.0716, -0.1135, -0.1244, -0.1197,\n",
       "        -0.1167, -0.0907,  0.0283,  0.2337,  0.3583,  0.3570,  0.2915,  0.2862,\n",
       "         0.3548,  0.4533,  0.5054,  0.5516,  0.6498,  0.5965,  0.3954,  0.1808,\n",
       "         0.0464, -0.0338, -0.0462, -0.0128,  0.0292,  0.0457,  0.0486,  0.0446,\n",
       "         0.0351,  0.1155,  0.3153,  0.4826,  0.5547,  0.5068,  0.4826,  0.4492,\n",
       "         0.4697,  0.5188,  0.5805,  0.4179,  0.2099,  0.0753,  0.0254,  0.0506,\n",
       "         0.0668,  0.0554,  0.0306,  0.0040, -0.0204, -0.0356, -0.0401, -0.0271,\n",
       "        -0.0194,  0.0143,  0.0502,  0.0664,  0.0476,  0.0491,  0.0548,  0.0600,\n",
       "         0.0503,  0.0248, -0.0051, -0.0196, -0.0345, -0.0645, -0.0855, -0.0572,\n",
       "         0.0203,  0.1685,  0.3255,  0.4108,  0.4397,  0.4046,  0.3772,  0.3767,\n",
       "         0.3549,  0.3282,  0.3325,  0.3747,  0.3826,  0.2676, -0.0352, -0.1711,\n",
       "        -0.1309,  0.0541,  0.2354,  0.3508,  0.4698,  0.5673,  0.5788,  0.5838,\n",
       "         0.6160,  0.5532,  0.3533,  0.0686, -0.0927, -0.1315, -0.0896, -0.0588,\n",
       "        -0.0540, -0.0501, -0.0496, -0.0205, -0.0285, -0.0533, -0.0627, -0.0382,\n",
       "        -0.0009,  0.0097,  0.1100,  0.2758,  0.3842,  0.4024,  0.4567,  0.4235,\n",
       "         0.3757,  0.3691,  0.4403,  0.4498,  0.3615,  0.2170,  0.1816,  0.2154,\n",
       "         0.2282,  0.2501,  0.2622,  0.2627,  0.3007,  0.3293,  0.3747,  0.3893,\n",
       "         0.3846,  0.3672,  0.3305,  0.3107,  0.3424,  0.4113,  0.4115,  0.4254,\n",
       "         0.4125,  0.3772,  0.3906,  0.4627,  0.5121,  0.4758,  0.4087,  0.4001,\n",
       "         0.3741,  0.3001,  0.2146,  0.2132,  0.2136,  0.2492,  0.2583,  0.2344,\n",
       "         0.2610,  0.3411,  0.4248,  0.3821,  0.2192,  0.0754,  0.0438,  0.1321,\n",
       "         0.1861,  0.1349,  0.0695,  0.0341,  0.0011, -0.0341, -0.1026, -0.1305,\n",
       "        -0.1463, -0.1338, -0.1330, -0.1252, -0.1317, -0.1553, -0.1772, -0.1829,\n",
       "        -0.1597, -0.1112, -0.0668, -0.0441, -0.0402, -0.0264, -0.0056, -0.0095,\n",
       "        -0.0130,  0.0432,  0.1114,  0.1306,  0.1139,  0.0917,  0.0876,  0.0840,\n",
       "         0.1213,  0.2311,  0.2869,  0.2657,  0.2339,  0.1757,  0.1089,  0.0554,\n",
       "        -0.0020, -0.0543, -0.0996, -0.1015, -0.0465, -0.0129, -0.0720, -0.1636,\n",
       "        -0.1640, -0.1582, -0.1579, -0.1136, -0.0215,  0.0253,  0.1096,  0.2681,\n",
       "         0.4808,  0.4930,  0.4374,  0.4626,  0.4788,  0.4091,  0.2606,  0.0618,\n",
       "        -0.0847, -0.1674, -0.2289, -0.1715, -0.0518,  0.1612,  0.3115,  0.3721,\n",
       "         0.4580,  0.5621,  0.5252,  0.4836,  0.3580,  0.1184, -0.0988, -0.1178,\n",
       "        -0.0318,  0.1544,  0.2577,  0.3201,  0.3486,  0.4000,  0.4616,  0.4078,\n",
       "         0.1958, -0.0083, -0.0806, -0.0555,  0.0189,  0.0967,  0.0886,  0.0470,\n",
       "         0.0018, -0.0041,  0.0277,  0.0773,  0.1040,  0.1367,  0.2054,  0.2247,\n",
       "         0.1808,  0.0794,  0.0066,  0.0085,  0.0627,  0.1088,  0.0940,  0.0346,\n",
       "        -0.0433, -0.0656, -0.0103,  0.2349,  0.3810,  0.4047,  0.3775,  0.3850,\n",
       "         0.4286,  0.4864,  0.4767,  0.3773,  0.4649,  0.4979,  0.5299,  0.6061,\n",
       "         0.4696,  0.2904,  0.1063,  0.0318,  0.0658,  0.0665,  0.0812,  0.1079,\n",
       "         0.1117,  0.0510, -0.0088])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity as cosine\n",
    "cosine(fr_feats[0][0,:,0:1], fr_feats[0][0,:,1:], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1500, 396])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_feats[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #12, val loss: 0.0084, val acc: 0.7485\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loss_sum = 0\n",
    "n_corrects = 0\n",
    "total = 0\n",
    "predicts = []\n",
    "labels = []\n",
    "for batch_idx, (seq_len, X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    batch_logits = []\n",
    "    for i in range(len(X)):\n",
    "        x_in = X[i:i+1,:,:seq_len[i]]\n",
    "        out_ = model(x_in)\n",
    "        batch_logits.append(out_)\n",
    "    logit = torch.cat(batch_logits, dim=0)\n",
    "    loss = criterion(logit, y)\n",
    "    loss_sum += loss.item()\n",
    "    predicts.append(logit.max(1)[1])\n",
    "    labels.append(y)\n",
    "    n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "    total += y.size(0)\n",
    "val_loss = loss_sum / total\n",
    "val_acc = n_corrects / total\n",
    "\n",
    "print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))\n",
    "predicts = torch.cat(predicts).cpu()\n",
    "labels = torch.cat(labels).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "conf_mat = confusion_matrix(labels, predicts)\n",
    "is_correct = predicts.eq(labels).numpy()\n",
    "predicts = predicts.numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "pred_result = pd.DataFrame([predicts, labels, is_correct]).T\n",
    "pred_result.columns = ['pred', 'label', 'is_correct']\n",
    "pred_class_acc = pred_result.groupby('label').is_correct.mean()\n",
    "\n",
    "sns.distplot(pred_class_acc, norm_hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777    67\n",
       "202    44\n",
       "451    41\n",
       "854    37\n",
       "791    32\n",
       "128    30\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the labels where acc is 0\n",
    "dev_train_df[dev_train_df.label.isin(pred_class_acc.sort_values()[:6].index)].label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_labels = pred_class_acc.sort_values()[:200].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_dev_train_df = dev_train_df[dev_train_df.label.isin(untrained_labels)]\n",
    "untrained_dev_train_dataset = FeatDataset.read_df(config, untrained_dev_train_df, 'train')\n",
    "untrained_dev_train_dataloader = init_default_loader(config, untrained_dev_train_dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "global_mean = si_embeds.mean(0)\n",
    "clf = LDA(solver='svd', n_components=200)\n",
    "clf.fit(si_embeds - global_mean, si_key_df.label)\n",
    "\n",
    "si_embeds = clf.transform(si_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "sv_embeds = clf.transform(sv_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "si_dataset, embed_dim, n_labels = embedToDataset(si_embeds.reshape(-1,200), si_key_df)\n",
    "sv_dataset, _, _ = embedToDataset(sv_embeds, sv_key_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
