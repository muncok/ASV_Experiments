{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py_plda_baseline\n",
    "\n",
    "py_plda model을 사용하는 baseline 실험들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system import get_embeds, cosine_sim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import key2df, df2dict, compute_eer, get_id2idx\n",
    "\n",
    "embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_xvector/\"\n",
    "sv_embeds = np.load(embed_dir+\"ln_lda_sv_embeds.npy\")\n",
    "sv_keys = pickle.load(open(embed_dir + \"/sv_keys.pkl\", \"rb\"))\n",
    "sv_id2idx = get_id2idx(sv_keys)\n",
    "\n",
    "cohort_ids = np.load(\"trials/dev940_eval311/dev_cohort_ids.npy\")\n",
    "cohort_embeds = get_embeds(cohort_ids, sv_embeds, sv_id2idx, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioffe_plda.verifier import Verifier\n",
    "py_plda_model = Verifier()\n",
    "py_plda_model = Verifier(pickle.load(open(\"py_plda_model_ln_lda.pkl\", \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_trials(enr_embeds, test_embeds, score_method):\n",
    "        if score_method == \"scoreAvg\":\n",
    "            score = py_plda_model.score_avg(enr_embeds, test_embeds).mean(0)\n",
    "        elif score_method == \"vectorAvg\":\n",
    "            score = py_plda_model.vector_avg(enr_embeds, test_embeds).mean(0)\n",
    "        elif score_method == \"multiSessScale\":\n",
    "            score = py_plda_model.multi_sess(enr_embeds, test_embeds, cov_scaling=True).mean(0)\n",
    "        elif score_method == \"multiSessAdapt\":\n",
    "            score = py_plda_model.multi_sess(enr_embeds, test_embeds, cov_adapt=True).mean(0)\n",
    "        else:\n",
    "            raise NotImplemtedError\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Trials Type 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_trials = pickle.load(open(\"trials/dev940_eval311/split_trials/adapt_enr1_hard_trials.pkl\", \"rb\"))\n",
    "test_trials = pickle.load(open(\"trials/dev940_eval311/split_trials/test_semihard_trials.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eT_list =  [-4.26017, 3.1, 10.317]\n",
    "init = []\n",
    "optimal = []\n",
    "\n",
    "scoreAvg = {k:[] for k in eT_list}\n",
    "vectorAvg = {k:[] for k in eT_list}\n",
    "multiSessScale = {k:[] for k in eT_list}\n",
    "multiSessAdapt = {k:[] for k in eT_list}\n",
    "\n",
    "adapt_preds = {k:[] for k in eT_list}\n",
    "\n",
    "labels = []\n",
    "adapt_labels = []\n",
    "\n",
    "for t_i in tqdm(range(0, len(adapt_trials), 1), total=len(adapt_trials)//1):\n",
    "    trial_info, enr_ids, adapt_trial = adapt_trials[t_i]\n",
    "    test_trial = test_trials[trial_info['spk']]\n",
    "    adapt_trial = (np.array(adapt_trial.id), np.array(adapt_trial.label))\n",
    "    test_trial = (np.array(test_trial.id), np.array(test_trial.label))\n",
    "\n",
    "    init_enr_embeds = get_embeds(enr_ids, sv_embeds,  sv_id2idx, norm=False)\n",
    "    adapt_embeds = get_embeds(adapt_trial[0], sv_embeds, sv_id2idx, norm=False)\n",
    "    test_embeds = get_embeds(test_trial[0], sv_embeds, sv_id2idx, norm=False)\n",
    "\n",
    "    optimal_enr_embeds = np.concatenate([init_enr_embeds, adapt_embeds[adapt_trial[1]==1]])\n",
    "    init.append(score_trials(init_enr_embeds, test_embeds, \"multiSessScale\"))\n",
    "    optimal.append(score_trials(optimal_enr_embeds, test_embeds, \"multiSessScale\"))\n",
    "    \n",
    "    adapt_scores = score_trials(init_enr_embeds, adapt_embeds, \"multiSessScale\")\n",
    "    \n",
    "    for eT in eT_list:\n",
    "        adapted_embeds = adapt_embeds[adapt_scores > eT]\n",
    "        adapt_preds[eT].append(adapt_scores > eT)\n",
    "        total_enr_embeds = np.concatenate([init_enr_embeds, adapted_embeds])\n",
    "        \n",
    "        multiSessScale[eT].append(score_trials(total_enr_embeds, test_embeds, \"multiSessScale\"))\n",
    "#         scoreAvg[eT].append(score_trials(total_enr_embeds, test_embeds, \"scoreAvg\"))\n",
    "#         vectorAvg[eT].append(score_trials(total_enr_embeds, test_embeds, \"vectorAvg\"))\n",
    "#         multiSessAdapt[eT].append(score_trials(total_enr_embeds, test_embeds, \"multiSessAdapt\"))\n",
    "        \n",
    "    adapt_labels.append(adapt_trial[1])\n",
    "    labels.append(test_trial[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "eer: 1.23%, fpr: 1.23%, fnr: 1.23%\n",
      "optimal\n",
      "eer: 0.67%, fpr: 0.67%, fnr: 0.67%\n",
      "\n",
      "eT: -4.26017\n",
      "multiSessScale\n",
      "eer: 2.77%, fpr: 2.72%, fnr: 2.77%\n",
      "precision:0.223, recall:1.000\n",
      "\n",
      "eT: 3.1\n",
      "multiSessScale\n",
      "eer: 1.23%, fpr: 1.13%, fnr: 1.23%\n",
      "precision:0.615, recall:0.980\n",
      "\n",
      "eT: 10.317\n",
      "multiSessScale\n",
      "eer: 1.13%, fpr: 1.13%, fnr: 1.13%\n",
      "precision:0.972, recall:0.940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"init\")\n",
    "compute_eer(np.concatenate(init), np.concatenate(labels))\n",
    "print(\"optimal\")\n",
    "compute_eer(np.concatenate(optimal), np.concatenate(labels))\n",
    "print()\n",
    "for eT in eT_list:\n",
    "    print(\"eT: {}\".format(eT))\n",
    "    print(\"multiSessScale\")\n",
    "    compute_eer(np.concatenate(multiSessScale[eT]), np.concatenate(labels))\n",
    "#     print(\"scoreAvg\")\n",
    "#     compute_eer(np.concatenate(scoreAvg[eT]), np.concatenate(labels))\n",
    "#     print(\"vectorAvg\")\n",
    "#     compute_eer(np.concatenate(vectorAvg[eT]), np.concatenate(labels));\n",
    "#     print(\"multiSessAdapt\")\n",
    "#     compute_eer(np.concatenate(multiSessAdapt[eT]), np.concatenate(labels))\n",
    "\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(\n",
    "        np.concatenate(adapt_labels), np.concatenate(adapt_preds[eT]), average='binary')\n",
    "    print(\"precision:{:.3f}, recall:{:.3f}\".format(precision, recall))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eT: -4.26017\n",
      "inc:0.67, equal:0.00, dec:0.33\n",
      "eT: 3.1\n",
      "inc:0.33, equal:0.00, dec:0.67\n",
      "eT: 10.317\n",
      "inc:0.33, equal:0.33, dec:0.33\n"
     ]
    }
   ],
   "source": [
    "init_eers = {k:[] for k in eT_list}\n",
    "opt_eers = {k:[] for k in eT_list} \n",
    "multiSessScale_eers = {k:[] for k in eT_list}\n",
    "for eT in eT_list:\n",
    "    print(\"eT: {}\".format(eT))\n",
    "    for init_score, optimal_score, multi_score, label in zip(init, optimal, multiSessScale[eT], labels):\n",
    "        init_eers[eT].append(compute_eer(init_score, label, verbose=False)[0])\n",
    "        opt_eers[eT].append(compute_eer(optimal_score, label, verbose=False)[0])\n",
    "        multiSessScale_eers[eT].append(compute_eer(multi_score, label, verbose=False)[0])\n",
    "    compare_value(init_eers[eT], multiSessScale_eers[eT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eT:10.317, precision:0.933, recall:0.840\n",
      "eT:10.317, precision:0.980, recall:0.980\n",
      "eT:10.317, precision:1.000, recall:1.000\n"
     ]
    }
   ],
   "source": [
    "# individual prec and recall\n",
    "for labels, preds in zip(adapt_labels, adapt_preds[eT]):\n",
    "    precision, recall, _, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    print(\"eT:{}, precision:{:.3f}, recall:{:.3f}\".format(eT, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Trials Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = pickle.load(open(\"trials/dev940_eval311/hard_enr3xsess_ntar9/trials.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7362 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "adapt_ratio = 0.2\n",
    "\n",
    "eT_list =  [-4.26017]\n",
    "init = []\n",
    "optimal = []\n",
    "labels = []\n",
    "scoreAvg = {k:[] for k in eT_list}\n",
    "vectorAvg = {k:[] for k in eT_list}\n",
    "multiSessScale = {k:[] for k in eT_list}\n",
    "multiSessAdapt = {k:[] for k in eT_list}\n",
    "\n",
    "for t_i in tqdm(range(len(trials)), total=len(trials)):\n",
    "    enr_spk, enr_ids, test_trial = trials[t_i]\n",
    "    test_trial = (np.array(test_trial.id), np.array(test_trial.label))\n",
    "\n",
    "    adapt_len = int(len(test_trial[0]) * adapt_ratio)\n",
    "    adapt_trial = (test_trial[0][:adapt_len], test_trial[1][:adapt_len])\n",
    "    test_trial = (test_trial[0][adapt_len:], test_trial[1][adapt_len:])\n",
    "    \n",
    "    init_enr_embeds = get_embeds(enr_ids, sv_embeds,  sv_id2idx, norm=False)\n",
    "    adapt_embeds = get_embeds(adapt_trial[0], sv_embeds, sv_id2idx, norm=False)\n",
    "    test_embeds = get_embeds(test_trial[0], sv_embeds, sv_id2idx, norm=False)\n",
    "\n",
    "    ## optimal, init scores\n",
    "    optimal_enr_embeds = np.concatenate([init_enr_embeds, \n",
    "                                         adapt_embeds[adapt_trial[1]==1]])\n",
    "    init.append(score_trials(init_enr_embeds, test_embeds, \"multiSessScale\"))\n",
    "    optimal.append(score_trials(optimal_enr_embeds, test_embeds, \"multiSessScale\"))\n",
    "   \n",
    "    ## adapt scores\n",
    "    adapt_scores = score_trials(total_enr_embeds, adapt_embeds, \"scoreAvg\")\n",
    "    \n",
    "    for eT in eT_list:\n",
    "        adapted_embeds = adapt_embeds[adapt_scores > eT]\n",
    "        total_enr_embeds = np.concatenate([init_enr_embeds, adapted_embeds])\n",
    "        \n",
    "        scoreAvg[eT].append(score_trials(total_enr_embeds, test_embeds, \"scoreAvg\"))\n",
    "        vectorAvg[eT].append(score_trials(total_enr_embeds, test_embeds, \"vectorAvg\"))\n",
    "        multiSessScale[eT].append(score_trials(total_enr_embeds, test_embeds, \"multiSessScale\"))\n",
    "        multiSessAdapt[eT].append(score_trials(total_enr_embeds, test_embeds, \"multiSessAdapt\"))\n",
    "        \n",
    "    labels.append(test_trial[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eT in eT_list:\n",
    "    print(\"optimal\")\n",
    "    compute_eer(np.concatenate(optimal), np.concatenate(labels))\n",
    "    print(\"init\")\n",
    "    compute_eer(np.concatenate(init), np.concatenate(labels))\n",
    "    print(\"scoreAvg\")\n",
    "    compute_eer(np.concatenate(scoreAvg[eT]), np.concatenate(labels))\n",
    "    print(\"vectorAvg\")\n",
    "    compute_eer(np.concatenate(vectorAvg[eT]), np.concatenate(labels));\n",
    "    print(\"multiSessScale\")\n",
    "    compute_eer(np.concatenate(multiSessScale[eT]), np.concatenate(labels))\n",
    "    print(\"multiSessAdapt\")\n",
    "    compute_eer(np.concatenate(multiSessAdapt[eT]), np.concatenate(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
