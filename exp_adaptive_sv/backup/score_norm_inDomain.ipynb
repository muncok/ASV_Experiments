{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Normalization with unsuperviesd info\n",
    "-------------\n",
    "\n",
    "z-norm과 s-norm을 구현하였다.\n",
    "\n",
    "기존에 다른 데이터셋으로 부터 imposter set을 가져와서 score normalization 했던 것에서 벗어나,  \n",
    "Unsupervised하게 모인 데이터를 가지고 score normalization을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapt trial의 prediction 기준으로 imposter_embeds를 설정하면 에러 때문에(에러가 꽤 클 경우), imposter set의 score 분포가 깨진다.  \n",
    "adapte_label을 기준으로 하면 성능이 좋다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptive_sv_system.score_norm_utils import key2df\n",
    "\n",
    "keys = pickle.load(open(\"./xvector_embeds/sv_keys.pkl\", \"rb\"))\n",
    "key_df = key2df(keys)\n",
    "key2id = {k:v for v, k in enumerate(keys)}\n",
    "\n",
    "sv_embeds = np.load(\"./xvector_embeds/sv_embeds.npy\")\n",
    "validation_set = pd.read_pickle(\"trials/enr306/validation_set.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uttrkey_to_id(key2id, keys):\n",
    "    return [key2id[key] for key in keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from score_norm_utils import uttrkey_to_id\n",
    "imposter_set = validation_set.groupby(\"spk\", group_keys=False).apply(lambda x: x.sample(n=5))\n",
    "ood_imposter_embeds = sv_embeds[uttrkey_to_id(key2id, imposter_set.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing trace then score normalization\n",
    "\n",
    "TODO: run_trial에 넣어야한다... 너무 느리다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trace = [adapt_tr, test_tr, ood_tr]  \n",
    "tr = trial_idxs, scores, label, pred 또는   \n",
    "tr = trial_idxs, scores, label, pred, enroll_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.56792\n",
    "threshold = 0.6292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./adaptive_sv_system/test/FS7_ENR1_QUE7/inc/\"\n",
    "traces = pickle.load(open(base_dir+\"trace.pkl\", \"rb\"))\n",
    "meta_info = pd.read_pickle(base_dir+\"meta_info_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/sklearn/metrics/ranking.py:571: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from adaptive_sv_system.score_norm_utils import eval_cosine_score, eval_z_score, eval_s_score\n",
    "\n",
    "test_c_acc = []\n",
    "ood_c_acc = []\n",
    "test_s_acc = []\n",
    "ood_s_acc = []\n",
    "mus = []\n",
    "test_c_pos_scores = []\n",
    "test_c_neg_scores = []\n",
    "test_s_pos_scores = []\n",
    "test_s_neg_scores = []\n",
    "for t_idx in range(len(traces)):\n",
    "    tr = traces[t_idx]\n",
    "    meta = meta_info.iloc[t_idx]\n",
    "    adapt_tr, test_tr, ood_tr = tr\n",
    "    init_enr_idx = meta_info.iloc[t_idx].enr_idxs\n",
    "    init_enr_embeds = sv_embeds[init_enr_idx]\n",
    "\n",
    "    adapt_trial_idxs, adapt_scores, adapt_label, adapt_pred, adapt_enroll_pred = adapt_tr\n",
    "    adapt_embeds = sv_embeds[adapt_trial_idxs.astype(np.int64)]\n",
    "    adapt_enr_embeds = adapt_embeds[adapt_enroll_pred == 1]\n",
    "    total_enr_embeds = np.concatenate([init_enr_embeds, adapt_enr_embeds])\n",
    "\n",
    "    test_trial_idxs, test_scores, test_label, test_pred = test_tr\n",
    "    test_embeds = sv_embeds[test_trial_idxs.astype(np.int64)]\n",
    "    test_label = torch.from_numpy(test_label).byte()\n",
    "    \n",
    "    ood_trial_idxs, ood_scores, ood_label, ood_pred = ood_tr\n",
    "    ood_embeds = sv_embeds[ood_trial_idxs.astype(np.int64)]\n",
    "    ood_label = torch.from_numpy(ood_label).byte()\n",
    "    \n",
    "    enr_embeds = total_enr_embeds\n",
    "#     enr_embeds = init_enr_embeds\n",
    "    \n",
    "#     imposter_embeds = adapt_embeds[adapt_pred==0]\n",
    "    imposter_embeds = ood_imposter_embeds\n",
    "\n",
    "    test_c_score, test_c_pred_acc, test_c_eer = eval_cosine_score(\n",
    "                                             enr_embeds, test_embeds, \n",
    "                                             test_label, threshold)\n",
    "    test_c_pos_scores += test_c_score[test_label == 1].numpy().tolist()\n",
    "    test_c_neg_scores += test_c_score[test_label == 0].numpy().tolist()\n",
    "    \n",
    "    ood_c_score, ood_c_pred_acc, ood_c_eer = eval_cosine_score(\n",
    "                                             enr_embeds, ood_embeds, \n",
    "                                             ood_label, threshold)\n",
    "    \n",
    "    z_score, z_pred_acc, z_eer = eval_z_score(\n",
    "                                              enr_embeds, test_embeds, \n",
    "                                              test_label, threshold, \n",
    "                                              imposter_embeds)\n",
    "    \n",
    "    test_s_eer, test_s_pred_err, test_s_score, test_s_mu = eval_s_score1(\n",
    "                                              enr_embeds, test_embeds,\n",
    "                                              test_label, threshold,\n",
    "                                              imposter_embeds)\n",
    "    test_s_pos_scores += test_s_score[test_label == 1].numpy().tolist()\n",
    "    test_s_neg_scores += test_s_score[test_label == 0].numpy().tolist()\n",
    "   \n",
    "    ood_s_eer, ood_s_pred_err, ood_s_score, ood_s_mu = eval_s_score1(\n",
    "                                              enr_embeds, ood_embeds,\n",
    "                                              ood_label, threshold,\n",
    "                                              imposter_embeds)\n",
    "    \n",
    "        \n",
    "    test_c_acc += [test_c_pred_acc]\n",
    "    ood_c_acc += [ood_c_pred_acc]\n",
    "    test_s_acc += [1 - test_s_pred_err]\n",
    "    ood_s_acc += [1 - ood_s_pred_err]\n",
    "    mus += [test_s_mu, ood_s_mu]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.035660320338470886, 0.5882129073143005)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adaptive_sv_system.score_norm_utils import compute_eer\n",
    "compute_eer(test_c_pos_scores, test_c_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02931399214264128, 2.32519268989563)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from adaptive_sv_system.score_norm_utils import compute_eer\n",
    "compute_eer(test_s_pos_scores, test_s_neg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_c_acc:0.9632\n",
      "ood_c_acc:0.9967\n",
      "test_s_acc:0.8295\n",
      "ood_s_acc:0.6653\n"
     ]
    }
   ],
   "source": [
    "print(\"test_c_acc:{:.4f}\\nood_c_acc:{:.4f}\\ntest_s_acc:{:.4f}\\nood_s_acc:{:.4f}\".format(\n",
    "    np.mean(test_c_acc), np.mean(ood_c_acc), np.mean(test_s_acc), np.mean(ood_s_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_c_acc:0.9632\n",
      "ood_c_acc:0.9967\n",
      "test_s_acc:0.9099\n",
      "ood_s_acc:1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"test_c_acc:{:.4f}\\nood_c_acc:{:.4f}\\ntest_s_acc:{:.4f}\\nood_s_acc:{:.4f}\".format(\n",
    "    np.mean(test_c_acc), np.mean(ood_c_acc), np.mean(test_s_acc), np.mean(ood_s_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
