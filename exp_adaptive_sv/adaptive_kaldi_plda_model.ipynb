{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system import get_embeds, cosine_sim, compute_plda_score\n",
    "from utils import key2df, df2dict, compute_eer, get_id2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_value(val_a, val_b, mask=None, verbose=True):\n",
    "    if mask is not None:\n",
    "        val_a = val_a[mask]\n",
    "        val_b = val_b[mask]\n",
    "    assert len(val_a) == len(val_b)\n",
    "    n = len(val_a)\n",
    "    r_inc = np.count_nonzero(val_a < val_b) / n\n",
    "    r_equal = np.count_nonzero(val_a == val_b) / n\n",
    "    r_dec = np.count_nonzero(val_a > val_b) / n\n",
    "    if verbose:\n",
    "        print(\"inc:{:.2f}, equal:{:.2f}, dec:{:.2f}\".format(r_inc, r_equal, r_dec))\n",
    "    return r_inc, r_equal, r_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_embeds\"\n",
    "sv_embeds = np.load(embed_dir + \"/sv_embeds.npy\")\n",
    "keys = pickle.load(open(embed_dir + \"/sv_keys.pkl\", \"rb\"))\n",
    "id2idx = get_id2idx(keys)\n",
    "\n",
    "plda_embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_xvector/\"\n",
    "plda_model_dir = plda_embed_dir + \"plda_train/\"\n",
    "plda_sv_embeds = np.load(plda_embed_dir + \"/sv_embeds.npy\")\n",
    "plda_keys = pickle.load(open(plda_embed_dir + \"/sv_keys.pkl\", \"rb\"))\n",
    "plda_id2idx = get_id2idx(plda_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxc1_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1.csv\")\n",
    "spk_uttr_stat = voxc1_df.spk.value_counts()\n",
    "voxc1_meta = pd.read_pickle(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_meta.pkl\")\n",
    "spk2gender = voxc1_meta.Gender.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_spks = spk_uttr_stat[(spk_uttr_stat < 150)].index.tolist()\n",
    "dev_uttrs = voxc1_df[voxc1_df.spk.isin(dev_spks)][['id', 'spk', 'gender', 'session']]\n",
    "eval_spks = spk_uttr_stat[spk_uttr_stat >= 150].index.tolist()\n",
    "eval_uttrs = voxc1_df[voxc1_df.spk.isin(eval_spks)][['id', 'spk', 'gender', 'session']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ioffe_plda.verifier import Verifier\n",
    "py_plda_model = Verifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cohort_uttrs = dev_uttrs.groupby(\"spk\").apply(lambda x: x.sample(n=10))\n",
    "eval_cohort_embeds = get_embeds(eval_cohort_uttrs.id, plda_sv_embeds, plda_id2idx, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_spk = eval_spks[44]\n",
    "eval_target_uttrs = eval_uttrs[eval_uttrs.spk == eval_spk].sample(frac=1.0) # for random init enrollments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTAyGLEk_FE\n"
     ]
    }
   ],
   "source": [
    "# target\n",
    "n_enr = 3\n",
    "len_adapt = 100\n",
    "enr_session = np.random.choice(eval_target_uttrs.session.unique(), size=1)[0]\n",
    "print(enr_session)\n",
    "enr_uttrs = eval_target_uttrs[eval_target_uttrs.session==enr_session].sample(n=n_enr, replace=True)\n",
    "eval_init_enr_embeds = get_embeds(enr_uttrs.id, plda_sv_embeds, plda_id2idx, norm=False)\n",
    "\n",
    "eval_target_uttrs = eval_target_uttrs.drop(index=enr_uttrs.index)\n",
    "eval_target_embeds = get_embeds(eval_target_uttrs.id, plda_sv_embeds, plda_id2idx, norm=False)\n",
    "eval_target_adapt_embeds = eval_target_embeds[:len_adapt]\n",
    "eval_target_test_embeds = eval_target_embeds[len_adapt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nontarget\n",
    "eval_nonTarget_uttrs = eval_uttrs[eval_uttrs.spk != eval_spk]\n",
    "nontarget_spks = np.random.choice(eval_nonTarget_uttrs.spk.unique(), 100, replace=False)\n",
    "eval_nonTarget_uttrs = eval_nonTarget_uttrs[eval_nonTarget_uttrs.spk.isin(nontarget_spks)]\n",
    "eval_nonTarget_embeds = get_embeds(eval_nonTarget_uttrs.id, plda_sv_embeds, id2idx, norm=False)\n",
    "eval_nonTarget_scores = compute_plda_score(eval_init_enr_embeds, eval_nonTarget_embeds, plda_model_dir)\n",
    "eval_nonTarget_sorted_idx = np.argsort(eval_nonTarget_scores.mean(0), axis=0)\n",
    "eval_hard_utter_idx = eval_nonTarget_sorted_idx[-len(eval_target_test_embeds)*9:] \n",
    "eval_hard_scores = eval_nonTarget_scores[:, eval_hard_utter_idx]\n",
    "eval_hard_nonTarget_uttrs = eval_nonTarget_uttrs.iloc[eval_hard_utter_idx]\n",
    "eval_hard_nonTarget_embeds = get_embeds(eval_hard_nonTarget_uttrs.id, plda_sv_embeds, plda_id2idx, norm=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real adaptation\n",
    "eT = 5\n",
    "\n",
    "eval_adapt_scores = compute_plda_score(eval_init_enr_embeds, eval_target_adapt_embeds, plda_model_dir)\n",
    "eval_true_adapted_embeds = eval_target_adapt_embeds[eval_adapt_scores.mean(0) > eT]\n",
    "true_adapt_norm_scores = np.empty(0)\n",
    "if len(eval_true_adapted_embeds) > 0:\n",
    "    adapt_cohort_scores = compute_plda_score(eval_true_adapted_embeds, eval_cohort_embeds, plda_model_dir)\n",
    "    adapt_cohort_mu = adapt_cohort_scores.mean(1)\n",
    "    adapt_cohort_std = adapt_cohort_scores.std(1)\n",
    "    true_adapt_norm_scores = (eval_adapt_scores.mean(0)[eval_adapt_scores.mean(0) > eT] - adapt_cohort_mu)/(adapt_cohort_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_false_adapted_embeds = eval_nonTarget_embeds[eval_nonTarget_scores.mean(0) > eT]\n",
    "false_adapt_norm_scores = np.empty(0)\n",
    "if len(eval_false_adapted_embeds) > 0:\n",
    "    adapt_cohort_scores = compute_plda_score(eval_false_adapted_embeds, eval_cohort_embeds, plda_model_dir)\n",
    "    adapt_cohort_mu = adapt_cohort_scores.mean(1)\n",
    "    adapt_cohort_std = adapt_cohort_scores.std(1)\n",
    "    false_adapt_norm_scores = (eval_nonTarget_scores.mean(0)[eval_nonTarget_scores.mean(0) > eT] - adapt_cohort_mu)/(adapt_cohort_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(eval_false_adapted_embeds) > 0:\n",
    "    eval_total_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_true_adapted_embeds, eval_false_adapted_embeds])\n",
    "else:\n",
    "    eval_total_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_true_adapted_embeds])\n",
    "adapt_sorted_idx = np.array([0,1,2] + (np.flip(np.argsort(np.concatenate([true_adapt_norm_scores, false_adapt_norm_scores])))+3).tolist())\n",
    "adapt_labels = np.concatenate([np.ones(len(eval_true_adapted_embeds)+3), np.zeros(len(eval_false_adapted_embeds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 512)\n",
      "(1, 512)\n",
      "(188, 512)\n"
     ]
    }
   ],
   "source": [
    "print(eval_true_adapted_embeds.shape)\n",
    "print(eval_false_adapted_embeds.shape)\n",
    "print(eval_target_test_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_sorted_idx = np.array([0,1,2] + (np.flip(\n",
    "    np.argsort(np.concatenate([eval_adapt_scores.mean(0)[eval_adapt_scores.mean(0)>eT],\n",
    "    eval_nonTarget_scores.mean(0)[eval_nonTarget_scores.mean(0)>eT]])))+3).tolist())\n",
    "adapt_labels[score_sorted_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_random_nonTarget_embeds = get_embeds(eval_nonTarget_uttrs.sample(n=5000).id, plda_sv_embeds, plda_id2idx, norm=False)  \n",
    "# eval_test_embeds = np.concatenate([eval_target_test_embeds, eval_random_nonTarget_embeds])\n",
    "# eval_test_labels = np.concatenate([np.ones(len(eval_target_test_embeds)), \n",
    "#                                            np.zeros(len(eval_random_nonTarget_embeds))])\n",
    "\n",
    "eval_test_embeds = np.concatenate([eval_target_test_embeds, eval_hard_nonTarget_embeds])\n",
    "eval_test_labels = np.concatenate([np.ones(len(eval_target_test_embeds)), \n",
    "                                           np.zeros(len(eval_hard_nonTarget_embeds))])\n",
    "\n",
    "# eval_test_embeds = np.concatenate([eval_target_test_embeds, eval_nonTarget_embeds])\n",
    "# eval_test_labels = np.concatenate([np.ones(len(eval_target_test_embeds)), \n",
    "#                                            np.zeros(len(eval_nonTarget_embeds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "(0.005319148936170248, 0.000591016548463357, 0.005319148936170248, 4.595991000000001)\n",
      "score fusion\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 6.816882456310678)\n",
      "feat fusion\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 1.300376)\n",
      "score fusion, max\n",
      "(0.010638297872340385, 0.01004728132387707, 0.010638297872340385, 28.469936278737862)\n",
      "score fusion, std\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 11.1119173124317)\n"
     ]
    }
   ],
   "source": [
    "eval_test_scores = compute_plda_score(eval_total_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "eval_cent_scores = compute_plda_score(eval_total_enr_embeds, eval_test_embeds, plda_model_dir, mean=True)\n",
    "\n",
    "# eval_adapt_scores = compute_plda_score(eval_init_enr_embeds, eval_total_enr_embeds[n_enr:], plda_model_dir)\n",
    "# eval_adapt_scores_ = cosine_sim(eval_init_enr_embeds_, eval_true_adapted_embeds_)\n",
    "# eval_test_scores = np.concatenate([eval_test_scores[:n_enr], \n",
    "#                                    eval_test_scores[n_enr:][np.argsort(eval_adapt_scores.mean(0))]])\n",
    "\n",
    "print(\"init\")\n",
    "print(compute_eer(eval_test_scores[:3].mean(0), eval_test_labels))\n",
    "print(\"score fusion\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "print(\"feat fusion\")\n",
    "print(compute_eer(eval_cent_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion, max\")\n",
    "print(compute_eer(eval_test_scores.mean(0)+eval_test_scores.max(0), eval_test_labels))\n",
    "print(\"score fusion, std\")\n",
    "print(compute_eer(eval_test_scores.mean(0)+eval_test_scores.std(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_test_embeds = np.concatenate([eval_total_enr_embeds[3:], eval_cohort_embeds], axis=0)\n",
    "eval_proxy_labels = np.concatenate([np.ones(len(eval_total_enr_embeds[3:])), np.zeros(len(eval_cohort_embeds))])\n",
    "eval_proxy_scores = compute_plda_score(eval_total_enr_embeds, proxy_test_embeds, plda_model_dir)\n",
    "proxy_eers = []\n",
    "for idx in range(0, len(eval_proxy_scores)):\n",
    "    proxy_eers.append(compute_eer(eval_proxy_scores[idx], eval_proxy_labels)[0])\n",
    "proxy_eer_sorted = np.argsort(proxy_eers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score fusion true\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 6.835607921568626)\n",
      "score fusion true, budget (random)\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 6.804656989999999)\n",
      "score fusion total (total)\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 6.816882456310678)\n",
      "\n",
      "score fusion proxy\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 6.524759706666669)\n",
      "score fusion proxy, max\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 22.51899970666667)\n",
      "score fusion proxy, std\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 11.119754329694093)\n",
      "\n",
      "score fusion proxy1\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, 6.5057529)\n",
      "score fusion proxy1, max\n",
      "(0.005319148936170248, 0.0017730496453900709, 0.005319148936170248, 20.651582899999998)\n",
      "score fusion proxy1, std\n",
      "(0.005319148936170248, 0.000591016548463357, 0.005319148936170248, 10.253739150957358)\n"
     ]
    }
   ],
   "source": [
    "n_adapt = 30\n",
    "eval_test_proxy_scores = eval_test_scores[proxy_eer_sorted[:n_adapt]]\n",
    "eval_test_proxy1_scores = eval_test_scores[adapt_sorted_idx[:n_adapt]]\n",
    "print(\"score fusion true\")\n",
    "if len(eval_false_adapted_embeds) > 0:\n",
    "    print(compute_eer(eval_test_scores[:-len(eval_false_adapted_embeds)].mean(0), eval_test_labels))\n",
    "else:\n",
    "    print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion true, budget (random)\")\n",
    "print(compute_eer(eval_test_scores[[0,1,2] + \n",
    "                  np.random.randint(3, 3+len(eval_true_adapted_embeds), n_adapt-3).tolist()].mean(0), \n",
    "                  eval_test_labels))\n",
    "print(\"score fusion total (total)\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "# print(\"score fusion total budget (worst)\")\n",
    "# print(compute_eer(eval_test_scores[[0,1,2] + np.arange(-n_adapt, 0).tolist()].mean(0), eval_test_labels))\n",
    "print()\n",
    "print(\"score fusion proxy\")\n",
    "print(compute_eer(eval_test_proxy_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion proxy, max\")\n",
    "print(compute_eer(eval_test_proxy_scores.mean(0)+eval_test_proxy_scores.max(0), eval_test_labels))\n",
    "print(\"score fusion proxy, std\")\n",
    "print(compute_eer(eval_test_proxy_scores.mean(0)+eval_test_proxy_scores.std(0), eval_test_labels))\n",
    "\n",
    "print()\n",
    "print(\"score fusion proxy1\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion proxy1, max\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0)+eval_test_proxy1_scores.max(0), eval_test_labels))\n",
    "print(\"score fusion proxy1, std\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0)+eval_test_proxy1_scores.std(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score fusion proxy1\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, -2.2094139999999998)\n",
      "score fusion proxy1, max\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, -4.4188279999999995)\n",
      "score fusion proxy1, std\n",
      "(0.005319148936170248, 0.0, 0.005319148936170248, -2.2094139999999998)\n"
     ]
    }
   ],
   "source": [
    "eval_test_proxy1_scores = compute_plda_score(eval_total_enr_embeds[adapt_sorted_idx[:n_adapt]], eval_test_embeds, plda_model_dir, mean=True)\n",
    "print(\"score fusion proxy1\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion proxy1, max\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0)+eval_test_proxy1_scores.max(0), eval_test_labels))\n",
    "print(\"score fusion proxy1, std\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0)+eval_test_proxy1_scores.std(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(adapt_sorted_idx)):\n",
    "    print(compute_eer(eval_test_scores[proxy_eer_sorted[:i]].mean(0), eval_test_labels)[0])\n",
    "    print(compute_eer(eval_test_scores[adapt_sorted_idx[:i]].mean(0), eval_test_labels)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-norm\n",
    "enr_cohort_scores = compute_plda_score(eval_total_enr_embeds, eval_cohort_embeds, plda_model_dir)\n",
    "enr_cohort_mu = enr_cohort_scores.mean(1, keepdims=True)\n",
    "enr_cohort_std = enr_cohort_scores.std(1, keepdims=True)\n",
    "eval_test_norm_scores = (eval_test_scores - enr_cohort_mu) / enr_cohort_std\n",
    "print(compute_eer(eval_test_norm_scores.mean(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-norm\n",
    "a = 1\n",
    "client_scores = compute_plda_score(eval_total_enr_embeds, eval_total_enr_embeds, plda_model_dir)\n",
    "client_mean = np.triu(client_scores, 1).mean()\n",
    "imp_scores = compute_plda_score(eval_total_enr_embeds, eval_cohort_embeds, plda_model_dir)\n",
    "imp_mean = imp_score.mean()\n",
    "f_score = (eval_test_scores-imp_mean)*(2*a/(client_mean - imp_mean)) + a\n",
    "print(compute_eer(f_score.mean(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max effect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_test_scores = eval_test_scores.mean(0)+eval_test_scores.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(np.argmax(eval_test_scores, axis=0), minlength=len(eval_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_thres = compute_eer(adapt_test_scores, eval_test_labels)[-1]\n",
    "adapt_fn_idx = np.nonzero((adapt_test_scores < adapt_thres) & (eval_test_labels == 1))\n",
    "adapt_fp_idx = np.nonzero((adapt_test_scores > adapt_thres) & (eval_test_labels == 0))\n",
    "max_thres = compute_eer(max_test_scores, eval_test_labels)[-1]\n",
    "max_fn_idx = np.nonzero((max_test_scores < max_thres) & (eval_test_labels == 1))\n",
    "max_fp_idx = np.nonzero((max_test_scores > max_thres) & (eval_test_labels == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapt_fp_idx)\n",
    "print(adapt_test_scores[adapt_fp_idx])\n",
    "print(max_fp_idx)\n",
    "print(max_test_scores[max_fp_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapt_fn_idx)\n",
    "print(adapt_test_scores[adapt_fn_idx])\n",
    "print(max_fn_idx)\n",
    "print(max_test_scores[max_fn_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### std effect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_test_scores = eval_test_scores.mean(0)+eval_test_scores.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_thres = compute_eer(adapt_test_scores, eval_test_labels)[-1]\n",
    "adapt_fn_idx = np.nonzero((adapt_test_scores < adapt_thres) & (eval_test_labels == 1))\n",
    "adapt_fp_idx = np.nonzero((adapt_test_scores > adapt_thres) & (eval_test_labels == 0))\n",
    "std_thres = compute_eer(std_test_scores, eval_test_labels)[-1]\n",
    "std_fn_idx = np.nonzero((std_test_scores < std_thres) & (eval_test_labels == 1))\n",
    "std_fp_idx = np.nonzero((std_test_scores > std_thres) & (eval_test_labels == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapt_fp_idx)\n",
    "print(adapt_test_scores[adapt_fp_idx])\n",
    "print(std_fp_idx)\n",
    "print(std_test_scores[std_fp_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapt_fn_idx)\n",
    "print(adapt_test_scores[adapt_fn_idx])\n",
    "print(std_fn_idx)\n",
    "print(std_test_scores[std_fn_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adapt effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test_scores = eval_test_scores[:3].mean(0)\n",
    "adapt_test_scores = eval_test_scores.mean(0)\n",
    "# eval_avg_test_scores = compute_plda_score(eval_high_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "# adapt_test_scores = eval_avg_test_scores.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 2\n",
    "for idx in range(3, len(eval_test_scores), step):\n",
    "    print(\"{} to {}\".format(idx, idx+step))\n",
    "    compare_value(init_test_scores, eval_test_scores[idx:idx+step].mean(0))\n",
    "#     compare_value(init_test_scores, eval_test_scores[idx:idx+step].mean(0), eval_test_labels==1)\n",
    "#     compare_value(init_test_scores, eval_test_scores[idx:idx+step].mean(0), eval_test_labels==0)\n",
    "    print(compute_eer(eval_test_scores[idx:idx+step].mean(0), eval_test_labels)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative\n",
    "for idx in range(3, len(eval_test_scores), 3):\n",
    "    compare_value(init_test_scores, eval_test_scores[3:idx+3].mean(0), eval_test_labels==1)\n",
    "    compare_value(init_test_scores, eval_test_scores[3:idx+3].mean(0), eval_test_labels==0)\n",
    "    print(compute_eer(eval_test_scores[3:idx+3].mean(0), eval_test_labels)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compute_plda_score(eval_total_enr_embeds[[9]], eval_test_embeds, plda_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_thres = compute_eer(init_test_scores, eval_test_labels)[-1]\n",
    "init_fn_idx = np.nonzero((init_test_scores < init_thres) & (eval_test_labels == 1))\n",
    "init_fp_idx = np.nonzero((init_test_scores > init_thres) & (eval_test_labels == 0))\n",
    "adapt_thres = compute_eer(adapt_test_scores, eval_test_labels)[-1]\n",
    "adapt_fn_idx = np.nonzero((adapt_test_scores < adapt_thres) & (eval_test_labels == 1))\n",
    "adapt_fp_idx = np.nonzero((adapt_test_scores > adapt_thres) & (eval_test_labels == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(init_fp_idx)\n",
    "print(init_test_scores[init_fp_idx])\n",
    "print(adapt_fp_idx)\n",
    "print(adapt_test_scores[adapt_fp_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(init_fn_idx)\n",
    "print(init_test_scores[init_fn_idx])\n",
    "print(adapt_fn_idx)\n",
    "print(adapt_test_scores[adapt_fn_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system_utils import compute_error\n",
    "compute_error(init_test_scores > init_thres, eval_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system_utils import compute_error\n",
    "compute_error(adapt_test_scores > init_thres*1.2, eval_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg embeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_adapt = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test_scores = compute_plda_score(eval_total_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "for idx in range(3, len(eval_test_scores)-n_adapt+3, 3):\n",
    "    idx = [0,1,2] + np.arange(idx, idx+n_adapt-3).tolist()\n",
    "    print(compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adapt_scores = compute_plda_score(eval_init_enr_embeds, eval_total_enr_embeds[n_enr:], plda_model_dir)\n",
    "eval_adapt_sorted_embeds = eval_total_enr_embeds[n_enr:][np.argsort(eval_adapt_scores.mean(0))]\n",
    "\n",
    "# confid based avg\n",
    "eval_high_avg_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_adapt_sorted_embeds[:n_adapt], \n",
    "                                           eval_adapt_sorted_embeds[n_adapt:].mean(0, keepdims=True)])\n",
    "eval_low_avg_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_adapt_sorted_embeds[-n_adapt:], \n",
    "                                          eval_adapt_sorted_embeds[:-n_adapt].mean(0, keepdims=True)])\n",
    "eval_mid_avg_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_adapt_sorted_embeds[:n_adapt//2], \n",
    "                                          eval_adapt_sorted_embeds[n_adapt//2:-n_adapt//2].mean(0, keepdims=True), \n",
    "                                          eval_adapt_sorted_embeds[-n_adapt//2:], ])\n",
    "eval_hist_avg_enr_embeds = []\n",
    "prev_edge = 0\n",
    "for edge in np.cumsum(np.histogram(eval_adapt_scores.mean(0), bins=n_adapt)[0]):\n",
    "    if prev_edge == edge: \n",
    "        continue\n",
    "    eval_hist_avg_enr_embeds.append(eval_adapt_sorted_embeds[prev_edge:edge].mean(0))\n",
    "    prev_edge = edge\n",
    "eval_hist_avg_enr_embeds = np.stack(eval_hist_avg_enr_embeds)\n",
    "\n",
    "# clustering based\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=n_adapt).fit(eval_adapt_sorted_embeds)\n",
    "eval_kmeans_avg_enr_embeds= kmeans.cluster_centers_\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_high_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"high avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_mid_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"mid avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_low_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"low avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_hist_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"hist avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_kmeans_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"kmeans avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adapt_scores = cosine_sim(eval_init_enr_embeds_, eval_true_adapted_embeds_)\n",
    "eval_adapt_sorted_embeds = eval_total_enr_embeds[n_enr:][np.argsort(eval_adapt_scores.mean(0))]\n",
    "\n",
    "# confid based avg\n",
    "eval_high_avg_enr_embeds = np.concatenate([eval_adapt_sorted_embeds[:n_adapt], \n",
    "                                           eval_adapt_sorted_embeds[n_adapt:].mean(0, keepdims=True)])\n",
    "eval_low_avg_enr_embeds = np.concatenate([eval_adapt_sorted_embeds[-n_adapt:], \n",
    "                                          eval_adapt_sorted_embeds[:-n_adapt].mean(0, keepdims=True)])\n",
    "eval_mid_avg_enr_embeds = np.concatenate([eval_adapt_sorted_embeds[:n_adapt//2], \n",
    "                                          eval_adapt_sorted_embeds[n_adapt//2:-n_adapt//2].mean(0, keepdims=True), \n",
    "                                          eval_adapt_sorted_embeds[-n_adapt//2:], ])\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_high_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"high avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_mid_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"mid avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_low_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"low avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative adaptation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True, sharey=True)\n",
    "ascend_eers = []\n",
    "for idx in range(0, len(eval_test_scores), 1):\n",
    "    idx = [0,1,2] + np.arange(n_enr, min(n_enr+idx, len(eval_test_scores))).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    ascend_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*ascend_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"hard trial, ascending confidence\", ax=axes[0])\n",
    "\n",
    "descend_eers = []\n",
    "for idx in range(0, len(eval_test_scores)-n_enr, 1):\n",
    "    idx = [0,1,2] + np.arange(len(eval_test_scores)-idx, len(eval_test_scores)).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    descend_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*descend_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"hard trial, descending confidence\", ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True, sharey=True)\n",
    "ascend_eers = []\n",
    "for idx in range(0, len(eval_test_scores), 1):\n",
    "    idx = [0,1,2] + np.arange(n_enr, min(n_enr+idx, len(eval_test_scores))).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    ascend_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*ascend_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"hard trial, ascending confidence\", ax=axes[0])\n",
    "\n",
    "descend_eers = []\n",
    "for idx in range(0, len(eval_test_scores)-n_enr, 1):\n",
    "    idx = [0,1,2] + np.arange(len(eval_test_scores)-idx, len(eval_test_scores)).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    descend_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*descend_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"hard trial, descending confidence\", ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"non-cumulative\")\n",
    "print(\"mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer\")\n",
    "non_cum_eers = []\n",
    "step = 5\n",
    "for idx in range(0, len(eval_test_scores), step):\n",
    "    idx = np.arange(0, n_enr).tolist() + np.arange(idx, min(idx+step, len(eval_test_scores))).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    non_cum_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*non_cum_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"non-cumulative confidence\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
