{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system_utils import compute_eer\n",
    "from batch_sv_system_utils import get_embeds, cosine_sim, compute_plda_score\n",
    "from utils import key2df, df2dict\n",
    "\n",
    "def get_id2idx(keys):\n",
    "    key_df = key2df(keys)\n",
    "    id2idx, idx2id = df2dict(key_df) \n",
    "    return id2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mkey2df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimeter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mkey2df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimeter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spk'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelimeter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'session'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelimeter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mid2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0midx2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mkey_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /host/projects/sv_experiments/exp_adaptive_sv/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "key2df??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_value(val_a, val_b, mask=None, verbose=True):\n",
    "    if mask is not None:\n",
    "        val_a = val_a[mask]\n",
    "        val_b = val_b[mask]\n",
    "    assert len(val_a) == len(val_b)\n",
    "    n = len(val_a)\n",
    "    r_inc = np.count_nonzero(val_a < val_b) / n\n",
    "    r_equal = np.count_nonzero(val_a == val_b) / n\n",
    "    r_dec = np.count_nonzero(val_a > val_b) / n\n",
    "    if verbose:\n",
    "        print(\"inc:{:.2f}, equal:{:.2f}, dec:{:.2f}\".format(r_inc, r_equal, r_dec))\n",
    "    return r_inc, r_equal, r_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_embeds\"\n",
    "sv_embeds = np.load(embed_dir + \"/sv_embeds.npy\")\n",
    "keys = pickle.load(open(embed_dir + \"/sv_keys.pkl\", \"rb\"))\n",
    "id2idx = get_id2idx(keys)\n",
    "\n",
    "plda_embed_dir = \"embeddings/voxc2_fbank64_voxc2untied_xvector/\"\n",
    "plda_model_dir = plda_embed_dir + \"plda_train/\"\n",
    "plda_sv_embeds = np.load(plda_embed_dir + \"/sv_embeds.npy\")\n",
    "plda_keys = pickle.load(open(plda_embed_dir + \"/sv_keys.pkl\", \"rb\"))\n",
    "plda_id2idx = get_id2idx(plda_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxc1_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1.csv\")\n",
    "spk_uttr_stat = voxc1_df.spk.value_counts()\n",
    "voxc1_meta = pd.read_pickle(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_meta.pkl\")\n",
    "spk2gender = voxc1_meta.Gender.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_spks = spk_uttr_stat[(spk_uttr_stat < 150)].index.tolist()\n",
    "dev_uttrs = voxc1_df[voxc1_df.spk.isin(dev_spks)][['id', 'spk', 'gender', 'session']]\n",
    "eval_spks = spk_uttr_stat[spk_uttr_stat >= 150].index.tolist()\n",
    "eval_uttrs = voxc1_df[voxc1_df.spk.isin(eval_spks)][['id', 'spk', 'gender', 'session']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate each enrollments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_plda_trial_output = pickle.load(open(\"trials/eval_165/hard_enr3x30_ntar9/batch_plda_score_eT15.pkl\", \"rb\"))\n",
    "eval_plda_score_list = [x[0] for x in eval_plda_trial_output]\n",
    "eval_plda_labels = [x[1] for x in eval_plda_trial_output]\n",
    "eval_plda_confids = [x[2] for x in eval_plda_trial_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_idx = 15\n",
    "scores, labels, confids = eval_plda_trial_output[t_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in np.argsort(confids): \n",
    "    confid = confids[idx]\n",
    "    enr_idx = [0,1,2] + [idx+3]\n",
    "    eer = compute_eer(scores[enr_idx].mean(0), labels)[0]\n",
    "    print(\"{}\\t confid:{:.4f}, eer:{:.4f}\".format(idx, confids[idx], eer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[enr_idx].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_spk = dev_spks[12]\n",
    "\n",
    "dev_target_uttrs = dev_uttrs[dev_uttrs.spk == dev_spk]\n",
    "dev_target_embeds = get_embeds(dev_target_uttrs.id, plda_sv_embeds, id2idx, norm=False)\n",
    "\n",
    "dev_nonTarget_uttrs = dev_uttrs[dev_uttrs.spk != dev_spk] \n",
    "dev_nonTarget_embeds = get_embeds(dev_nonTarget_uttrs.id, plda_sv_embeds, id2idx, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_enr = 1\n",
    "dev_adapt_scores = compute_plda_score(dev_target_embeds[:n_enr], dev_target_embeds[n_enr:],\n",
    "dev_total_enr_embeds = np.concatenate([dev_target_embeds[:n_enr], dev_adapted_embeds])\n",
    "dev_target_test_embeds = dev_targ plda_model_dir)\n",
    "dev_adapted_embeds = dev_target_embeds[n_enr:][dev_adapt_scores.mean(0) > 15]et_embeds[n_enr:][dev_adapt_scores.mean(0) <= 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_total_enr_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_target_test_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_target_norm_embeds = get_embeds(dev_target_uttrs.id, sv_embeds, id2idx, norm=True)\n",
    "dev_nonTarget_norm_embeds = get_embeds(dev_nonTarget_uttrs.id, sv_embeds, id2idx, norm=True)\n",
    "\n",
    "# dev_nonTarget_scores = cosine_sim(dev_target_norm_embeds, dev_nonTarget_norm_embeds)\n",
    "# dev_sorted_idx = np.argsort(dev_nonTarget_scores, axis=1)\n",
    "# dev_hard_utter_idx = dev_sorted_idx[:, -100:].ravel()\n",
    "dev_nonTarget_scores = cosine_sim(dev_total_enr_embeds, dev_nonTarget_norm_embeds)\n",
    "dev_sorted_idx = np.argsort(dev_nonTarget_scores.mean(0), axis=0)\n",
    "dev_hard_utter_idx = dev_sorted_idx[-2000:]\n",
    "\n",
    "dev_hard_nonTarget_uttrs = dev_nonTarget_uttrs.iloc[dev_hard_utter_idx]\n",
    "dev_hard_nonTaget_embeds = get_embeds(dev_hard_nonTarget_uttrs.id, sv_embeds, id2idx, norm=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_test_embeds = np.concatenate([dev_target_test_embeds, dev_hard_nonTaget_embeds])\n",
    "dev_test_labels = np.concatenate([np.ones(len(dev_target_test_embeds)), np.zeros(len(dev_hard_nonTaget_embeds))])\n",
    "dev_test_scores = compute_plda_score(dev_total_enr_embeds, dev_test_embeds, plda_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_eer(dev_test_scores.mean(0), dev_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_test_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cohort_uttrs = dev_uttrs.groupby(\"spk\").apply(lambda x: x.sample(n=10))\n",
    "eval_cohort_embeds = get_embeds(eval_cohort_uttrs.id, plda_sv_embeds, plda_id2idx, norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_spk = eval_spks[44]\n",
    "eval_target_uttrs = eval_uttrs[eval_uttrs.spk == eval_spk].sample(frac=1.0) # for random init enrollments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BKfZFsbpYjg\n"
     ]
    }
   ],
   "source": [
    "# target\n",
    "n_enr = 3\n",
    "len_adapt = 100\n",
    "enr_session = np.random.choice(eval_target_uttrs.session.unique(), size=1)[0]\n",
    "print(enr_session)\n",
    "enr_uttrs = eval_target_uttrs[eval_target_uttrs.session==enr_session].sample(n=n_enr, replace=True)\n",
    "eval_init_enr_embeds = get_embeds(enr_uttrs.id, plda_sv_embeds, plda_id2idx, norm=False)\n",
    "\n",
    "eval_target_uttrs = eval_target_uttrs.drop(index=enr_uttrs.index)\n",
    "eval_target_embeds = get_embeds(eval_target_uttrs.id, plda_sv_embeds, plda_id2idx, norm=False)\n",
    "eval_target_adapt_embeds = eval_target_embeds[:len_adapt]\n",
    "eval_target_test_embeds = eval_target_embeds[len_adapt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nontarget\n",
    "eval_nonTarget_uttrs = eval_uttrs[eval_uttrs.spk != eval_spk]\n",
    "nontarget_spks = np.random.choice(eval_nonTarget_uttrs.spk.unique(), 100, replace=False)\n",
    "eval_nonTarget_uttrs = eval_nonTarget_uttrs[eval_nonTarget_uttrs.spk.isin(nontarget_spks)]\n",
    "eval_nonTarget_embeds = get_embeds(eval_nonTarget_uttrs.id, plda_sv_embeds, id2idx, norm=False)\n",
    "eval_nonTarget_scores = compute_plda_score(eval_init_enr_embeds, eval_nonTarget_embeds, plda_model_dir)\n",
    "eval_nonTarget_sorted_idx = np.argsort(eval_nonTarget_scores.mean(0), axis=0)\n",
    "eval_hard_utter_idx = eval_nonTarget_sorted_idx[-len(eval_target_test_embeds)*9:] \n",
    "eval_hard_scores = eval_nonTarget_scores[:, eval_hard_utter_idx]\n",
    "eval_hard_nonTarget_uttrs = eval_nonTarget_uttrs.iloc[eval_hard_utter_idx]\n",
    "eval_hard_nonTarget_embeds = get_embeds(eval_hard_nonTarget_uttrs.id, plda_sv_embeds, plda_id2idx, norm=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real adaptation\n",
    "eT = 5\n",
    "\n",
    "eval_adapt_scores = compute_plda_score(eval_init_enr_embeds, eval_target_adapt_embeds, plda_model_dir)\n",
    "eval_true_adapted_embeds = eval_target_adapt_embeds[eval_adapt_scores.mean(0) > eT]\n",
    "true_adapt_norm_scores = np.empty(0)\n",
    "if len(eval_true_adapted_embeds) > 0:\n",
    "    adapt_cohort_scores = compute_plda_score(eval_true_adapted_embeds, eval_cohort_embeds, plda_model_dir)\n",
    "    adapt_cohort_mu = adapt_cohort_scores.mean(1)\n",
    "    adapt_cohort_std = adapt_cohort_scores.std(1)\n",
    "    true_adapt_norm_scores = (eval_adapt_scores.mean(0)[eval_adapt_scores.mean(0) > eT] - adapt_cohort_mu)/(adapt_cohort_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_false_adapted_embeds = eval_nonTarget_embeds[eval_nonTarget_scores.mean(0) > eT]\n",
    "false_adapt_norm_scores = np.empty(0)\n",
    "if len(eval_false_adapted_embeds) > 0:\n",
    "    adapt_cohort_scores = compute_plda_score(eval_false_adapted_embeds, eval_cohort_embeds, plda_model_dir)\n",
    "    adapt_cohort_mu = adapt_cohort_scores.mean(1)\n",
    "    adapt_cohort_std = adapt_cohort_scores.std(1)\n",
    "    false_adapt_norm_scores = (eval_nonTarget_scores.mean(0)[eval_nonTarget_scores.mean(0) > eT] - adapt_cohort_mu)/(adapt_cohort_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(eval_false_adapted_embeds) > 0:\n",
    "    eval_total_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_true_adapted_embeds, eval_false_adapted_embeds])\n",
    "else:\n",
    "    eval_total_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_true_adapted_embeds])\n",
    "adapt_sorted_idx = np.array([0,1,2] + (np.flip(np.argsort(np.concatenate([true_adapt_norm_scores, false_adapt_norm_scores])))+3).tolist())\n",
    "adapt_labels = np.concatenate([np.ones(len(eval_true_adapted_embeds)+3), np.zeros(len(eval_false_adapted_embeds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 512)\n",
      "(36, 512)\n",
      "(189, 512)\n"
     ]
    }
   ],
   "source": [
    "print(eval_true_adapted_embeds.shape)\n",
    "print(eval_false_adapted_embeds.shape)\n",
    "print(eval_target_test_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_sorted_idx = np.array([0,1,2] + (np.flip(\n",
    "    np.argsort(np.concatenate([eval_adapt_scores.mean(0)[eval_adapt_scores.mean(0)>eT],\n",
    "    eval_nonTarget_scores.mean(0)[eval_nonTarget_scores.mean(0)>eT]])))+3).tolist())\n",
    "adapt_labels[score_sorted_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_random_nonTarget_embeds = get_embeds(eval_nonTarget_uttrs.sample(n=5000).id, plda_sv_embeds, plda_id2idx, norm=False)  \n",
    "# eval_test_embeds = np.concatenate([eval_target_test_embeds, eval_random_nonTarget_embeds])\n",
    "# eval_test_labels = np.concatenate([np.ones(len(eval_target_test_embeds)), \n",
    "#                                            np.zeros(len(eval_random_nonTarget_embeds))])\n",
    "\n",
    "eval_test_embeds = np.concatenate([eval_target_test_embeds, eval_hard_nonTarget_embeds])\n",
    "eval_test_labels = np.concatenate([np.ones(len(eval_target_test_embeds)), \n",
    "                                           np.zeros(len(eval_hard_nonTarget_embeds))])\n",
    "\n",
    "# eval_test_embeds = np.concatenate([eval_target_test_embeds, eval_nonTarget_embeds])\n",
    "# eval_test_labels = np.concatenate([np.ones(len(eval_target_test_embeds)), \n",
    "#                                            np.zeros(len(eval_nonTarget_embeds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "(0.025867136978248117, 0.025279247501469725, 0.02645502645502651, 4.212917)\n",
      "score fusion\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 5.322588592753619)\n",
      "feat fusion\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 11.06226)\n",
      "score fusion, max\n",
      "(0.030570252792475015, 0.029394473838918283, 0.031746031746031744, 34.46714994036232)\n",
      "score fusion, std\n",
      "(0.002939447383891856, 0.0005878894767783657, 0.005291005291005346, 13.296895867345693)\n"
     ]
    }
   ],
   "source": [
    "eval_test_scores = compute_plda_score(eval_total_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "eval_cent_scores = compute_plda_score(eval_total_enr_embeds, eval_test_embeds, plda_model_dir, mean=True)\n",
    "\n",
    "# eval_adapt_scores = compute_plda_score(eval_init_enr_embeds, eval_total_enr_embeds[n_enr:], plda_model_dir)\n",
    "# eval_adapt_scores_ = cosine_sim(eval_init_enr_embeds_, eval_true_adapted_embeds_)\n",
    "# eval_test_scores = np.concatenate([eval_test_scores[:n_enr], \n",
    "#                                    eval_test_scores[n_enr:][np.argsort(eval_adapt_scores.mean(0))]])\n",
    "\n",
    "print(\"init\")\n",
    "print(compute_eer(eval_test_scores[:3].mean(0), eval_test_labels))\n",
    "print(\"score fusion\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "print(\"feat fusion\")\n",
    "print(compute_eer(eval_cent_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion, max\")\n",
    "print(compute_eer(eval_test_scores.mean(0)+eval_test_scores.max(0), eval_test_labels))\n",
    "print(\"score fusion, std\")\n",
    "print(compute_eer(eval_test_scores.mean(0)+eval_test_scores.std(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_test_embeds = np.concatenate([eval_total_enr_embeds[3:], eval_cohort_embeds], axis=0)\n",
    "eval_proxy_labels = np.concatenate([np.ones(len(eval_total_enr_embeds[3:])), np.zeros(len(eval_cohort_embeds))])\n",
    "eval_proxy_scores = compute_plda_score(eval_total_enr_embeds, proxy_test_embeds, plda_model_dir)\n",
    "proxy_eers = []\n",
    "for idx in range(0, len(eval_proxy_scores)):\n",
    "    proxy_eers.append(compute_eer(eval_proxy_scores[idx], eval_proxy_labels)[0])\n",
    "proxy_eer_sorted = np.argsort(proxy_eers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score fusion true\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 9.345337335294117)\n",
      "score fusion true, budget (random)\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 7.935584436666668)\n",
      "score fusion total (total)\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 5.322588592753619)\n",
      "\n",
      "score fusion proxy\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 9.207183736666668)\n",
      "score fusion proxy, max\n",
      "(0.0035273368606702216, 0.001763668430335097, 0.005291005291005346, 29.665143736666668)\n",
      "score fusion proxy, std\n",
      "(0.002939447383891856, 0.0005878894767783657, 0.005291005291005346, 14.220798638665805)\n",
      "\n",
      "score fusion proxy1\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 9.751398100000001)\n",
      "score fusion proxy1, max\n",
      "(0.002939447383891856, 0.0005878894767783657, 0.005291005291005346, 26.850928099999997)\n",
      "score fusion proxy1, std\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 13.106324386120471)\n"
     ]
    }
   ],
   "source": [
    "n_adapt = 30\n",
    "eval_test_proxy_scores = eval_test_scores[proxy_eer_sorted[:n_adapt]]\n",
    "eval_test_proxy1_scores = eval_test_scores[adapt_sorted_idx[:n_adapt]]\n",
    "print(\"score fusion true\")\n",
    "if len(eval_false_adapted_embeds) > 0:\n",
    "    print(compute_eer(eval_test_scores[:-len(eval_false_adapted_embeds)].mean(0), eval_test_labels))\n",
    "else:\n",
    "    print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion true, budget (random)\")\n",
    "print(compute_eer(eval_test_scores[[0,1,2] + \n",
    "                  np.random.randint(3, 3+len(eval_true_adapted_embeds), n_adapt-3).tolist()].mean(0), \n",
    "                  eval_test_labels))\n",
    "print(\"score fusion total (total)\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "# print(\"score fusion total budget (worst)\")\n",
    "# print(compute_eer(eval_test_scores[[0,1,2] + np.arange(-n_adapt, 0).tolist()].mean(0), eval_test_labels))\n",
    "print()\n",
    "print(\"score fusion proxy\")\n",
    "print(compute_eer(eval_test_proxy_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion proxy, max\")\n",
    "print(compute_eer(eval_test_proxy_scores.mean(0)+eval_test_proxy_scores.max(0), eval_test_labels))\n",
    "print(\"score fusion proxy, std\")\n",
    "print(compute_eer(eval_test_proxy_scores.mean(0)+eval_test_proxy_scores.std(0), eval_test_labels))\n",
    "\n",
    "print()\n",
    "print(\"score fusion proxy1\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion proxy1, max\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0)+eval_test_proxy1_scores.max(0), eval_test_labels))\n",
    "print(\"score fusion proxy1, std\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0)+eval_test_proxy1_scores.std(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score fusion proxy1\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 14.99114)\n",
      "score fusion proxy1, max\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 29.98228)\n",
      "score fusion proxy1, std\n",
      "(0.002645502645502673, 0.0, 0.005291005291005346, 14.99114)\n"
     ]
    }
   ],
   "source": [
    "eval_test_proxy1_scores = compute_plda_score(eval_total_enr_embeds[adapt_sorted_idx[:n_adapt]], eval_test_embeds, plda_model_dir, mean=True)\n",
    "print(\"score fusion proxy1\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0), eval_test_labels))\n",
    "print(\"score fusion proxy1, max\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0)+eval_test_proxy1_scores.max(0), eval_test_labels))\n",
    "print(\"score fusion proxy1, std\")\n",
    "print(compute_eer(eval_test_proxy1_scores.mean(0)+eval_test_proxy1_scores.std(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01368712660354125\n",
      "0.01368712660354125\n",
      "\n",
      "0.01368712660354125\n",
      "0.01368712660354125\n",
      "\n",
      "0.01368712660354125\n",
      "0.01368712660354125\n",
      "\n",
      "0.01625134657698326\n",
      "0.011467654437618859\n",
      "\n",
      "0.016228451755791812\n",
      "0.011170021762130054\n",
      "\n",
      "0.01480897284192213\n",
      "0.009979491060174837\n",
      "\n",
      "0.015060815875028041\n",
      "0.010528966768769552\n",
      "\n",
      "0.011444759616427411\n",
      "0.009338436066814334\n",
      "\n",
      "0.010963968371407035\n",
      "0.014969236590262254\n",
      "\n",
      "0.011307390689278732\n",
      "0.015106605517410934\n",
      "\n",
      "0.014648709093582003\n",
      "0.010551861589961\n",
      "\n",
      "0.011605023364767538\n",
      "0.011055547656172822\n",
      "\n",
      "0.010506071947578105\n",
      "0.011513444080001751\n",
      "\n",
      "0.00949869981515446\n",
      "0.008239484649624903\n",
      "\n",
      "0.010300018556855087\n",
      "0.009223961960857101\n",
      "\n",
      "0.010322913378046535\n",
      "0.008674486252262387\n",
      "\n",
      "0.010437387484003767\n",
      "0.008445538040347922\n",
      "\n",
      "0.009201067139665654\n",
      "0.00963606874230314\n",
      "\n",
      "0.008995013748942637\n",
      "0.007964746795327545\n",
      "\n",
      "0.009155277497282763\n",
      "0.007987641616518992\n",
      "\n",
      "0.009132382676091316\n",
      "0.008102115722476225\n",
      "\n",
      "0.008788960358219619\n",
      "0.008102115722476225\n",
      "\n",
      "0.008811855179411065\n",
      "0.007758693404604526\n",
      "\n",
      "0.008674486252262387\n",
      "0.007483955550307168\n",
      "\n",
      "0.008216589828433457\n",
      "0.008147905364859117\n",
      "\n",
      "0.007827377868178866\n",
      "0.008216589828433457\n",
      "\n",
      "0.007048953947669685\n",
      "0.008491327682730814\n",
      "\n",
      "0.006797110914563774\n",
      "0.0075068503714986155\n",
      "\n",
      "0.01348239870246615\n",
      "0.007690008941030187\n",
      "\n",
      "0.013528188344849043\n",
      "0.007209217696009811\n",
      "\n",
      "0.013528188344849043\n",
      "0.007232112517201256\n",
      "\n",
      "0.014146348517018098\n",
      "0.007369481444349936\n",
      "\n",
      "0.013551083166040489\n",
      "0.007117638411244025\n",
      "\n",
      "0.013963189947486526\n",
      "0.007071848768861131\n",
      "\n",
      "0.013619767629614829\n",
      "0.006705531629797988\n",
      "\n",
      "0.006888690199329559\n",
      "0.014443981192506903\n",
      "\n",
      "0.006865795378138113\n",
      "0.014237927801783885\n",
      "\n",
      "0.006865795378138113\n",
      "0.014306612265358225\n",
      "\n",
      "0.013299240132934579\n",
      "0.013367924596508918\n",
      "\n",
      "0.013207660848168792\n",
      "0.0138029261991464\n",
      "\n",
      "0.01277265924553131\n",
      "0.013436609060083256\n",
      "\n",
      "0.012543711033616844\n",
      "0.014123453695826653\n",
      "\n",
      "0.01309318674221156\n",
      "0.013436609060083256\n",
      "\n",
      "0.013230555669360239\n",
      "0.013001607457445774\n",
      "\n",
      "0.012887133351488542\n",
      "0.013367924596508918\n",
      "\n",
      "0.012749764424339862\n",
      "0.012223183536936593\n",
      "\n",
      "0.012978712636254327\n",
      "0.012085814609787915\n",
      "\n",
      "0.01277265924553131\n",
      "0.011971340503830682\n",
      "\n",
      "0.01249792139123395\n",
      "0.01165081300715043\n",
      "\n",
      "0.012429236927659612\n",
      "0.011421864795235966\n",
      "\n",
      "0.01185686639787345\n",
      "0.011307390689278732\n",
      "\n",
      "0.011055547656172822\n",
      "0.0111929165833215\n",
      "\n",
      "0.011170021762130054\n",
      "0.010918178729024144\n",
      "\n",
      "0.011833971576682003\n",
      "0.01027712373566364\n",
      "\n",
      "0.010986863192598482\n",
      "0.01048317712638666\n",
      "\n",
      "0.011307390689278732\n",
      "0.010208439272089302\n",
      "\n",
      "0.011742392291916218\n",
      "0.010254228914472193\n",
      "\n",
      "0.011215811404512947\n",
      "0.010025280702557728\n",
      "\n",
      "0.011948445682639235\n",
      "0.009842122133026158\n",
      "\n",
      "0.012017130146213573\n",
      "0.009727648027068925\n",
      "\n",
      "0.011582128543576091\n",
      "0.00981922731183471\n",
      "\n",
      "0.01231476282170238\n",
      "0.009613173921111693\n",
      "\n",
      "0.012223183536936593\n",
      "0.009567384278728799\n",
      "\n",
      "0.011742392291916218\n",
      "0.009544489457537353\n",
      "\n",
      "0.012406342106468165\n",
      "0.009544489457537353\n",
      "\n",
      "0.012978712636254327\n",
      "0.009452910172771566\n",
      "\n",
      "0.012864238530297094\n",
      "0.006500803728722887\n",
      "\n",
      "0.012818448887914202\n",
      "0.009384225709197228\n",
      "\n",
      "0.012795554066722756\n",
      "0.00943001535158012\n",
      "\n",
      "0.012749764424339862\n",
      "0.00654659337110578\n",
      "\n",
      "0.013253450490551684\n",
      "0.00949869981515446\n",
      "\n",
      "0.00826237947081635\n",
      "0.009361330888005781\n",
      "\n",
      "0.008102115722476225\n",
      "0.009704753205877478\n",
      "\n",
      "0.013024502278637221\n",
      "0.009613173921111693\n",
      "\n",
      "0.008468432861539367\n",
      "0.009452910172771566\n",
      "\n",
      "0.008399748397965028\n",
      "0.009452910172771566\n",
      "\n",
      "0.008880539642985404\n",
      "0.01009396516613207\n",
      "\n",
      "0.009292646424431441\n",
      "0.009315541245622888\n",
      "\n",
      "0.009842122133026158\n",
      "0.010025280702557728\n",
      "\n",
      "0.01027712373566364\n",
      "0.00995659623898339\n",
      "\n",
      "0.010643440874726785\n",
      "0.00995659623898339\n",
      "\n",
      "0.010895283907832697\n",
      "0.009613173921111693\n",
      "\n",
      "0.011170021762130054\n",
      "0.009407120530388674\n",
      "\n",
      "0.011238706225704394\n",
      "0.009407120530388674\n",
      "\n",
      "0.011536338901193198\n",
      "0.009246856782048549\n",
      "\n",
      "0.011696602649533323\n",
      "0.00981922731183471\n",
      "\n",
      "0.01467160391477345\n",
      "0.009681858384686031\n",
      "\n",
      "0.01467160391477345\n",
      "0.010208439272089302\n",
      "\n",
      "0.0149921314114537\n",
      "0.010849494265449802\n",
      "\n",
      "0.014877657305496467\n",
      "0.010620546053535337\n",
      "\n",
      "0.015083710696219487\n",
      "0.010528966768769552\n",
      "\n",
      "0.014900552126687915\n",
      "0.010437387484003767\n",
      "\n",
      "0.015083710696219487\n",
      "0.010895283907832697\n",
      "\n",
      "0.015152395159793827\n",
      "0.011353180331661626\n",
      "\n",
      "0.01519818480217672\n",
      "0.011284495868087286\n",
      "\n",
      "0.015083710696219487\n",
      "0.011673707828341876\n",
      "\n",
      "0.015060815875028041\n",
      "0.011971340503830682\n",
      "\n",
      "0.015152395159793827\n",
      "0.011879761219064896\n",
      "\n",
      "0.015358448550516844\n",
      "0.012337657642893825\n",
      "\n",
      "0.015564501941239864\n",
      "0.012658185139574077\n",
      "\n",
      "0.015610291583622756\n",
      "0.012932922993871434\n",
      "\n",
      "0.013070291921020112\n",
      "0.008399748397965028\n",
      "\n",
      "0.015816344974345774\n",
      "0.008422643219156475\n",
      "\n",
      "0.015930819080303006\n",
      "0.00826237947081635\n",
      "\n",
      "0.016182662113408918\n",
      "0.008102115722476225\n",
      "\n",
      "0.016320031040557598\n",
      "0.008537117325113707\n",
      "\n",
      "0.016342925861749045\n",
      "0.008880539642985404\n",
      "\n",
      "0.016388715504131936\n",
      "0.009338436066814334\n",
      "\n",
      "0.016480294788897724\n",
      "0.009750542848260373\n",
      "\n",
      "0.01652608443128062\n",
      "0.010048175523749175\n",
      "\n",
      "0.01652608443128062\n",
      "0.01048317712638666\n",
      "\n",
      "0.016846611927960865\n",
      "0.010231334093280748\n",
      "\n",
      "0.016938191212726654\n",
      "0.010208439272089302\n",
      "\n",
      "0.019821613193200936\n",
      "0.01048317712638666\n",
      "\n",
      "0.019981876941541063\n",
      "0.010689230517109679\n",
      "\n",
      "0.02014214068988119\n",
      "0.010918178729024144\n",
      "\n",
      "0.02046266818656144\n",
      "0.011078442477364267\n",
      "\n",
      "0.02041687854417855\n",
      "0.011215811404512947\n",
      "\n",
      "0.020256614795838422\n",
      "0.011307390689278732\n",
      "\n",
      "0.020622931934901566\n",
      "0.011673707828341876\n",
      "\n",
      "0.02092056461039037\n",
      "0.01453423498762477\n",
      "\n",
      "0.0210350387163476\n",
      "0.014763183199539235\n",
      "\n",
      "0.021149512822304838\n",
      "0.01480897284192213\n",
      "\n",
      "0.020943459431581816\n",
      "0.015015026232645147\n",
      "\n",
      "0.021012143895156155\n",
      "0.015243974444559612\n",
      "\n",
      "0.021103723179921943\n",
      "0.015335553729325399\n",
      "\n",
      "0.02151582996136798\n",
      "0.015289764086942506\n",
      "\n",
      "0.021859252279239676\n",
      "0.015427133014091186\n",
      "\n",
      "0.022042410848771246\n",
      "0.015518712298856971\n",
      "\n",
      "0.02215688495472848\n",
      "0.015495817477665524\n",
      "\n",
      "0.022340043524260052\n",
      "0.015633186404814203\n",
      "\n",
      "0.022019516027579802\n",
      "0.01577055533196288\n",
      "\n",
      "0.02510899139877716\n",
      "0.01583923979553722\n",
      "\n",
      "0.025406624074265964\n",
      "0.015885029437920112\n",
      "\n",
      "0.02545241371664886\n",
      "0.015885029437920112\n",
      "\n",
      "0.02538372925307452\n",
      "0.01577055533196288\n",
      "\n",
      "0.025543993001414644\n",
      "0.015885029437920112\n",
      "\n",
      "0.02602478424643502\n",
      "0.01579345015315433\n",
      "\n",
      "0.02602478424643502\n",
      "0.015862134616728668\n",
      "\n",
      "0.026276627279540932\n",
      "0.0159766087226859\n",
      "\n",
      "0.025933204961669235\n",
      "0.016320031040557598\n",
      "\n",
      "0.0261621531735837\n",
      "0.016205556934600365\n",
      "\n",
      "0.026139258352392253\n",
      "0.016457399967706277\n",
      "\n",
      "0.026368206564306718\n",
      "0.016548979252472062\n",
      "\n",
      "0.026597154776221182\n",
      "0.016663453358429295\n",
      "\n",
      "0.02675741852456131\n",
      "0.016915296391535207\n",
      "\n",
      "0.026917682272901436\n",
      "0.017052665318683886\n",
      "\n",
      "0.026848997809327094\n",
      "0.017052665318683886\n",
      "\n",
      "0.02668873406098697\n",
      "0.019775823550818045\n",
      "\n",
      "0.026505575491455397\n",
      "0.01986740283558383\n",
      "\n",
      "0.026436891027881056\n",
      "0.01995898212034962\n",
      "\n",
      "0.026345311743115274\n",
      "0.02014214068988119\n",
      "\n",
      "0.026597154776221182\n",
      "0.020371088901795655\n",
      "\n",
      "0.026826102988135647\n",
      "0.02032529925941276\n",
      "\n",
      "0.026963471915284327\n",
      "0.02041687854417855\n",
      "\n",
      "0.026826102988135647\n",
      "0.020554247471327225\n",
      "\n",
      "0.02648268067026395\n",
      "0.020668721577284457\n",
      "\n",
      "0.026391101385498165\n",
      "0.02087477496800748\n",
      "\n",
      "0.026459785849072506\n",
      "0.021103723179921943\n",
      "\n",
      "0.026528470312646844\n",
      "0.021103723179921943\n",
      "\n",
      "0.02629952210073238\n",
      "0.021126618001113387\n",
      "\n",
      "0.026391101385498165\n",
      "0.020966354252773264\n",
      "\n",
      "0.02666583923979552\n",
      "0.020897669789198922\n",
      "\n",
      "0.026917682272901436\n",
      "0.021149512822304838\n",
      "\n",
      "0.027009261557667218\n",
      "0.02126398692826207\n",
      "\n",
      "0.027009261557667218\n",
      "0.021286881749453514\n",
      "\n",
      "0.026986366736475774\n",
      "0.021355566213027852\n",
      "\n",
      "0.026963471915284327\n",
      "0.021653198888516658\n",
      "\n",
      "0.02712373566362445\n",
      "0.021721883352091\n",
      "\n",
      "0.026917682272901436\n",
      "0.021950831564005464\n",
      "\n",
      "0.026826102988135647\n",
      "0.02215688495472848\n",
      "\n",
      "0.026642944418604077\n",
      "0.02208820049115414\n",
      "\n",
      "0.02666583923979552\n",
      "0.02215688495472848\n",
      "\n",
      "0.02657425995502974\n",
      "0.025017412114011373\n",
      "\n",
      "0.026459785849072506\n",
      "0.025040306935202823\n",
      "\n",
      "0.026459785849072506\n",
      "0.025017412114011373\n",
      "\n",
      "0.026322416921923823\n",
      "0.025131886219968605\n",
      "\n",
      "0.02602478424643502\n",
      "0.0251776758623515\n",
      "\n",
      "0.025612677464988985\n",
      "0.025360834431883073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(adapt_sorted_idx)):\n",
    "    print(compute_eer(eval_test_scores[proxy_eer_sorted[:i]].mean(0), eval_test_labels)[0])\n",
    "    print(compute_eer(eval_test_scores[adapt_sorted_idx[:i]].mean(0), eval_test_labels)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### score normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-norm\n",
    "enr_cohort_scores = compute_plda_score(eval_total_enr_embeds, eval_cohort_embeds, plda_model_dir)\n",
    "enr_cohort_mu = enr_cohort_scores.mean(1, keepdims=True)\n",
    "enr_cohort_std = enr_cohort_scores.std(1, keepdims=True)\n",
    "eval_test_norm_scores = (eval_test_scores - enr_cohort_mu) / enr_cohort_std\n",
    "print(compute_eer(eval_test_norm_scores.mean(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-norm\n",
    "a = 1\n",
    "client_scores = compute_plda_score(eval_total_enr_embeds, eval_total_enr_embeds, plda_model_dir)\n",
    "client_mean = np.triu(client_scores, 1).mean()\n",
    "imp_scores = compute_plda_score(eval_total_enr_embeds, eval_cohort_embeds, plda_model_dir)\n",
    "imp_mean = imp_score.mean()\n",
    "f_score = (eval_test_scores-imp_mean)*(2*a/(client_mean - imp_mean)) + a\n",
    "print(compute_eer(f_score.mean(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max effect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_test_scores = eval_test_scores.mean(0)+eval_test_scores.max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(np.argmax(eval_test_scores, axis=0), minlength=len(eval_test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_thres = compute_eer(adapt_test_scores, eval_test_labels)[-1]\n",
    "adapt_fn_idx = np.nonzero((adapt_test_scores < adapt_thres) & (eval_test_labels == 1))\n",
    "adapt_fp_idx = np.nonzero((adapt_test_scores > adapt_thres) & (eval_test_labels == 0))\n",
    "max_thres = compute_eer(max_test_scores, eval_test_labels)[-1]\n",
    "max_fn_idx = np.nonzero((max_test_scores < max_thres) & (eval_test_labels == 1))\n",
    "max_fp_idx = np.nonzero((max_test_scores > max_thres) & (eval_test_labels == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapt_fp_idx)\n",
    "print(adapt_test_scores[adapt_fp_idx])\n",
    "print(max_fp_idx)\n",
    "print(max_test_scores[max_fp_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapt_fn_idx)\n",
    "print(adapt_test_scores[adapt_fn_idx])\n",
    "print(max_fn_idx)\n",
    "print(max_test_scores[max_fn_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### std effect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_test_scores = eval_test_scores.mean(0)+eval_test_scores.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_thres = compute_eer(adapt_test_scores, eval_test_labels)[-1]\n",
    "adapt_fn_idx = np.nonzero((adapt_test_scores < adapt_thres) & (eval_test_labels == 1))\n",
    "adapt_fp_idx = np.nonzero((adapt_test_scores > adapt_thres) & (eval_test_labels == 0))\n",
    "std_thres = compute_eer(std_test_scores, eval_test_labels)[-1]\n",
    "std_fn_idx = np.nonzero((std_test_scores < std_thres) & (eval_test_labels == 1))\n",
    "std_fp_idx = np.nonzero((std_test_scores > std_thres) & (eval_test_labels == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapt_fp_idx)\n",
    "print(adapt_test_scores[adapt_fp_idx])\n",
    "print(std_fp_idx)\n",
    "print(std_test_scores[std_fp_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapt_fn_idx)\n",
    "print(adapt_test_scores[adapt_fn_idx])\n",
    "print(std_fn_idx)\n",
    "print(std_test_scores[std_fn_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adapt effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_test_scores = eval_test_scores[:3].mean(0)\n",
    "adapt_test_scores = eval_test_scores.mean(0)\n",
    "# eval_avg_test_scores = compute_plda_score(eval_high_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "# adapt_test_scores = eval_avg_test_scores.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 2\n",
    "for idx in range(3, len(eval_test_scores), step):\n",
    "    print(\"{} to {}\".format(idx, idx+step))\n",
    "    compare_value(init_test_scores, eval_test_scores[idx:idx+step].mean(0))\n",
    "#     compare_value(init_test_scores, eval_test_scores[idx:idx+step].mean(0), eval_test_labels==1)\n",
    "#     compare_value(init_test_scores, eval_test_scores[idx:idx+step].mean(0), eval_test_labels==0)\n",
    "    print(compute_eer(eval_test_scores[idx:idx+step].mean(0), eval_test_labels)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative\n",
    "for idx in range(3, len(eval_test_scores), 3):\n",
    "    compare_value(init_test_scores, eval_test_scores[3:idx+3].mean(0), eval_test_labels==1)\n",
    "    compare_value(init_test_scores, eval_test_scores[3:idx+3].mean(0), eval_test_labels==0)\n",
    "    print(compute_eer(eval_test_scores[3:idx+3].mean(0), eval_test_labels)[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compute_plda_score(eval_total_enr_embeds[[9]], eval_test_embeds, plda_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_thres = compute_eer(init_test_scores, eval_test_labels)[-1]\n",
    "init_fn_idx = np.nonzero((init_test_scores < init_thres) & (eval_test_labels == 1))\n",
    "init_fp_idx = np.nonzero((init_test_scores > init_thres) & (eval_test_labels == 0))\n",
    "adapt_thres = compute_eer(adapt_test_scores, eval_test_labels)[-1]\n",
    "adapt_fn_idx = np.nonzero((adapt_test_scores < adapt_thres) & (eval_test_labels == 1))\n",
    "adapt_fp_idx = np.nonzero((adapt_test_scores > adapt_thres) & (eval_test_labels == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(init_fp_idx)\n",
    "print(init_test_scores[init_fp_idx])\n",
    "print(adapt_fp_idx)\n",
    "print(adapt_test_scores[adapt_fp_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(init_fn_idx)\n",
    "print(init_test_scores[init_fn_idx])\n",
    "print(adapt_fn_idx)\n",
    "print(adapt_test_scores[adapt_fn_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system_utils import compute_error\n",
    "compute_error(init_test_scores > init_thres, eval_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system_utils import compute_error\n",
    "compute_error(adapt_test_scores > init_thres*1.2, eval_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avg embeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_adapt = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test_scores = compute_plda_score(eval_total_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "for idx in range(3, len(eval_test_scores)-n_adapt+3, 3):\n",
    "    idx = [0,1,2] + np.arange(idx, idx+n_adapt-3).tolist()\n",
    "    print(compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adapt_scores = compute_plda_score(eval_init_enr_embeds, eval_total_enr_embeds[n_enr:], plda_model_dir)\n",
    "eval_adapt_sorted_embeds = eval_total_enr_embeds[n_enr:][np.argsort(eval_adapt_scores.mean(0))]\n",
    "\n",
    "# confid based avg\n",
    "eval_high_avg_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_adapt_sorted_embeds[:n_adapt], \n",
    "                                           eval_adapt_sorted_embeds[n_adapt:].mean(0, keepdims=True)])\n",
    "eval_low_avg_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_adapt_sorted_embeds[-n_adapt:], \n",
    "                                          eval_adapt_sorted_embeds[:-n_adapt].mean(0, keepdims=True)])\n",
    "eval_mid_avg_enr_embeds = np.concatenate([eval_init_enr_embeds, eval_adapt_sorted_embeds[:n_adapt//2], \n",
    "                                          eval_adapt_sorted_embeds[n_adapt//2:-n_adapt//2].mean(0, keepdims=True), \n",
    "                                          eval_adapt_sorted_embeds[-n_adapt//2:], ])\n",
    "eval_hist_avg_enr_embeds = []\n",
    "prev_edge = 0\n",
    "for edge in np.cumsum(np.histogram(eval_adapt_scores.mean(0), bins=n_adapt)[0]):\n",
    "    if prev_edge == edge: \n",
    "        continue\n",
    "    eval_hist_avg_enr_embeds.append(eval_adapt_sorted_embeds[prev_edge:edge].mean(0))\n",
    "    prev_edge = edge\n",
    "eval_hist_avg_enr_embeds = np.stack(eval_hist_avg_enr_embeds)\n",
    "\n",
    "# clustering based\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=n_adapt).fit(eval_adapt_sorted_embeds)\n",
    "eval_kmeans_avg_enr_embeds= kmeans.cluster_centers_\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_high_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"high avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_mid_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"mid avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_low_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"low avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_hist_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"hist avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_kmeans_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"kmeans avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_adapt_scores = cosine_sim(eval_init_enr_embeds_, eval_true_adapted_embeds_)\n",
    "eval_adapt_sorted_embeds = eval_total_enr_embeds[n_enr:][np.argsort(eval_adapt_scores.mean(0))]\n",
    "\n",
    "# confid based avg\n",
    "eval_high_avg_enr_embeds = np.concatenate([eval_adapt_sorted_embeds[:n_adapt], \n",
    "                                           eval_adapt_sorted_embeds[n_adapt:].mean(0, keepdims=True)])\n",
    "eval_low_avg_enr_embeds = np.concatenate([eval_adapt_sorted_embeds[-n_adapt:], \n",
    "                                          eval_adapt_sorted_embeds[:-n_adapt].mean(0, keepdims=True)])\n",
    "eval_mid_avg_enr_embeds = np.concatenate([eval_adapt_sorted_embeds[:n_adapt//2], \n",
    "                                          eval_adapt_sorted_embeds[n_adapt//2:-n_adapt//2].mean(0, keepdims=True), \n",
    "                                          eval_adapt_sorted_embeds[-n_adapt//2:], ])\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_high_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"high avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_mid_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"mid avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))\n",
    "\n",
    "eval_test_scores = compute_plda_score(eval_low_avg_enr_embeds, eval_test_embeds, plda_model_dir)\n",
    "print(\"low avg\")\n",
    "print(compute_eer(eval_test_scores.mean(0), eval_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative adaptation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True, sharey=True)\n",
    "ascend_eers = []\n",
    "for idx in range(0, len(eval_test_scores), 1):\n",
    "    idx = [0,1,2] + np.arange(n_enr, min(n_enr+idx, len(eval_test_scores))).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    ascend_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*ascend_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"hard trial, ascending confidence\", ax=axes[0])\n",
    "\n",
    "descend_eers = []\n",
    "for idx in range(0, len(eval_test_scores)-n_enr, 1):\n",
    "    idx = [0,1,2] + np.arange(len(eval_test_scores)-idx, len(eval_test_scores)).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    descend_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*descend_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"hard trial, descending confidence\", ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex=True, sharey=True)\n",
    "ascend_eers = []\n",
    "for idx in range(0, len(eval_test_scores), 1):\n",
    "    idx = [0,1,2] + np.arange(n_enr, min(n_enr+idx, len(eval_test_scores))).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    ascend_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*ascend_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"hard trial, ascending confidence\", ax=axes[0])\n",
    "\n",
    "descend_eers = []\n",
    "for idx in range(0, len(eval_test_scores)-n_enr, 1):\n",
    "    idx = [0,1,2] + np.arange(len(eval_test_scores)-idx, len(eval_test_scores)).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    descend_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*descend_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"hard trial, descending confidence\", ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"non-cumulative\")\n",
    "print(\"mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer\")\n",
    "non_cum_eers = []\n",
    "step = 5\n",
    "for idx in range(0, len(eval_test_scores), step):\n",
    "    idx = np.arange(0, n_enr).tolist() + np.arange(idx, min(idx+step, len(eval_test_scores))).tolist()\n",
    "    mean_eer = compute_eer(eval_test_scores[idx].mean(0), eval_test_labels)[0]\n",
    "    mean_max_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0), eval_test_labels)[0]\n",
    "    mean_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].std(0), eval_test_labels)[0]\n",
    "    mean_max_std_eer = compute_eer(eval_test_scores[idx].mean(0)+eval_test_scores[idx].max(0)+eval_test_scores[idx].std(0),\n",
    "                                   eval_test_labels)[0]\n",
    "    non_cum_eers.append((mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "#     print(\"{:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(mean_eer, mean_max_eer, mean_std_eer, mean_max_std_eer))\n",
    "\n",
    "eer_stat = pd.DataFrame(list(zip(*non_cum_eers))).T\n",
    "eer_stat.columns = ['mean', 'mean_max', 'mean_std', 'mean_max_std']\n",
    "eer_stat.plot(figsize=(20,10), title=\"non-cumulative confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_sv_system_utils import plot_score\n",
    "plot_score(eval_test_scores[20:40].mean(0), eval_test_labels, 10, title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
