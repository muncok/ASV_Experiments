{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sv_system.dnn.train.si_train import set_seed\n",
    "from sv_system.dnn.utils import secToSample, secToSample\n",
    "import torch.nn.functional as F\n",
    "from sv_system.dnn.data.dataloader import init_default_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.dnn.utils.score_utils import compute_eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.dnn.utils.parser import get_sv_parser\n",
    "options = get_sv_parser().parse_args(args=[])\n",
    "options.n_dct_filters = 40\n",
    "options.n_mels = 40\n",
    "options.timeshift_ms = 100\n",
    "options.data_folder = \"/home/muncok/DL/dataset/SV_sets\"\n",
    "options.window_size= 0.025\n",
    "options.window_stride= 0.010\n",
    "options.cache_size = 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.input_format = 'fbank'\n",
    "options.input_clip = True\n",
    "options.input_length = secToSample(1)\n",
    "options.splice_frames = secToSample(0.1)//160+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.data_folder = \"/home/muncok/DL/dataset/SV_sets/speech_commands/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SI_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from models/voxc/si_train/full_train/si_voxc_res15_0.1s_full_fbank.pt\n"
     ]
    }
   ],
   "source": [
    "from dnn.model.SpeechModel import SpeechResModel\n",
    "model = SpeechResModel(\"res15\", 10)\n",
    "model.load_partial(\"models/voxc/si_train/full_train/si_voxc_res15_0.1s_full_fbank.pt\")\n",
    "model.cuda()\n",
    "lda = None  # None means not using lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from models/commands/equal_num_102spk_dot1.pt\n"
     ]
    }
   ],
   "source": [
    "from sv_system.dnn.model.AuxModels import SimpleCNN\n",
    "options.input = \"models/commands/equal_num_102spk_dot1.pt\"\n",
    "options.model = \"SimpleCNN\"\n",
    "model = SimpleCNN(1880, options.splice_frames)\n",
    "model.load(options.input)\n",
    "lda = pickle.load(open(\"models/lda/equal_num_102spk_dot1_splice_lda.pkl\", \"rb\"))\n",
    "# lda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (convb_1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (convb_2): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (convb_3): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (convb_4): Sequential(\n",
       "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (output): Linear(in_features=320, out_features=1880, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.dnn.sv_score import embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.data_folder = '/home/muncok/DL/dataset/SV_sets/speech_commands/'\n",
    "enroll_df = pd.read_pickle('./trials/commands/final/equal_num_102spk_enroll.pkl')\n",
    "pos_test_df = pd.read_pickle('./trials/commands/final/equal_num_102spk_pos_test.pkl')\n",
    "neg_test_df = pd.read_pickle(\"./trials/commands/final/equal_num_102spk_neg_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "enroll_spks = enroll_df.spk.unique().tolist()\n",
    "\n",
    "user_spks = enroll_spks[:2]\n",
    "enroll_spks = enroll_spks[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.dnn.data.dataset import SpeechDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA is loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = SpeechDataset(SpeechDataset.read_df(options.data_folder, enroll_df), \"test\", vars(options))\n",
    "val_dataloader = init_default_loader(dataset)\n",
    "enroll_embeddings, _ = embeds(options, val_dataloader, model, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA is loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = SpeechDataset(SpeechDataset.read_df(options.data_folder, pos_test_df), \"test\", vars(options))\n",
    "val_dataloader = init_default_loader(dataset)\n",
    "pos_embeddings, _ = embeds(options, val_dataloader, model, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA is loaded\n"
     ]
    }
   ],
   "source": [
    "dataset = SpeechDataset(SpeechDataset.read_df(options.data_folder, neg_test_df), \"test\", vars(options))\n",
    "val_dataloader = init_default_loader(dataset)\n",
    "imposter_embeddings, _ = embeds(options, val_dataloader, model, lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Not Seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_average = 1 # number of average means the number of words in a sentence\n",
    "n_enroll_uttrs = 40 # number of enroll uttrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_models = dict()\n",
    "for spk in enroll_spks:\n",
    "    enroll_idx = np.nonzero(enroll_df.spk == spk)\n",
    "    spk_models[spk] = enroll_embeddings[enroll_idx][:n_enroll_uttrs].mean(0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "uni\n",
      "eer:3.469, thres:0.3095\n"
     ]
    }
   ],
   "source": [
    "# average embedding --> scoring\n",
    "\n",
    "pos_scores = dict()\n",
    "neg_scores = dict()\n",
    "\n",
    "for spk in enroll_spks:\n",
    "    pos_test_idx = np.nonzero((pos_test_df.spk == spk))\n",
    "    pos_embeds = pos_embeddings[pos_test_idx]\n",
    "    pos_scores[spk] = F.cosine_similarity(pos_embeds,\n",
    "                                          spk_models[spk])\n",
    "   \n",
    "    neg_scores[spk] = F.cosine_similarity(imposter_embeddings, spk_models[spk])\n",
    "\n",
    "# for spk in enroll_spks:\n",
    "#     print(spk)\n",
    "#     compute_eer(pos_scores[spk], neg_scores[spk])\n",
    "print(\"\\nuni\")\n",
    "uni_pos_scores = np.concatenate([v for v in pos_scores.values()])\n",
    "uni_neg_scores = np.concatenate([v for v in neg_scores.values()])\n",
    "compute_eer(uni_pos_scores, uni_neg_scores)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on user speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_enroll_uttrs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_models = dict()\n",
    "thres = 0.7\n",
    "for spk in user_spks:\n",
    "    enroll_idx = np.nonzero(enroll_df.spk == spk)\n",
    "    spk_models[spk] = enroll_embeddings[enroll_idx][:n_enroll_uttrs].mean(0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0ff728b5\n",
      "eer:5.714, thres:0.3951\n",
      "\n",
      "uni\n",
      "eer:5.714, thres:0.3951\n"
     ]
    }
   ],
   "source": [
    "pos_scores = dict()\n",
    "neg_scores = dict()\n",
    "additional_embeds = {spk:[] for spk in user_spks}\n",
    "for spk in user_spks:\n",
    "    pos_embeds = pos_embeddings[np.nonzero(pos_test_df.spk == spk)]\n",
    "    pos_scores[spk] = F.cosine_similarity(pos_embeds,\n",
    "                                          spk_models[spk])\n",
    "    additional_embeds[spk].append(pos_embeds[pos_scores[spk] > thres])    \n",
    "#     print(\"additional_pos: {}\".format(np.count_nonzero(pos_scores[spk] > thres)))\n",
    "    \n",
    "    neg_scores[spk] = F.cosine_similarity(imposter_embeddings, spk_models[spk])\n",
    "    additional_embeds[spk].append(imposter_embeddings[neg_scores[spk] > thres])  \n",
    "#     print(\"additional_neg: {}\".format(np.count_nonzero(neg_scores[spk] > thres)))\n",
    "    \n",
    "    print(spk)\n",
    "    compute_eer(pos_scores[spk], neg_scores[spk])\n",
    "    \n",
    "print(\"\\nuni\")\n",
    "uni_pos_scores = np.concatenate([v for v in pos_scores.values()])\n",
    "uni_neg_scores = np.concatenate([v for v in neg_scores.values()])\n",
    "compute_eer(uni_pos_scores, uni_neg_scores)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### provide additional info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spk: 0ff728b5 has 2 additional_info\n"
     ]
    }
   ],
   "source": [
    "for spk in user_spks:\n",
    "    additional_spk_info = torch.cat(additional_embeds[spk])\n",
    "    if len(additional_spk_info) == 0: continue\n",
    "    print(\"spk: {} has {} additional_info\".format(spk, len (additional_spk_info)))\n",
    "    spk_models[spk] = (spk_models[spk] + additional_spk_info.mean(0, True)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0ff728b5\n",
      "eer:5.714, thres:0.4016\n",
      "\n",
      "uni\n",
      "eer:5.714, thres:0.4016\n"
     ]
    }
   ],
   "source": [
    "pos_scores = dict()\n",
    "neg_scores = dict()\n",
    "\n",
    "for spk in user_spks:\n",
    "    pos_test_idx = np.nonzero((pos_test_df.spk == spk))\n",
    "    pos_embeds = pos_embeddings[pos_test_idx]\n",
    "    pos_scores[spk] = F.cosine_similarity(pos_embeds,\n",
    "                                          spk_models[spk])\n",
    "    neg_scores[spk] = F.cosine_similarity(imposter_embeddings, spk_models[spk])\n",
    "\n",
    "    print(spk)\n",
    "    compute_eer(pos_scores[spk], neg_scores[spk])\n",
    "    \n",
    "print(\"\\nuni\")\n",
    "uni_pos_scores = np.concatenate([v for v in pos_scores.values()])\n",
    "uni_neg_scores = np.concatenate([v for v in neg_scores.values()])\n",
    "compute_eer(uni_pos_scores, uni_neg_scores)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
