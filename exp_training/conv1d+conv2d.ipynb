{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1d + Conv2d\n",
    "----\n",
    "\n",
    "trello: https://trello.com/c/CDnYK17O\n",
    "\n",
    "\n",
    "front-end를 conv1d로 두고\n",
    "\n",
    "뒤 layer를 conv2d로 두었을 떄 효과에 대해 ... 알아보기 위하여..\n",
    "\n",
    "----\n",
    "\n",
    "일단 성능이 좋게 나오지않는다.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "# datasets\n",
    "# voxc1_fbank_xvector\n",
    "# gcommand_fbank_xvector\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\",\n",
    "                              input_frames=500, splice_frames=[300, 500], stride_frames=1, input_format='fbank',\n",
    "                              cuda=True,\n",
    "                              lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False,\n",
    "                              batch_size=128,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                              n_epochs=50\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.data_utils import find_dataset, find_trial\n",
    "\n",
    "_, datasets = find_dataset(config, basedir='../')\n",
    "trial = find_trial(config, basedir='../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_loaders\n",
    "\n",
    "dataloaders = init_loaders(config, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sv_system.model.model_utils import find_model\n",
    "# model = find_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.model.tdnnModel import gTDNN\n",
    "from sv_system.model.ResNet34 import BasicBlock\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class tdnn_conv(nn.Module):\n",
    "    \"\"\"xvector architecture\"\"\"\n",
    "    def __init__(self, config, n_labels=31):\n",
    "        super(tdnn_conv, self).__init__()\n",
    "        inDim = config['input_dim']\n",
    "        self.tdnn = nn.Sequential(\n",
    "            nn.Conv1d(inDim, 256, stride=1, dilation=1, kernel_size=5),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        layers = [3,4,6,3]\n",
    "        self.inplanes = inplanes = 4\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, inplanes, kernel_size=3, stride=1, padding=3,\n",
    "                                   bias=False),\n",
    "            nn.BatchNorm2d(inplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            self._make_layer(BasicBlock, inplanes, layers[0]),\n",
    "            self._make_layer(BasicBlock, 2*inplanes, layers[1], stride=2),\n",
    "            self._make_layer(BasicBlock, 4*inplanes, layers[2], stride=2),\n",
    "            self._make_layer(BasicBlock, 8*inplanes, layers[3], stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(8*inplanes, n_labels)\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            # classifier does not contain Conv2d or BN2d\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = x.squeeze()\n",
    "        # (batch, time, freq) -> (batch, freq, time)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.tdnn(x)\n",
    "        x = x.unsqueeze(1)\n",
    "#         print(x.shape)\n",
    "        x = self.extractor(x)\n",
    "        x = F.avg_pool2d(x,x.shape[-2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tdnn_conv(config, config['n_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.train.train_utils import set_seed, find_optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion, optimizer = find_optimizer(config, model)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_eer']:\n",
    "    train_loader, val_loader, test_loader, sv_loader = dataloaders\n",
    "else:\n",
    "    train_loader, val_loader, test_loader = dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += torch.sum(torch.eq(torch.argmax(logit, dim=1), y)).item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "                  .format(batch_idx+1, len(train_loader), loss_sum /(batch_idx+1),)\n",
    "                 )\n",
    "        acc = n_corrects / total\n",
    "\n",
    "    acc = n_corrects / total\n",
    "    return loss_sum, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 6.991380\n",
      "Batch 200/1042\t Loss 6.906220\n",
      "Batch 300/1042\t Loss 6.825552\n",
      "Batch 400/1042\t Loss 6.724923\n",
      "Batch 500/1042\t Loss 6.615981\n",
      "Batch 600/1042\t Loss 6.513283\n",
      "Batch 700/1042\t Loss 6.414030\n",
      "Batch 800/1042\t Loss 6.322789\n",
      "Batch 900/1042\t Loss 6.237069\n",
      "Batch 1000/1042\t Loss 6.155338\n",
      "epoch #0, val accuracy: 0.009139151312410831\n",
      "epoch #0, sv eer: 0.20950910446171866\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 5.257168\n",
      "Batch 200/1042\t Loss 5.223507\n",
      "Batch 300/1042\t Loss 5.187330\n",
      "Batch 400/1042\t Loss 5.149717\n",
      "Batch 500/1042\t Loss 5.108807\n",
      "Batch 600/1042\t Loss 5.071156\n",
      "Batch 700/1042\t Loss 5.035321\n",
      "Batch 800/1042\t Loss 5.001936\n",
      "Batch 900/1042\t Loss 4.966598\n",
      "Batch 1000/1042\t Loss 4.929929\n",
      "epoch #1, val accuracy: 0.021816037595272064\n",
      "epoch #1, sv eer: 0.18187626450857203\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 4.422289\n",
      "Batch 200/1042\t Loss 4.382648\n",
      "Batch 300/1042\t Loss 4.338158\n",
      "Batch 400/1042\t Loss 4.292605\n",
      "Batch 500/1042\t Loss 4.258589\n",
      "Batch 600/1042\t Loss 4.217248\n",
      "Batch 700/1042\t Loss 4.175918\n",
      "Batch 800/1042\t Loss 4.137290\n",
      "Batch 900/1042\t Loss 4.103159\n",
      "Batch 1000/1042\t Loss 4.067317\n",
      "epoch #2, val accuracy: 0.061657682061195374\n",
      "epoch #2, sv eer: 0.17330422745181556\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 3.567351\n",
      "Batch 200/1042\t Loss 3.541177\n",
      "Batch 300/1042\t Loss 3.524635\n",
      "Batch 400/1042\t Loss 3.494429\n",
      "Batch 500/1042\t Loss 3.460980\n",
      "Batch 600/1042\t Loss 3.431878\n",
      "Batch 700/1042\t Loss 3.405339\n",
      "Batch 800/1042\t Loss 3.380878\n",
      "Batch 900/1042\t Loss 3.356454\n",
      "Batch 1000/1042\t Loss 3.333512\n",
      "epoch #3, val accuracy: 0.07856721431016922\n",
      "epoch #3, sv eer: 0.15823660951975296\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 2.980987\n",
      "Batch 200/1042\t Loss 2.973713\n",
      "Batch 300/1042\t Loss 2.959112\n",
      "Batch 400/1042\t Loss 2.930149\n",
      "Batch 500/1042\t Loss 2.913940\n",
      "Batch 600/1042\t Loss 2.901110\n",
      "Batch 700/1042\t Loss 2.890625\n",
      "Batch 800/1042\t Loss 2.873506\n",
      "Batch 900/1042\t Loss 2.859043\n",
      "Batch 1000/1042\t Loss 2.845408\n",
      "epoch #4, val accuracy: 0.05603520944714546\n",
      "epoch #4, sv eer: 0.17218613566180385\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 2.622752\n",
      "Batch 200/1042\t Loss 2.613827\n",
      "Batch 300/1042\t Loss 2.602972\n",
      "Batch 400/1042\t Loss 2.590801\n",
      "Batch 500/1042\t Loss 2.585881\n",
      "Batch 600/1042\t Loss 2.580662\n",
      "Batch 700/1042\t Loss 2.568232\n",
      "Batch 800/1042\t Loss 2.557124\n",
      "Batch 900/1042\t Loss 2.554473\n",
      "Batch 1000/1042\t Loss 2.543279\n",
      "epoch #5, val accuracy: 0.10672169923782349\n",
      "epoch #5, sv eer: 0.15509530401448196\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 2.360203\n",
      "Batch 200/1042\t Loss 2.358275\n",
      "Batch 300/1042\t Loss 2.357357\n",
      "Batch 400/1042\t Loss 2.355690\n",
      "Batch 500/1042\t Loss 2.354747\n",
      "Batch 600/1042\t Loss 2.353899\n",
      "Batch 700/1042\t Loss 2.346413\n",
      "Batch 800/1042\t Loss 2.341319\n",
      "Batch 900/1042\t Loss 2.332348\n",
      "Batch 1000/1042\t Loss 2.328241\n",
      "epoch #6, val accuracy: 0.10274174809455872\n",
      "epoch #6, sv eer: 0.1603663081673943\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 2.166288\n",
      "Batch 200/1042\t Loss 2.175162\n",
      "Batch 300/1042\t Loss 2.177855\n",
      "Batch 400/1042\t Loss 2.168523\n",
      "Batch 500/1042\t Loss 2.164697\n",
      "Batch 600/1042\t Loss 2.166319\n",
      "Batch 700/1042\t Loss 2.167166\n",
      "Batch 800/1042\t Loss 2.160854\n",
      "Batch 900/1042\t Loss 2.157473\n",
      "Batch 1000/1042\t Loss 2.149996\n",
      "epoch #7, val accuracy: 0.1414041370153427\n",
      "epoch #7, sv eer: 0.15227345330635714\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 2.016747\n",
      "Batch 200/1042\t Loss 2.017262\n",
      "Batch 300/1042\t Loss 2.014276\n",
      "Batch 400/1042\t Loss 2.019937\n",
      "Batch 500/1042\t Loss 2.017706\n",
      "Batch 600/1042\t Loss 2.016711\n",
      "Batch 700/1042\t Loss 2.016999\n",
      "Batch 800/1042\t Loss 2.016431\n",
      "Batch 900/1042\t Loss 2.016200\n",
      "Batch 1000/1042\t Loss 2.016470\n",
      "epoch #8, val accuracy: 0.18299360573291779\n",
      "epoch #8, sv eer: 0.14790757107869237\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.901866\n",
      "Batch 200/1042\t Loss 1.907079\n",
      "Batch 300/1042\t Loss 1.916297\n",
      "Batch 400/1042\t Loss 1.921682\n",
      "Batch 500/1042\t Loss 1.925667\n",
      "Batch 600/1042\t Loss 1.924413\n",
      "Batch 700/1042\t Loss 1.930328\n",
      "Batch 800/1042\t Loss 1.930207\n",
      "Batch 900/1042\t Loss 1.930852\n",
      "Batch 1000/1042\t Loss 1.928739\n",
      "epoch #9, val accuracy: 0.12706367671489716\n",
      "epoch #9, sv eer: 0.14684272175487167\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.798131\n",
      "Batch 200/1042\t Loss 1.826910\n",
      "Batch 300/1042\t Loss 1.823066\n",
      "Batch 400/1042\t Loss 1.833327\n",
      "Batch 500/1042\t Loss 1.830913\n",
      "Batch 600/1042\t Loss 1.834346\n",
      "Batch 700/1042\t Loss 1.832606\n",
      "Batch 800/1042\t Loss 1.837774\n",
      "Batch 900/1042\t Loss 1.836640\n",
      "Batch 1000/1042\t Loss 1.839622\n",
      "epoch #10, val accuracy: 0.17012718319892883\n",
      "epoch #10, sv eer: 0.152060483441593\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.743067\n",
      "Batch 200/1042\t Loss 1.733620\n",
      "Batch 300/1042\t Loss 1.725680\n",
      "Batch 400/1042\t Loss 1.739240\n",
      "Batch 500/1042\t Loss 1.746574\n",
      "Batch 600/1042\t Loss 1.754022\n",
      "Batch 700/1042\t Loss 1.755362\n",
      "Batch 800/1042\t Loss 1.755993\n",
      "Batch 900/1042\t Loss 1.757933\n",
      "Batch 1000/1042\t Loss 1.758958\n",
      "epoch #11, val accuracy: 0.17090633511543274\n",
      "epoch #11, sv eer: 0.15967415610691088\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.656604\n",
      "Batch 200/1042\t Loss 1.657263\n",
      "Batch 300/1042\t Loss 1.683883\n",
      "Batch 400/1042\t Loss 1.693654\n",
      "Batch 500/1042\t Loss 1.693602\n",
      "Batch 600/1042\t Loss 1.697883\n",
      "Batch 700/1042\t Loss 1.706080\n",
      "Batch 800/1042\t Loss 1.704614\n",
      "Batch 900/1042\t Loss 1.708570\n",
      "Batch 1000/1042\t Loss 1.710123\n",
      "epoch #12, val accuracy: 0.17090633511543274\n",
      "epoch #12, sv eer: 0.15067617932062613\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.602423\n",
      "Batch 200/1042\t Loss 1.599375\n",
      "Batch 300/1042\t Loss 1.613538\n",
      "Batch 400/1042\t Loss 1.623085\n",
      "Batch 500/1042\t Loss 1.630745\n",
      "Batch 600/1042\t Loss 1.630863\n",
      "Batch 700/1042\t Loss 1.638391\n",
      "Batch 800/1042\t Loss 1.641351\n",
      "Batch 900/1042\t Loss 1.646347\n",
      "Batch 1000/1042\t Loss 1.649395\n",
      "epoch #13, val accuracy: 0.20447270572185516\n",
      "epoch #13, sv eer: 0.14189117239910554\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.559146\n",
      "Batch 200/1042\t Loss 1.552870\n",
      "Batch 300/1042\t Loss 1.567841\n",
      "Batch 400/1042\t Loss 1.584161\n",
      "Batch 500/1042\t Loss 1.592672\n",
      "Batch 600/1042\t Loss 1.597183\n",
      "Batch 700/1042\t Loss 1.596338\n",
      "Batch 800/1042\t Loss 1.601716\n",
      "Batch 900/1042\t Loss 1.606555\n",
      "Batch 1000/1042\t Loss 1.609811\n",
      "epoch #14, val accuracy: 0.11089117079973221\n",
      "epoch #14, sv eer: 0.16276221914599084\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.512123\n",
      "Batch 200/1042\t Loss 1.531167\n",
      "Batch 300/1042\t Loss 1.543505\n",
      "Batch 400/1042\t Loss 1.549288\n",
      "Batch 500/1042\t Loss 1.563092\n",
      "Batch 600/1042\t Loss 1.563144\n",
      "Batch 700/1042\t Loss 1.561676\n",
      "Batch 800/1042\t Loss 1.566639\n",
      "Batch 900/1042\t Loss 1.569489\n",
      "Batch 1000/1042\t Loss 1.570958\n",
      "epoch #15, val accuracy: 0.2410924881696701\n",
      "epoch #15, sv eer: 0.14710893408582684\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.479212\n",
      "Batch 200/1042\t Loss 1.472784\n",
      "Batch 300/1042\t Loss 1.494853\n",
      "Batch 400/1042\t Loss 1.493092\n",
      "Batch 500/1042\t Loss 1.501812\n",
      "Batch 600/1042\t Loss 1.509973\n",
      "Batch 700/1042\t Loss 1.518991\n",
      "Batch 800/1042\t Loss 1.523780\n",
      "Batch 900/1042\t Loss 1.527996\n",
      "Batch 1000/1042\t Loss 1.531611\n",
      "epoch #16, val accuracy: 0.1849309206008911\n",
      "epoch #16, sv eer: 0.14487275050580342\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.451018\n",
      "Batch 200/1042\t Loss 1.475530\n",
      "Batch 300/1042\t Loss 1.477287\n",
      "Batch 400/1042\t Loss 1.489633\n",
      "Batch 500/1042\t Loss 1.494874\n",
      "Batch 600/1042\t Loss 1.500289\n",
      "Batch 700/1042\t Loss 1.503144\n",
      "Batch 800/1042\t Loss 1.507062\n",
      "Batch 900/1042\t Loss 1.508323\n",
      "Batch 1000/1042\t Loss 1.511318\n",
      "epoch #17, val accuracy: 0.20188257098197937\n",
      "epoch #17, sv eer: 0.1531253327654137\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.416720\n",
      "Batch 200/1042\t Loss 1.434831\n",
      "Batch 300/1042\t Loss 1.450364\n",
      "Batch 400/1042\t Loss 1.458979\n",
      "Batch 500/1042\t Loss 1.462457\n",
      "Batch 600/1042\t Loss 1.462785\n",
      "Batch 700/1042\t Loss 1.471565\n",
      "Batch 800/1042\t Loss 1.474799\n",
      "Batch 900/1042\t Loss 1.478409\n",
      "Batch 1000/1042\t Loss 1.481104\n",
      "epoch #18, val accuracy: 0.15189100801944733\n",
      "epoch #18, sv eer: 0.1636140986050474\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.409738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/1042\t Loss 1.411238\n",
      "Batch 300/1042\t Loss 1.414634\n",
      "Batch 400/1042\t Loss 1.420889\n",
      "Batch 500/1042\t Loss 1.430446\n",
      "Batch 600/1042\t Loss 1.435078\n",
      "Batch 700/1042\t Loss 1.439196\n",
      "Batch 800/1042\t Loss 1.444803\n",
      "Batch 900/1042\t Loss 1.455286\n",
      "Batch 1000/1042\t Loss 1.458860\n",
      "epoch #19, val accuracy: 0.21104277670383453\n",
      "epoch #19, sv eer: 0.1481205409434565\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.376482\n",
      "Batch 200/1042\t Loss 1.374528\n",
      "Batch 300/1042\t Loss 1.377743\n",
      "Batch 400/1042\t Loss 1.394299\n",
      "Batch 500/1042\t Loss 1.397961\n",
      "Batch 600/1042\t Loss 1.409074\n",
      "Batch 700/1042\t Loss 1.416968\n",
      "Batch 800/1042\t Loss 1.421832\n",
      "Batch 900/1042\t Loss 1.425110\n",
      "Batch 1000/1042\t Loss 1.431569\n",
      "epoch #20, val accuracy: 0.22706788778305054\n",
      "epoch #20, sv eer: 0.15323181769779576\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.326941\n",
      "Batch 200/1042\t Loss 1.356696\n",
      "Batch 300/1042\t Loss 1.376526\n",
      "Batch 400/1042\t Loss 1.383783\n",
      "Batch 500/1042\t Loss 1.387747\n",
      "Batch 600/1042\t Loss 1.397212\n",
      "Batch 700/1042\t Loss 1.405782\n",
      "Batch 800/1042\t Loss 1.410883\n",
      "Batch 900/1042\t Loss 1.412017\n",
      "Batch 1000/1042\t Loss 1.414055\n",
      "epoch #21, val accuracy: 0.13386540114879608\n",
      "epoch #21, sv eer: 0.1631349164093281\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.309605\n",
      "Batch 200/1042\t Loss 1.325757\n",
      "Batch 300/1042\t Loss 1.354629\n",
      "Batch 400/1042\t Loss 1.363442\n",
      "Batch 500/1042\t Loss 1.372222\n",
      "Batch 600/1042\t Loss 1.379590\n",
      "Batch 700/1042\t Loss 1.383422\n",
      "Batch 800/1042\t Loss 1.386610\n",
      "Batch 900/1042\t Loss 1.393937\n",
      "Batch 1000/1042\t Loss 1.396736\n",
      "epoch #22, val accuracy: 0.2109585553407669\n",
      "epoch #22, sv eer: 0.1476413587477372\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.303368\n",
      "Batch 200/1042\t Loss 1.323156\n",
      "Batch 300/1042\t Loss 1.329805\n",
      "Batch 400/1042\t Loss 1.343240\n",
      "Batch 500/1042\t Loss 1.348056\n",
      "Batch 600/1042\t Loss 1.359099\n",
      "Batch 700/1042\t Loss 1.363414\n",
      "Batch 800/1042\t Loss 1.367057\n",
      "Batch 900/1042\t Loss 1.371964\n",
      "Batch 1000/1042\t Loss 1.374511\n",
      "epoch #23, val accuracy: 0.20247219502925873\n",
      "epoch #23, sv eer: 0.15536151634543713\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.270757\n",
      "Batch 200/1042\t Loss 1.280435\n",
      "Batch 300/1042\t Loss 1.296955\n",
      "Batch 400/1042\t Loss 1.315664\n",
      "Batch 500/1042\t Loss 1.321850\n",
      "Batch 600/1042\t Loss 1.331875\n",
      "Batch 700/1042\t Loss 1.339208\n",
      "Batch 800/1042\t Loss 1.348637\n",
      "Batch 900/1042\t Loss 1.358334\n",
      "Batch 1000/1042\t Loss 1.363150\n",
      "epoch #24, val accuracy: 0.1700008511543274\n",
      "epoch #24, sv eer: 0.16760728356937493\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.240020\n",
      "Batch 200/1042\t Loss 1.266610\n",
      "Batch 300/1042\t Loss 1.283453\n",
      "Batch 400/1042\t Loss 1.301723\n",
      "Batch 500/1042\t Loss 1.314078\n",
      "Batch 600/1042\t Loss 1.321176\n",
      "Batch 700/1042\t Loss 1.326758\n",
      "Batch 800/1042\t Loss 1.332270\n",
      "Batch 900/1042\t Loss 1.338544\n",
      "Batch 1000/1042\t Loss 1.343585\n",
      "epoch #25, val accuracy: 0.30485597252845764\n",
      "epoch #25, sv eer: 0.14258332445958896\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.247982\n",
      "Batch 200/1042\t Loss 1.265805\n",
      "Batch 300/1042\t Loss 1.290699\n",
      "Batch 400/1042\t Loss 1.293269\n",
      "Batch 500/1042\t Loss 1.302172\n",
      "Batch 600/1042\t Loss 1.311093\n",
      "Batch 700/1042\t Loss 1.315909\n",
      "Batch 800/1042\t Loss 1.319284\n",
      "Batch 900/1042\t Loss 1.323562\n",
      "Batch 1000/1042\t Loss 1.325097\n",
      "epoch #26, val accuracy: 0.2195080816745758\n",
      "epoch #26, sv eer: 0.14332871898626345\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.268927\n",
      "Batch 200/1042\t Loss 1.265121\n",
      "Batch 300/1042\t Loss 1.278689\n",
      "Batch 400/1042\t Loss 1.295476\n",
      "Batch 500/1042\t Loss 1.293123\n",
      "Batch 600/1042\t Loss 1.303655\n",
      "Batch 700/1042\t Loss 1.308797\n",
      "Batch 800/1042\t Loss 1.313590\n",
      "Batch 900/1042\t Loss 1.314053\n",
      "Batch 1000/1042\t Loss 1.319838\n",
      "epoch #27, val accuracy: 0.2648248076438904\n",
      "epoch #27, sv eer: 0.15099563411777234\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.220393\n",
      "Batch 200/1042\t Loss 1.250684\n",
      "Batch 300/1042\t Loss 1.257035\n",
      "Batch 400/1042\t Loss 1.263702\n",
      "Batch 500/1042\t Loss 1.270719\n",
      "Batch 600/1042\t Loss 1.284862\n",
      "Batch 700/1042\t Loss 1.292315\n",
      "Batch 800/1042\t Loss 1.297610\n",
      "Batch 900/1042\t Loss 1.303519\n",
      "Batch 1000/1042\t Loss 1.306707\n",
      "epoch #28, val accuracy: 0.18036134541034698\n",
      "epoch #28, sv eer: 0.1570652752635502\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.233781\n",
      "Batch 200/1042\t Loss 1.251841\n",
      "Batch 300/1042\t Loss 1.259281\n",
      "Batch 400/1042\t Loss 1.263077\n",
      "Batch 500/1042\t Loss 1.271233\n",
      "Batch 600/1042\t Loss 1.280080\n",
      "Batch 700/1042\t Loss 1.288358\n",
      "Batch 800/1042\t Loss 1.292511\n",
      "Batch 900/1042\t Loss 1.298740\n",
      "Batch 1000/1042\t Loss 1.302873\n",
      "epoch #29, val accuracy: 0.27882832288742065\n",
      "epoch #29, sv eer: 0.15136833138110958\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.214141\n",
      "Batch 200/1042\t Loss 1.240642\n",
      "Batch 300/1042\t Loss 1.247335\n",
      "Batch 400/1042\t Loss 1.255764\n",
      "Batch 500/1042\t Loss 1.263137\n",
      "Batch 600/1042\t Loss 1.269564\n",
      "Batch 700/1042\t Loss 1.272711\n",
      "Batch 800/1042\t Loss 1.281825\n",
      "Batch 900/1042\t Loss 1.285273\n",
      "Batch 1000/1042\t Loss 1.289799\n",
      "epoch #30, val accuracy: 0.2433035671710968\n",
      "epoch #30, sv eer: 0.15695879033116814\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.199726\n",
      "Batch 200/1042\t Loss 1.220340\n",
      "Batch 300/1042\t Loss 1.230103\n",
      "Batch 400/1042\t Loss 1.239046\n",
      "Batch 500/1042\t Loss 1.250726\n",
      "Batch 600/1042\t Loss 1.254305\n",
      "Batch 700/1042\t Loss 1.262242\n",
      "Batch 800/1042\t Loss 1.272435\n",
      "Batch 900/1042\t Loss 1.275116\n",
      "Batch 1000/1042\t Loss 1.280440\n",
      "epoch #31, val accuracy: 0.22205610573291779\n",
      "epoch #31, sv eer: 0.14380790118198275\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.205635\n",
      "Batch 200/1042\t Loss 1.206426\n",
      "Batch 300/1042\t Loss 1.219239\n",
      "Batch 400/1042\t Loss 1.232773\n",
      "Batch 500/1042\t Loss 1.244773\n",
      "Batch 600/1042\t Loss 1.257158\n",
      "Batch 700/1042\t Loss 1.269890\n",
      "Batch 800/1042\t Loss 1.268173\n",
      "Batch 900/1042\t Loss 1.271885\n",
      "Batch 1000/1042\t Loss 1.278808\n",
      "epoch #32, val accuracy: 0.22456200420856476\n",
      "epoch #32, sv eer: 0.15110211905015442\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.172002\n",
      "Batch 200/1042\t Loss 1.202303\n",
      "Batch 300/1042\t Loss 1.204772\n",
      "Batch 400/1042\t Loss 1.224030\n",
      "Batch 500/1042\t Loss 1.230831\n",
      "Batch 600/1042\t Loss 1.240235\n",
      "Batch 700/1042\t Loss 1.245883\n",
      "Batch 800/1042\t Loss 1.255010\n",
      "Batch 900/1042\t Loss 1.258514\n",
      "Batch 1000/1042\t Loss 1.264073\n",
      "epoch #33, val accuracy: 0.20632579922676086\n",
      "epoch #33, sv eer: 0.15541475881162817\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.175694\n",
      "Batch 200/1042\t Loss 1.192600\n",
      "Batch 300/1042\t Loss 1.207767\n",
      "Batch 400/1042\t Loss 1.208571\n",
      "Batch 500/1042\t Loss 1.213737\n",
      "Batch 600/1042\t Loss 1.225287\n",
      "Batch 700/1042\t Loss 1.233976\n",
      "Batch 800/1042\t Loss 1.241481\n",
      "Batch 900/1042\t Loss 1.246690\n",
      "Batch 1000/1042\t Loss 1.253709\n",
      "epoch #34, val accuracy: 0.2366071343421936\n",
      "epoch #34, sv eer: 0.14881269300393996\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.196388\n",
      "Batch 200/1042\t Loss 1.196092\n",
      "Batch 300/1042\t Loss 1.205279\n",
      "Batch 400/1042\t Loss 1.214443\n",
      "Batch 500/1042\t Loss 1.219853\n",
      "Batch 600/1042\t Loss 1.229084\n",
      "Batch 700/1042\t Loss 1.237530\n",
      "Batch 800/1042\t Loss 1.242376\n",
      "Batch 900/1042\t Loss 1.248189\n",
      "Batch 1000/1042\t Loss 1.252855\n",
      "epoch #35, val accuracy: 0.1666315793991089\n",
      "epoch #35, sv eer: 0.1594079437759557\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.199186\n",
      "Batch 200/1042\t Loss 1.190696\n",
      "Batch 300/1042\t Loss 1.203061\n",
      "Batch 400/1042\t Loss 1.212002\n",
      "Batch 500/1042\t Loss 1.219139\n",
      "Batch 600/1042\t Loss 1.226004\n",
      "Batch 700/1042\t Loss 1.230136\n",
      "Batch 800/1042\t Loss 1.239054\n",
      "Batch 900/1042\t Loss 1.243968\n",
      "Batch 1000/1042\t Loss 1.248845\n",
      "epoch #36, val accuracy: 0.2585916519165039\n",
      "epoch #36, sv eer: 0.15040996698967096\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.174976\n",
      "Batch 200/1042\t Loss 1.175873\n",
      "Batch 300/1042\t Loss 1.179249\n",
      "Batch 400/1042\t Loss 1.183419\n",
      "Batch 500/1042\t Loss 1.196501\n",
      "Batch 600/1042\t Loss 1.201793\n",
      "Batch 700/1042\t Loss 1.207234\n",
      "Batch 800/1042\t Loss 1.218178\n",
      "Batch 900/1042\t Loss 1.227413\n",
      "Batch 1000/1042\t Loss 1.232138\n",
      "epoch #37, val accuracy: 0.23418547213077545\n",
      "epoch #37, sv eer: 0.14961132999680546\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.146862\n",
      "Batch 200/1042\t Loss 1.168666\n",
      "Batch 300/1042\t Loss 1.178776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400/1042\t Loss 1.180897\n",
      "Batch 500/1042\t Loss 1.194386\n",
      "Batch 600/1042\t Loss 1.201495\n",
      "Batch 700/1042\t Loss 1.208444\n",
      "Batch 800/1042\t Loss 1.216177\n",
      "Batch 900/1042\t Loss 1.222077\n",
      "Batch 1000/1042\t Loss 1.226234\n",
      "epoch #38, val accuracy: 0.14260445535182953\n",
      "epoch #38, sv eer: 0.15903524651261847\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.167061\n",
      "Batch 200/1042\t Loss 1.163484\n",
      "Batch 300/1042\t Loss 1.168688\n",
      "Batch 400/1042\t Loss 1.181935\n",
      "Batch 500/1042\t Loss 1.193815\n",
      "Batch 600/1042\t Loss 1.199134\n",
      "Batch 700/1042\t Loss 1.206444\n",
      "Batch 800/1042\t Loss 1.209265\n",
      "Batch 900/1042\t Loss 1.212929\n",
      "Batch 1000/1042\t Loss 1.218889\n",
      "epoch #39, val accuracy: 0.18280407786369324\n",
      "epoch #39, sv eer: 0.14391438611436483\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.150017\n",
      "Batch 200/1042\t Loss 1.149382\n",
      "Batch 300/1042\t Loss 1.169692\n",
      "Batch 400/1042\t Loss 1.187703\n",
      "Batch 500/1042\t Loss 1.191653\n",
      "Batch 600/1042\t Loss 1.199549\n",
      "Batch 700/1042\t Loss 1.203072\n",
      "Batch 800/1042\t Loss 1.212141\n",
      "Batch 900/1042\t Loss 1.214598\n",
      "Batch 1000/1042\t Loss 1.220091\n",
      "epoch #40, val accuracy: 0.2016930729150772\n",
      "epoch #40, sv eer: 0.15647960813544884\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.122481\n",
      "Batch 200/1042\t Loss 1.145066\n",
      "Batch 300/1042\t Loss 1.163179\n",
      "Batch 400/1042\t Loss 1.165615\n",
      "Batch 500/1042\t Loss 1.171201\n",
      "Batch 600/1042\t Loss 1.177313\n",
      "Batch 700/1042\t Loss 1.184326\n",
      "Batch 800/1042\t Loss 1.192957\n",
      "Batch 900/1042\t Loss 1.202525\n",
      "Batch 1000/1042\t Loss 1.208366\n",
      "epoch #41, val accuracy: 0.1420990526676178\n",
      "epoch #41, sv eer: 0.16164412735597913\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.122539\n",
      "Batch 200/1042\t Loss 1.122967\n",
      "Batch 300/1042\t Loss 1.138244\n",
      "Batch 400/1042\t Loss 1.150193\n",
      "Batch 500/1042\t Loss 1.159845\n",
      "Batch 600/1042\t Loss 1.175091\n",
      "Batch 700/1042\t Loss 1.183548\n",
      "Batch 800/1042\t Loss 1.185166\n",
      "Batch 900/1042\t Loss 1.191695\n",
      "Batch 1000/1042\t Loss 1.197335\n",
      "epoch #42, val accuracy: 0.22561489045619965\n",
      "epoch #42, sv eer: 0.15211372590778405\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.124905\n",
      "Batch 200/1042\t Loss 1.132408\n",
      "Batch 300/1042\t Loss 1.143247\n",
      "Batch 400/1042\t Loss 1.156976\n",
      "Batch 500/1042\t Loss 1.176271\n",
      "Batch 600/1042\t Loss 1.184943\n",
      "Batch 700/1042\t Loss 1.189395\n",
      "Batch 800/1042\t Loss 1.193457\n",
      "Batch 900/1042\t Loss 1.200138\n",
      "Batch 1000/1042\t Loss 1.199017\n",
      "epoch #43, val accuracy: 0.16159872710704803\n",
      "epoch #43, sv eer: 0.1482802683420296\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.129317\n",
      "Batch 200/1042\t Loss 1.121339\n",
      "Batch 300/1042\t Loss 1.131451\n",
      "Batch 400/1042\t Loss 1.147537\n",
      "Batch 500/1042\t Loss 1.161376\n",
      "Batch 600/1042\t Loss 1.169392\n",
      "Batch 700/1042\t Loss 1.173436\n",
      "Batch 800/1042\t Loss 1.181782\n",
      "Batch 900/1042\t Loss 1.184733\n",
      "Batch 1000/1042\t Loss 1.186869\n",
      "epoch #44, val accuracy: 0.1990397721529007\n",
      "epoch #44, sv eer: 0.158343094452135\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.110039\n",
      "Batch 200/1042\t Loss 1.111591\n",
      "Batch 300/1042\t Loss 1.123087\n",
      "Batch 400/1042\t Loss 1.137249\n",
      "Batch 500/1042\t Loss 1.150237\n",
      "Batch 600/1042\t Loss 1.163761\n",
      "Batch 700/1042\t Loss 1.170132\n",
      "Batch 800/1042\t Loss 1.173155\n",
      "Batch 900/1042\t Loss 1.177136\n",
      "Batch 1000/1042\t Loss 1.184293\n",
      "epoch #45, val accuracy: 0.23224814236164093\n",
      "epoch #45, sv eer: 0.1524864231711213\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.109149\n",
      "Batch 200/1042\t Loss 1.124874\n",
      "Batch 300/1042\t Loss 1.130386\n",
      "Batch 400/1042\t Loss 1.134863\n",
      "Batch 500/1042\t Loss 1.150140\n",
      "Batch 600/1042\t Loss 1.157277\n",
      "Batch 700/1042\t Loss 1.163103\n",
      "Batch 800/1042\t Loss 1.167526\n",
      "Batch 900/1042\t Loss 1.170566\n",
      "Batch 1000/1042\t Loss 1.174086\n",
      "epoch #46, val accuracy: 0.23532260954380035\n",
      "epoch #46, sv eer: 0.14870620807155788\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.107729\n",
      "Batch 200/1042\t Loss 1.108584\n",
      "Batch 300/1042\t Loss 1.128543\n",
      "Batch 400/1042\t Loss 1.133981\n",
      "Batch 500/1042\t Loss 1.135126\n",
      "Batch 600/1042\t Loss 1.144606\n",
      "Batch 700/1042\t Loss 1.147728\n",
      "Batch 800/1042\t Loss 1.158112\n",
      "Batch 900/1042\t Loss 1.160972\n",
      "Batch 1000/1042\t Loss 1.168288\n",
      "epoch #47, val accuracy: 0.310625821352005\n",
      "epoch #47, sv eer: 0.1485997231391758\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.072553\n",
      "Batch 200/1042\t Loss 1.082586\n",
      "Batch 300/1042\t Loss 1.104348\n",
      "Batch 400/1042\t Loss 1.121420\n",
      "Batch 500/1042\t Loss 1.131378\n",
      "Batch 600/1042\t Loss 1.141659\n",
      "Batch 700/1042\t Loss 1.147676\n",
      "Batch 800/1042\t Loss 1.153914\n",
      "Batch 900/1042\t Loss 1.164632\n",
      "Batch 1000/1042\t Loss 1.170558\n",
      "epoch #48, val accuracy: 0.1856258511543274\n",
      "epoch #48, sv eer: 0.1446597806410393\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.089342\n",
      "Batch 200/1042\t Loss 1.102507\n",
      "Batch 300/1042\t Loss 1.121098\n",
      "Batch 400/1042\t Loss 1.133837\n",
      "Batch 500/1042\t Loss 1.138360\n",
      "Batch 600/1042\t Loss 1.140743\n",
      "Batch 700/1042\t Loss 1.148720\n",
      "Batch 800/1042\t Loss 1.158202\n",
      "Batch 900/1042\t Loss 1.167264\n",
      "Batch 1000/1042\t Loss 1.170847\n",
      "epoch #49, val accuracy: 0.09259181469678879\n",
      "epoch #49, sv eer: 0.17735065488233415\n"
     ]
    }
   ],
   "source": [
    "from sv_system.train.si_train import val, sv_test\n",
    "\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "\n",
    "#     train code\n",
    "    train_loss, train_acc = train(model)\n",
    "\n",
    "#     validation code\n",
    "    val_loss, val_acc = val(config, val_loader, model, criterion)\n",
    "    print(\"epoch #{}, val accuracy: {}\".format(epoch_idx, val_acc))\n",
    "\n",
    "#     evaluate best_metric\n",
    "    if not config['no_eer']:\n",
    "        # eer validation code\n",
    "        eer, label, score = sv_test(config, sv_loader, model, trial)\n",
    "        print(\"epoch #{}, sv eer: {}\".format(epoch_idx, eer))\n",
    "    \n",
    "    scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
