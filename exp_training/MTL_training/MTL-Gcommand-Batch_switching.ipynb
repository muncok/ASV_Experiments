{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MTL이 잘 안되서\n",
    "Epoch 단위로 task를 바꾸는게 아니라 batch단위로..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "sys.path.append('../sv_system/')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "args = easydict.EasyDict(dict(dataset=\"gcommand_equal30_wav\",\n",
    "                              input_frames=100, splice_frames=[20, 100], stride_frames=1, input_format='fbank',\n",
    "                              cuda=True,\n",
    "                              lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False,\n",
    "                              batch_size=128,\n",
    "                              arch=\"ResNet34\", loss=\"softmax\",\n",
    "                              n_epochs=50,\n",
    "                              lamb=0.3\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_si_df = pd.read_pickle(\"../dataset/dataframes/gcommand/equal_num_30spk/equal_num_30spk_si.pkl\")\n",
    "gc_sv_df = pd.read_pickle(\"../dataset/dataframes/gcommand/equal_num_30spk/equal_num_30spk_sv.pkl\")\n",
    "\n",
    "unique_spks = gc_si_df.spk.unique().tolist()\n",
    "unique_sents = gc_si_df.sent.unique().tolist()\n",
    "\n",
    "gc_si_df['sent_label'] = gc_si_df.sent.apply(lambda x: unique_sents.index(x))\n",
    "gc_sv_df['sent_label'] = gc_sv_df.sent.apply(lambda x: unique_sents.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.data_utils import find_dataset, find_trial\n",
    "\n",
    "_, datasets = find_dataset(config, basedir='../')\n",
    "trial = find_trial(config, basedir='../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataset import mtlSpeechDataset\n",
    "\n",
    "joint_train_dataset = mtlSpeechDataset.read_df(config, gc_si_df, \"train\")\n",
    "datasets[2] = joint_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_loaders\n",
    "\n",
    "dataloaders = init_loaders(config, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.model.ResNet34 import ResNet34\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet34_v1(ResNet34):\n",
    "    \"\"\"\n",
    "        additional fc layer before output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, config, inplanes=16, n_labels1=1000, n_labels2=1000, fc_dims=None):\n",
    "        super().__init__(config, inplanes, 10)\n",
    "\n",
    "        extractor_output_dim = 8*inplanes\n",
    "        if not fc_dims:\n",
    "            fc_dims = extractor_output_dim\n",
    "\n",
    "        fc = [nn.Linear(extractor_output_dim,fc_dims),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        self.fc = nn.Sequential(*fc)\n",
    "        \n",
    "        self.classifier_1 = nn.Linear(fc_dims, n_labels1) # for spks\n",
    "        self.classifier_2 = nn.Linear(fc_dims, n_labels2) # for sents\n",
    "    \n",
    "    def extract(self, x):\n",
    "        x = self.extractor(x)\n",
    "        x = F.avg_pool2d(x,x.shape[-2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        feat = self.fc(x)\n",
    "        \n",
    "        return feat\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.spk_out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def joint_forward(self, x):\n",
    "        feat = self.extract(x)\n",
    "        \n",
    "        out1 = self.classifier_1(feat)\n",
    "        out2 = self.classifier_2(feat)\n",
    "        \n",
    "        return out1, out2\n",
    "        \n",
    "    def spk_out(self, x):\n",
    "        feat = self.extract(x)\n",
    "        out1 = self.classifier_1(feat)\n",
    "        \n",
    "        return out1\n",
    "    \n",
    "    def sent_out(self, x):\n",
    "        feat = self.extract(x)\n",
    "        out2 = self.classifier_2(feat)\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.model.tdnnModel import gTDNN, st_pool_layer\n",
    "\n",
    "class tdnn_xvector(gTDNN):\n",
    "    \"\"\"xvector architecture\"\"\"\n",
    "    def __init__(self, config, n_labels_spk, n_labels_sent):\n",
    "        super(tdnn_xvector, self).__init__(config, n_labels_spk)\n",
    "        inDim = config['input_dim']\n",
    "        \n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Conv1d(inDim, 512, stride=1, dilation=1, kernel_size=5),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=3, kernel_size=3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=4, kernel_size=3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.spk_residual = nn.Sequential(\n",
    "            nn.Conv1d(512, 1500, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(True),\n",
    "            st_pool_layer(),\n",
    "            nn.Linear(3000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.spk_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, n_labels_spk),\n",
    "        )\n",
    "        \n",
    "        self.sent_residual = nn.Sequential(\n",
    "            nn.Conv1d(512, 1500, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(True),\n",
    "            st_pool_layer(),\n",
    "            nn.Linear(3000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.sent_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, n_labels_sent)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.extractor(x)\n",
    "        x = self.spk_residual(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def sent_out(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.extractor(x)\n",
    "        x = self.sent_residual(x)\n",
    "        out_sent = self.sent_classifier(x)\n",
    "        \n",
    "        return out_sent\n",
    "    \n",
    "    def spk_out(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.extractor(x)\n",
    "        x = self.spk_residual(x)\n",
    "        out_spk = self.spk_classifier(x)\n",
    "        \n",
    "        return out_spk\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spk_out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def joint_forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.extractor(x)\n",
    "        \n",
    "        spk_x = self.spk_residual(x)\n",
    "        out_spk = self.spk_classifier(spk_x)\n",
    "        \n",
    "        sent_x = self.sent_residual(x)\n",
    "        out_sent = self.sent_classifier(sent_x)      \n",
    "\n",
    "        return out_spk, out_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tdnn_xvector(config,  len(unique_spks), len(unique_sents))\n",
    "# model = ResNet34_v1(config, n_labels1=len(unique_spks), n_labels2=len(unique_sents), fc_dims=256)\n",
    "\n",
    "if not config[\"no_cuda\"]:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sv_system.train.train_utils import set_seed, find_optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion, joint_optimizer = find_optimizer(config, model)\n",
    "\n",
    "optimizer_spk = torch.optim.SGD([{'params':model.extractor.parameters()}, \n",
    "                                 {'params':model.spk_residual.parameters()},\n",
    "                                 {'params':model.spk_classifier.parameters()}],\n",
    "                                lr=0.1,\n",
    "                                momentum=config['momentum'],\n",
    "                                weight_decay=config['weight_decay'],\n",
    "                                nesterov=config['use_nesterov'])\n",
    "\n",
    "\n",
    "optimizer_sent = torch.optim.SGD([{'params':model.extractor.parameters()}, \n",
    "                                  {'params':model.sent_residual.parameters()},\n",
    "                                  {'params':model.sent_classifier.parameters()}], \n",
    "                                lr=0.1,\n",
    "                                momentum=config['momentum'],\n",
    "                                weight_decay=config['weight_decay'],\n",
    "                                nesterov=config['use_nesterov'])\n",
    "\n",
    "# sent_optimizer = torch.optim.SGD([{'params':model.extractor.parameters()}, \n",
    "#                                   {'params':model.classifier_2.parameters()}], \n",
    "#                                 lr=0.01,\n",
    "#                                 momentum=config['momentum'],\n",
    "#                                 weight_decay=config['weight_decay'],\n",
    "#                                 nesterov=config['use_nesterov'])\n",
    "\n",
    "# spk_optimizer = torch.optim.SGD([{'params':model.extractor.parameters()}, \n",
    "#                                  {'params':model.classifier_1.parameters()}],\n",
    "#                                 lr=0.1,\n",
    "#                                 momentum=config['momentum'],\n",
    "#                                 weight_decay=config['weight_decay'],\n",
    "#                                 nesterov=config['use_nesterov'])\n",
    "\n",
    "\n",
    "\n",
    "scheduler_spk = ReduceLROnPlateau(optimizer_spk, 'min', factor=0.1, patience=5)\n",
    "scheduler_sent = ReduceLROnPlateau(optimizer_sent, 'min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_eer']:\n",
    "    train_loader, val_loader, joint_loader, sv_loader = dataloaders\n",
    "else:\n",
    "    train_loader, val_loader, joint_loader = dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtl trai\n",
    "from tqdm import tqdm_notebook\n",
    "from sv_system.train.train_utils import print_eval\n",
    "\n",
    "def batch_switch_train(config, train_loader, model, optimizer_spk, \n",
    "                       optimizer_sent, criterion):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    print_steps = (np.arange(1,11)*0.1 \\\n",
    "                    * len(train_loader)).astype(np.int64)\n",
    "\n",
    "    splice_frames = config['splice_frames']\n",
    "    if len(splice_frames) > 1:\n",
    "        splice_frames_ = np.random.randint(splice_frames[0], splice_frames[1])\n",
    "    else:\n",
    "        splice_frames_ = splice_frames[-1]\n",
    "\n",
    "    for batch_idx, (X, y_spk, y_sent) in enumerate(train_loader):\n",
    "        # X.shape is (batch, channel, time, bank)\n",
    "        X = X.narrow(2, 0, splice_frames_)\n",
    "        if not config[\"no_cuda\"]:\n",
    "            X = X.cuda()\n",
    "            y_spk = y_spk.cuda()\n",
    "            y_sent = y_sent.cuda()\n",
    "            \n",
    "        \n",
    "        logit_spk, logit_sent = model.joint_forward(X)\n",
    "        loss_spk = criterion(logit_spk, y_spk)\n",
    "        loss_sent = criterion(logit_sent, y_sent)\n",
    "        \n",
    "        if batch_idx % 2 == 0:\n",
    "            loss = loss_spk\n",
    "            optimizer = optimizer_spk\n",
    "        else:\n",
    "            loss = loss_sent\n",
    "            optimizer = optimizer_sent\n",
    "        optimizer.zero_grad()           \n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.argmax(logit_spk, dim=1)\n",
    "        corrects += predicted.eq(y_spk).cpu().sum().float()\n",
    "        total += y_spk.size(0)\n",
    "        if batch_idx in print_steps:\n",
    "            print(\"train loss: {:.4f}, acc: {:.5f} \" \\\n",
    "                  .format(loss_sum/total, corrects/total))\n",
    "            \n",
    "    return loss_sum, corrects/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "train loss: 0.0531, acc: 0.02973 \n",
      "train loss: 0.0502, acc: 0.03279 \n",
      "train loss: 0.0469, acc: 0.03501 \n",
      "train loss: 0.0454, acc: 0.03629 \n",
      "train loss: 0.0444, acc: 0.03760 \n",
      "train loss: 0.0439, acc: 0.03866 \n",
      "train loss: 0.0433, acc: 0.04097 \n",
      "train loss: 0.0429, acc: 0.04315 \n",
      "train loss: 0.0440, acc: 0.04022 \n",
      "epoch #0, train accuracy: 0.0362376943230629\n",
      "epoch #0, sv eer: 0.3446\n",
      "------------------------------\n",
      "train loss: 0.0435, acc: 0.00000 \n",
      "train loss: 0.0441, acc: 0.00154 \n",
      "train loss: 0.0444, acc: 0.00310 \n",
      "train loss: 0.0447, acc: 0.00393 \n",
      "train loss: 0.0448, acc: 0.00533 \n",
      "train loss: 0.0450, acc: 0.00641 \n",
      "train loss: 0.0449, acc: 0.00781 \n",
      "train loss: 0.0450, acc: 0.00962 \n",
      "train loss: 0.0448, acc: 0.01026 \n",
      "epoch #1, train accuracy: 0.009243173524737358\n",
      "epoch #1, sv eer: 0.2964333333333333\n",
      "------------------------------\n",
      "train loss: 0.0414, acc: 0.00000 \n",
      "train loss: 0.0426, acc: 0.00165 \n",
      "train loss: 0.0431, acc: 0.00310 \n",
      "train loss: 0.0434, acc: 0.00560 \n",
      "train loss: 0.0434, acc: 0.00906 \n",
      "train loss: 0.0435, acc: 0.01240 \n",
      "train loss: 0.0433, acc: 0.01537 \n",
      "train loss: 0.0433, acc: 0.01854 \n",
      "train loss: 0.0433, acc: 0.01807 \n",
      "epoch #2, train accuracy: 0.01628134958446026\n",
      "epoch #2, sv eer: 0.2763888888888889\n",
      "------------------------------\n",
      "train loss: 0.0416, acc: 0.00000 \n",
      "train loss: 0.0426, acc: 0.00264 \n",
      "train loss: 0.0428, acc: 0.00597 \n",
      "train loss: 0.0430, acc: 0.01014 \n",
      "train loss: 0.0429, acc: 0.01518 \n",
      "train loss: 0.0428, acc: 0.01955 \n",
      "train loss: 0.0426, acc: 0.02464 \n",
      "train loss: 0.0424, acc: 0.02947 \n",
      "train loss: 0.0425, acc: 0.02841 \n",
      "epoch #3, train accuracy: 0.02559133991599083\n",
      "epoch #3, sv eer: 0.2662777777777778\n",
      "------------------------------\n",
      "train loss: 0.0418, acc: 0.00130 \n",
      "train loss: 0.0429, acc: 0.00176 \n",
      "train loss: 0.0432, acc: 0.00199 \n",
      "train loss: 0.0437, acc: 0.00360 \n",
      "train loss: 0.0439, acc: 0.00510 \n",
      "train loss: 0.0442, acc: 0.00600 \n",
      "train loss: 0.0442, acc: 0.00778 \n",
      "train loss: 0.0443, acc: 0.00937 \n",
      "train loss: 0.0443, acc: 0.00868 \n",
      "epoch #4, train accuracy: 0.007906811311841011\n",
      "epoch #4, sv eer: 0.30127777777777776\n",
      "------------------------------\n",
      "train loss: 0.0421, acc: 0.00022 \n",
      "train loss: 0.0434, acc: 0.00110 \n",
      "train loss: 0.0437, acc: 0.00147 \n",
      "train loss: 0.0441, acc: 0.00227 \n",
      "train loss: 0.0442, acc: 0.00377 \n",
      "train loss: 0.0444, acc: 0.00548 \n",
      "train loss: 0.0443, acc: 0.00743 \n",
      "train loss: 0.0443, acc: 0.00976 \n",
      "train loss: 0.0442, acc: 0.00944 \n",
      "epoch #5, train accuracy: 0.008530446328222752\n",
      "epoch #5, sv eer: 0.26997777777777776\n",
      "------------------------------\n",
      "train loss: 0.0424, acc: 0.00087 \n",
      "train loss: 0.0430, acc: 0.00407 \n",
      "train loss: 0.0433, acc: 0.00494 \n",
      "train loss: 0.0435, acc: 0.00759 \n",
      "train loss: 0.0435, acc: 0.01043 \n",
      "train loss: 0.0436, acc: 0.01318 \n",
      "train loss: 0.0434, acc: 0.01715 \n",
      "train loss: 0.0434, acc: 0.02071 \n",
      "train loss: 0.0433, acc: 0.01963 \n",
      "epoch #6, train accuracy: 0.01768452860414982\n",
      "epoch #6, sv eer: 0.24867777777777778\n",
      "------------------------------\n",
      "train loss: 0.0422, acc: 0.00087 \n",
      "train loss: 0.0432, acc: 0.00209 \n",
      "train loss: 0.0434, acc: 0.00251 \n",
      "train loss: 0.0437, acc: 0.00443 \n",
      "train loss: 0.0437, acc: 0.00772 \n",
      "train loss: 0.0438, acc: 0.01059 \n",
      "train loss: 0.0436, acc: 0.01356 \n",
      "train loss: 0.0435, acc: 0.01663 \n",
      "train loss: 0.0433, acc: 0.01609 \n",
      "epoch #7, train accuracy: 0.014544078148901463\n",
      "epoch #7, sv eer: 0.26645555555555556\n",
      "------------------------------\n",
      "train loss: 0.0417, acc: 0.00065 \n",
      "train loss: 0.0428, acc: 0.00220 \n",
      "train loss: 0.0429, acc: 0.00457 \n",
      "train loss: 0.0431, acc: 0.00831 \n",
      "train loss: 0.0431, acc: 0.01234 \n",
      "train loss: 0.0432, acc: 0.01592 \n",
      "train loss: 0.0429, acc: 0.01956 \n",
      "train loss: 0.0427, acc: 0.02260 \n",
      "train loss: 0.0426, acc: 0.02161 \n",
      "epoch #8, train accuracy: 0.019510891288518906\n",
      "epoch #8, sv eer: 0.25322222222222224\n",
      "------------------------------\n",
      "train loss: 0.0424, acc: 0.00260 \n",
      "train loss: 0.0427, acc: 0.00275 \n",
      "train loss: 0.0434, acc: 0.00265 \n",
      "train loss: 0.0441, acc: 0.00266 \n",
      "train loss: 0.0445, acc: 0.00266 \n",
      "train loss: 0.0450, acc: 0.00263 \n",
      "train loss: 0.0452, acc: 0.00267 \n",
      "train loss: 0.0456, acc: 0.00292 \n",
      "train loss: 0.0455, acc: 0.00272 \n",
      "epoch #9, train accuracy: 0.002449997700750828\n",
      "epoch #9, sv eer: 0.0\n",
      "------------------------------\n",
      "train loss: 0.0414, acc: 0.00000 \n",
      "train loss: 0.0430, acc: 0.00000 \n",
      "train loss: 0.0438, acc: 0.00000 \n",
      "train loss: 0.0445, acc: 0.00000 \n",
      "train loss: 0.0448, acc: 0.00084 \n",
      "train loss: 0.0452, acc: 0.00163 \n",
      "train loss: 0.0454, acc: 0.00213 \n",
      "train loss: 0.0458, acc: 0.00253 \n",
      "train loss: 0.0457, acc: 0.00237 \n",
      "epoch #10, train accuracy: 0.002138179959729314\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3420209e3f81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no_eer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# eer validation code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0meer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msv_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch #{}, sv eer: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/host/projects/sv_experiments/sv_system/train/si_train.py\u001b[0m in \u001b[0;36msv_test\u001b[0;34m(config, sv_loader, model, trial)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeds_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         sim_matrix = F.cosine_similarity(\n\u001b[0;32m---> 78\u001b[0;31m                 embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mcord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menrolment_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mscore_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcord\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(x1, x2, dim, eps)\u001b[0m\n\u001b[1;32m   1985\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m     \"\"\"\n\u001b[0;32m-> 1987\u001b[0;31m     \u001b[0mw12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1988\u001b[0m     \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1989\u001b[0m     \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sv_system.train.si_train import train, val, sv_test\n",
    "\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "#     train code\n",
    "    if epoch_idx < 0:\n",
    "        train_loss, train_acc = train(config, train_loader, model, optimizer_spk, criterion)\n",
    "    else:\n",
    "        train_loss, train_acc = batch_switch_train(config, joint_loader, model, optimizer_spk,\n",
    "                                                   optimizer_sent, criterion)\n",
    "\n",
    "#     validation code\n",
    "#     val_loss, val_acc = val(config, val_loader, model, criterion)\n",
    "    \n",
    "    print(\"epoch #{}, train accuracy: {}\".format(epoch_idx, train_acc))\n",
    "#     print(\"epoch #{}, val accuracy: {}\".format(epoch_idx, val_acc))\n",
    "\n",
    "#     evaluate best_metric\n",
    "    if not config['no_eer']:\n",
    "        # eer validation code\n",
    "        eer, label, score = sv_test(config, sv_loader, model, trial)\n",
    "        print(\"epoch #{}, sv eer: {}\".format(epoch_idx, eer))\n",
    "    \n",
    "#     scheduler_spk.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().state_dict(), open(\"gcommand_ResNet34_v1_mtl_lamb0.3.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV_Test\n",
    "\n",
    "equal_sent and diff_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.sv_score.score_utils import embeds_utterance\n",
    "\n",
    "def sv_test(config, sv_loader, model, trial):\n",
    "    embeddings, _ = embeds_utterance(config, sv_loader, model, lda=None)\n",
    "    sim_matrix = F.cosine_similarity(\n",
    "            embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)\n",
    "    cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]\n",
    "    score_vector = sim_matrix[cord].numpy()\n",
    "    label_vector = np.array(trial.label)\n",
    "    fpr, tpr, thres = roc_curve(\n",
    "            label_vector, score_vector, pos_label=1)\n",
    "    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "\n",
    "    return eer, label_vector, score_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_sent_trial = trial[trial.equal_command]\n",
    "diff_sent_trial = trial[~trial.equal_command]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "equal_sent_eer, _, _ = sv_test(config, sv_loader, model, equal_sent_trial)\n",
    "diff_sent_eer, _, _ = sv_test(config, sv_loader, model, diff_sent_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcommand_ResNet34_v1_mtl_lamb0.1.pt\n",
    "print(f\"equal: {equal_sent_eer}\\ndiff: {diff_sent_eer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['lamb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
