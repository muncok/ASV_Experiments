{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 sent label로 같이 MTL을 하면\n",
    "\n",
    "같은 sent일 때는 좀더 성능이 좋아질지?\n",
    "\n",
    "혹은 positive도 강화되지만 negative도 강화될지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "sys.path.append('../sv_system/')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "args = easydict.EasyDict(dict(dataset=\"gcommand_equal30_wav\",\n",
    "                              input_frames=100, splice_frames=[20, 100], stride_frames=1, input_format='fbank',\n",
    "                              cuda=True,\n",
    "                              lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False,\n",
    "                              batch_size=128,\n",
    "                              arch=\"ResNet34\", loss=\"softmax\",\n",
    "                              n_epochs=50,\n",
    "                              lamb=0.3\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_si_df = pd.read_pickle(\"../dataset/dataframes/gcommand/equal_num_30spk/equal_num_30spk_si.pkl\")\n",
    "gc_sv_df = pd.read_pickle(\"../dataset/dataframes/gcommand/equal_num_30spk/equal_num_30spk_sv.pkl\")\n",
    "\n",
    "unique_spks = gc_si_df.spk.unique().tolist()\n",
    "unique_sents = gc_si_df.sent.unique().tolist()\n",
    "\n",
    "gc_si_df['sent_label'] = gc_si_df.sent.apply(lambda x: unique_sents.index(x))\n",
    "gc_sv_df['sent_label'] = gc_sv_df.sent.apply(lambda x: unique_sents.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.data_utils import find_dataset, find_trial\n",
    "\n",
    "\n",
    "_, datasets = find_dataset(config, basedir='../')\n",
    "trial = find_trial(config, basedir='../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataset import mtlSpeechDataset\n",
    "\n",
    "train_dataset = mtlSpeechDataset.read_df(config, gc_si_df, \"train\")\n",
    "datasets[0] = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_loaders\n",
    "\n",
    "dataloaders = init_loaders(config, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.model.ResNet34 import ResNet34\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet34_v1(ResNet34):\n",
    "    \"\"\"\n",
    "        additional fc layer before output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, config, inplanes=16, n_labels1=1000, n_labels2=1000, fc_dims=None):\n",
    "        super().__init__(config, inplanes, 10)\n",
    "\n",
    "        extractor_output_dim = 8*inplanes\n",
    "        if not fc_dims:\n",
    "            fc_dims = extractor_output_dim\n",
    "\n",
    "        fc = [nn.Linear(extractor_output_dim,fc_dims),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        self.fc = nn.Sequential(*fc)\n",
    "        \n",
    "        self.classifier_1 = nn.Linear(fc_dims, n_labels1) # for spks\n",
    "        self.classifier_2 = nn.Linear(fc_dims, n_labels2) # for sents\n",
    "    \n",
    "    def extract(self, x):\n",
    "        x = self.extractor(x)\n",
    "        feat = self.fc(x)\n",
    "        \n",
    "        return feat\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.extract(x)\n",
    "        \n",
    "        x = F.avg_pool2d(feat,feat.shape[-2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out1 = self.classifier_1(x)\n",
    "        \n",
    "        x = feat.view(feat.size(0), -1)\n",
    "        out2 = self.classifier_2(x)\n",
    "        \n",
    "        return out1, out2\n",
    "        \n",
    "    def spk_out(self, x):\n",
    "        feat = self.extract(x)\n",
    "        \n",
    "        x = F.avg_pool2d(feat,feat.shape[-2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out1 = self.classifier_1(x)\n",
    "        \n",
    "        return out1\n",
    "    \n",
    "    def sent_out(self, x):\n",
    "        feat = self.extract(x)\n",
    "        \n",
    "        x = feat.view(feat.size(0), -1)\n",
    "        out2 = self.classifier_2(x)\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.model.tdnnModel import gTDNN, st_pool_layer\n",
    "\n",
    "class tdnn_xvector(gTDNN):\n",
    "    \"\"\"xvector architecture\"\"\"\n",
    "    def __init__(self, config, n_labels_spk, n_labels_sent):\n",
    "        super(tdnn_xvector, self).__init__(config, n_labels_spk)\n",
    "        inDim = config['input_dim']\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Conv1d(inDim, 512, stride=1, dilation=1, kernel_size=5),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=3, kernel_size=3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=4, kernel_size=3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.spk_seg = nn.Sequential(\n",
    "            nn.Conv1d(512, 1500, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(True),\n",
    "            st_pool_layer(),\n",
    "            nn.Linear(3000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            ####################\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, n_labels_spk),\n",
    "        )\n",
    "        \n",
    "        self.sent_seg = nn.Sequential(\n",
    "            nn.Conv1d(512, 1500, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(True),\n",
    "            st_pool_layer(),\n",
    "            nn.Linear(3000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            ##################3333333##\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, n_labels_sent),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        # (batch, time, freq) -> (batch, freq, time)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.extractor(x)\n",
    "        x = self.spk_seg(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def sent_out(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0,2,1)\n",
    "        feat = self.extractor(x)\n",
    "        out_sent = self.sent_seg(feat)\n",
    "        \n",
    "        return out_sent\n",
    "    \n",
    "    def spk_out(self, x):\n",
    "        feat = self.embed(x)\n",
    "        out_spk = self.classifier(feat)\n",
    "\n",
    "        return out_spk\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spk_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tdnn_xvector(config,  len(unique_spks), len(unique_sents))\n",
    "# model = ResNet34_v1(config, n_labels1=len(unique_spks), n_labels2=len(unique_sents), fc_dims=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config[\"no_cuda\"]:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sv_system.train.train_utils import set_seed, find_optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion, joint_optimizer = find_optimizer(config, model)\n",
    "\n",
    "sent_optimizer = torch.optim.SGD([{'params':model.extractor.parameters()}, \n",
    "                                  {'params':model.sent_seg.parameters()}], \n",
    "                                lr=0.01,\n",
    "                                momentum=config['momentum'],\n",
    "                                weight_decay=config['weight_decay'],\n",
    "                                nesterov=config['use_nesterov'])\n",
    "\n",
    "spk_optimizer = torch.optim.SGD([{'params':model.extractor.parameters()}, \n",
    "                                 {'params':model.spk_seg.parameters()},\n",
    "                                 {'params':model.classifier.parameters()}],\n",
    "                                lr=0.1,\n",
    "                                momentum=config['momentum'],\n",
    "                                weight_decay=config['weight_decay'],\n",
    "                                nesterov=config['use_nesterov'])\n",
    "\n",
    "\n",
    "scheduler = ReduceLROnPlateau(spk_optimizer, 'min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_eer']:\n",
    "    train_loader, val_loader, test_loader, sv_loader = dataloaders\n",
    "else:\n",
    "    train_loader, val_loader, test_loader = dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtl trai\n",
    "from tqdm import tqdm_notebook\n",
    "from sv_system.train.train_utils import print_eval\n",
    "\n",
    "def sent_train(config, train_loader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    loss_sent_sum = 0\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    lamb = config['lamb']\n",
    "    print_steps = (np.array([0.25, 0.5, 0.75, 1.0]) \\\n",
    "                    * len(train_loader)).astype(np.int64)\n",
    "\n",
    "    splice_frames = config['splice_frames']\n",
    "    if len(splice_frames) > 1:\n",
    "        splice_frames_ = np.random.randint(splice_frames[0], splice_frames[1])\n",
    "    else:\n",
    "        splice_frames_ = splice_frames[-1]\n",
    "\n",
    "    for batch_idx, (X, y_spk, y_sent) in enumerate(train_loader):\n",
    "        # X.shape is (batch, channel, time, bank)\n",
    "        X = X.narrow(2, 0, splice_frames_)\n",
    "        if not config[\"no_cuda\"]:\n",
    "            X = X.cuda()\n",
    "            y_sent = y_sent.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logit = model.sent_out(X)\n",
    "        loss_sent = criterion(logit, y_sent)\n",
    "        loss_sent_sum += loss_sent.item()\n",
    "        loss_sent.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.argmax(logit, dim=1)\n",
    "        corrects += predicted.eq(y_sent).cpu().sum().float()\n",
    "        total += y_sent.size(0)\n",
    "        if batch_idx in print_steps:\n",
    "            print(\" sent_loss: {:.4f}, acc: {:.5f} \" \\\n",
    "                  .format(loss_sent_sum/total, corrects/total))\n",
    "        \n",
    "    return loss_sent_sum, corrects/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spk_train(config, train_loader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    loss_spk_sum = 0\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    lamb = config['lamb']\n",
    "    print_steps = (np.array([0.25, 0.5, 0.75, 1.0]) \\\n",
    "                    * len(train_loader)).astype(np.int64)\n",
    "\n",
    "    splice_frames = config['splice_frames']\n",
    "    if len(splice_frames) > 1:\n",
    "        splice_frames_ = np.random.randint(splice_frames[0], splice_frames[1])\n",
    "    else:\n",
    "        splice_frames_ = splice_frames[-1]\n",
    "\n",
    "    for batch_idx, (X, y_spk, y_sent) in enumerate(train_loader):\n",
    "        # X.shape is (batch, channel, time, bank)\n",
    "        X = X.narrow(2, 0, splice_frames_)\n",
    "        if not config[\"no_cuda\"]:\n",
    "            X = X.cuda()\n",
    "            y_spk = y_spk.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logit = model.spk_out(X)\n",
    "        loss_spk = criterion(logit, y_spk)\n",
    "        loss_spk_sum += loss_spk.item()\n",
    "        loss_spk.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.argmax(logit, dim=1)\n",
    "        corrects += predicted.eq(y_spk).cpu().sum().float()\n",
    "        total += y_spk.size(0)\n",
    "        if batch_idx in print_steps:\n",
    "            print(\" spk_loss: {:.4f}, acc: {:.5f} \" \\\n",
    "                  .format(loss_spk_sum/total, corrects/total))\n",
    "            \n",
    "    return loss_spk_sum, corrects/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtl trai\n",
    "from tqdm import tqdm_notebook\n",
    "from sv_system.train.train_utils import print_eval\n",
    "\n",
    "def joint_train(config, train_loader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    loss_spk_sum = 0\n",
    "    loss_sent_sum = 0\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    lamb = config['lamb']\n",
    "    print_steps = (np.array([0.25, 0.5, 0.75, 1.0]) \\\n",
    "                    * len(train_loader)).astype(np.int64)\n",
    "\n",
    "    splice_frames = config['splice_frames']\n",
    "    if len(splice_frames) > 1:\n",
    "        splice_frames_ = np.random.randint(splice_frames[0], splice_frames[1])\n",
    "    else:\n",
    "        splice_frames_ = splice_frames[-1]\n",
    "\n",
    "    for batch_idx, (X, y_spk, y_sent) in enumerate(train_loader):\n",
    "        # X.shape is (batch, channel, time, bank)\n",
    "        X = X.narrow(2, 0, splice_frames_)\n",
    "        if not config[\"no_cuda\"]:\n",
    "            X = X.cuda()\n",
    "            y_spk = y_spk.cuda()\n",
    "            y_sent = y_sent.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logit1, logit2 = model.multi_out(X)\n",
    "        loss_spk = criterion(logit1, y_spk) * (1-lamb)\n",
    "        loss_sent = criterion(logit2, y_sent) * (lamb)\n",
    "        loss = loss_spk + loss_sent \n",
    "        loss_sum += loss.item()\n",
    "        loss_spk_sum += loss_spk.item()\n",
    "        loss_sent_sum += loss_sent.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.argmax(logit1, dim=1)\n",
    "        corrects += predicted.eq(y_spk).cpu().sum().float()\n",
    "        total += y_spk.size(0)\n",
    "        if batch_idx in print_steps:\n",
    "            print(\"train loss: {:.4f}, spk_loss: {:.4f}, sent_loss: {:.4f}, acc: {:.5f} \" \\\n",
    "                  .format(corrects/total, loss_sum/total,\n",
    "                          loss_spk_sum/total, loss_sent_sum/total))\n",
    "            \n",
    "    return loss_sum, corrects/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb: 0.3\n",
      "------------------------------\n",
      " sent_loss: 0.0177, acc: 0.33496 \n",
      " sent_loss: 0.0151, acc: 0.42685 \n",
      " sent_loss: 0.0138, acc: 0.47150 \n",
      "epoch #0, train accuracy: 0.4984186291694641\n",
      "epoch #0, val accuracy: 0.00022321428696159273\n",
      "epoch #0, sv eer: 0.3740777777777778\n",
      "------------------------------\n",
      " spk_loss: 0.0525, acc: 0.02885 \n",
      " spk_loss: 0.0479, acc: 0.05642 \n",
      " spk_loss: 0.0446, acc: 0.07700 \n",
      "epoch #1, train accuracy: 0.09969263523817062\n",
      "epoch #1, val accuracy: 0.1490459442138672\n",
      "epoch #1, sv eer: 0.20202222222222221\n",
      "------------------------------\n",
      " sent_loss: 0.0099, acc: 0.62580 \n",
      " sent_loss: 0.0082, acc: 0.68723 \n",
      " sent_loss: 0.0075, acc: 0.71301 \n",
      "epoch #2, train accuracy: 0.7301216125488281\n",
      "epoch #2, val accuracy: 0.07721054553985596\n",
      "epoch #2, sv eer: 0.26166666666666666\n",
      "------------------------------\n",
      " spk_loss: 0.0336, acc: 0.15936 \n",
      " spk_loss: 0.0323, acc: 0.17915 \n",
      " spk_loss: 0.0313, acc: 0.19484 \n",
      "epoch #3, train accuracy: 0.20798254013061523\n",
      "epoch #3, val accuracy: 0.14329636096954346\n",
      "epoch #3, sv eer: 0.1862\n",
      "------------------------------\n",
      " sent_loss: 0.0052, acc: 0.80105 \n",
      " sent_loss: 0.0042, acc: 0.83536 \n",
      " sent_loss: 0.0038, acc: 0.85446 \n",
      "epoch #4, train accuracy: 0.8658737540245056\n",
      "epoch #4, val accuracy: 0.16021744906902313\n",
      "epoch #4, sv eer: 0.19922222222222222\n",
      "------------------------------\n",
      " spk_loss: 0.0381, acc: 0.10502 \n",
      " spk_loss: 0.0366, acc: 0.12061 \n",
      " spk_loss: 0.0357, acc: 0.13400 \n",
      "epoch #5, train accuracy: 0.14261214435100555\n",
      "epoch #5, val accuracy: 0.10686924308538437\n",
      "epoch #5, sv eer: 0.17944444444444443\n",
      "------------------------------\n",
      " sent_loss: 0.0189, acc: 0.29173 \n",
      " sent_loss: 0.0175, acc: 0.33097 \n",
      " sent_loss: 0.0168, acc: 0.34920 \n",
      "epoch #6, train accuracy: 0.35948148369789124\n",
      "epoch #6, val accuracy: 0.09156105667352676\n",
      "epoch #6, sv eer: 0.1828\n",
      "------------------------------\n",
      " spk_loss: 0.0222, acc: 0.36310 \n",
      " spk_loss: 0.0206, acc: 0.39577 \n",
      " spk_loss: 0.0198, acc: 0.41022 \n",
      "epoch #7, train accuracy: 0.4230923354625702\n",
      "epoch #7, val accuracy: 0.3707805275917053\n",
      "epoch #7, sv eer: 0.1689\n",
      "------------------------------\n",
      " sent_loss: 0.0110, acc: 0.56516 \n",
      " sent_loss: 0.0103, acc: 0.58980 \n",
      " sent_loss: 0.0100, acc: 0.59866 \n",
      "epoch #8, train accuracy: 0.6075772047042847\n",
      "epoch #8, val accuracy: 0.36209315061569214\n",
      "epoch #8, sv eer: 0.17203333333333334\n",
      "------------------------------\n",
      " spk_loss: 0.0239, acc: 0.31561 \n",
      " spk_loss: 0.0233, acc: 0.32537 \n",
      " spk_loss: 0.0230, acc: 0.33472 \n",
      "epoch #9, train accuracy: 0.3429996967315674\n",
      "epoch #9, val accuracy: 0.291460245847702\n",
      "epoch #9, sv eer: 0.16642222222222222\n",
      "------------------------------\n",
      " sent_loss: 0.0258, acc: 0.05176 \n",
      " sent_loss: 0.0253, acc: 0.05278 \n",
      " sent_loss: 0.0250, acc: 0.05114 \n",
      "epoch #10, train accuracy: 0.05071495473384857\n",
      "epoch #10, val accuracy: 0.0023437500931322575\n",
      "epoch #10, sv eer: 0.0\n",
      "------------------------------\n",
      " spk_loss: 0.0584, acc: 0.00320 \n",
      " spk_loss: 0.0583, acc: 0.00275 \n",
      " spk_loss: 0.0583, acc: 0.00240 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1975:\n",
      "Process Process-1978:\n",
      "Process Process-1972:\n",
      "Process Process-1982:\n",
      "Process Process-1984:\n",
      "Process Process-1970:\n",
      "Process Process-1971:\n",
      "Traceback (most recent call last):\n",
      "Process Process-1969:\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1983:\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-1976:\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 263, in mel\n",
      "    mel_f = mel_frequencies(n_mels + 2, fmin=fmin, fmax=fmax, htk=htk)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 263, in mel\n",
      "    mel_f = mel_frequencies(n_mels + 2, fmin=fmin, fmax=fmax, htk=htk)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/time_frequency.py\", line 908, in mel_frequencies\n",
      "    max_mel = hz_to_mel(fmax, htk=htk)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/time_frequency.py\", line 908, in mel_frequencies\n",
      "    max_mel = hz_to_mel(fmax, htk=htk)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 270, in mel\n",
      "    lower = -ramps[i] / fdiff[i]\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/time_frequency.py\", line 639, in hz_to_mel\n",
      "    mels = min_log_mel + np.log(frequencies / min_log_hz) / logstep\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Process Process-1980:\n",
      "Process Process-1973:\n",
      "Process Process-1977:\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1974:\n",
      "Process Process-1979:\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1491, in melspectrogram\n",
      "    power=power)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/spectrum.py\", line 1531, in _spectrogram\n",
      "    S = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length))**power\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "Process Process-1981:\n",
      "Traceback (most recent call last):\n",
      "  File \"../sv_system/data/manage_audio.py\", line 27, in preprocess_audio\n",
      "    data.add_(-mean)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/spectrum.py\", line 184, in stft\n",
      "    axis=0)[:stft_matrix.shape[0]]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "Traceback (most recent call last):\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/scipy/fftpack/basic.py\", line 285, in fft\n",
      "    tmp = work_function(tmp,n,1,0,overwrite_x)\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 270, in mel\n",
      "    lower = -ramps[i] / fdiff[i]\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1491, in melspectrogram\n",
      "    power=power)\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/spectrum.py\", line 1531, in _spectrogram\n",
      "    S = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length))**power\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../sv_system/data/dataset.py\", line 368, in preprocess\n",
      "    data = librosa.core.load(example, sr=16000)[0]\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/spectrum.py\", line 183, in stft\n",
      "    y_frames[:, bl_s:bl_t],\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/audio.py\", line 112, in load\n",
      "    with audioread.audio_open(os.path.realpath(path)) as input_file:\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "  File \"../sv_system/data/dataset.py\", line 368, in preprocess\n",
      "    data = librosa.core.load(example, sr=16000)[0]\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 274, in mel\n",
      "    weights[i] = np.maximum(0, np.minimum(lower, upper))\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1491, in melspectrogram\n",
      "    power=power)\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/spectrum.py\", line 1531, in _spectrogram\n",
      "    S = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length))**power\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-23f3259be642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspk_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspk_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-4d5254aa2f4d>\u001b[0m in \u001b[0;36mspk_train\u001b[0;34m(config, train_loader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0my_spk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_spk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspk_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mloss_spk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_spk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss_spk_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_spk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-6bb2fde3ff26>\u001b[0m in \u001b[0;36mspk_out\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mspk_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mout_spk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-6bb2fde3ff26>\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# (batch, time, freq) -> (batch, freq, time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspk_seg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m         return F.batch_norm(\n\u001b[1;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1193\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m     )\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/audio.py\", line 126, in load\n",
      "    for frame in input_file:\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"../sv_system/data/dataset.py\", line 368, in preprocess\n",
      "    data = librosa.core.load(example, sr=16000)[0]\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/audioread/__init__.py\", line 80, in audio_open\n",
      "    return rawread.RawAudioFile(path)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/audio.py\", line 126, in load\n",
      "    for frame in input_file:\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 257, in mel\n",
      "    weights = np.zeros((n_mels, int(1 + n_fft // 2)))\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/audioread/rawread.py\", line 64, in __init__\n",
      "    self._file = aifc.open(self._fh)\n",
      "  File \"../sv_system/data/dataset.py\", line 400, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/audioread/rawread.py\", line 135, in read_data\n",
      "    data = audioop.lin2lin(data, old_width, TARGET_WIDTH)\n",
      "  File \"../sv_system/data/dataset.py\", line 382, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/aifc.py\", line 913, in open\n",
      "    return Aifc_read(f)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/aifc.py\", line 358, in __init__\n",
      "    self.initfp(f)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1491, in melspectrogram\n",
      "    power=power)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 263, in mel\n",
      "    mel_f = mel_frequencies(n_mels + 2, fmin=fmin, fmax=fmax, htk=htk)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sv_system.train.si_train import val, sv_test\n",
    "\n",
    "print(\"lamb: {}\".format(config['lamb']))\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "#     curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    \n",
    "#     #lr_scheduling\n",
    "#     idx = 0\n",
    "#     while(epoch_idx >= config['lr_schedule'][idx]):\n",
    "#     # use new lr from schedule epoch not a next epoch\n",
    "#         idx += 1\n",
    "#         if idx == len(config['lr_schedule']):\n",
    "#             break\n",
    "#     curr_lr = config['lrs'][idx]\n",
    "#     optimizer.state_dict()['param_groups'][0]['lr'] = curr_lr\n",
    "#     print(\"curr_lr: {}\".format(curr_lr))\n",
    "\n",
    "#     train code\n",
    "    if epoch_idx < 0:\n",
    "        train_loss, train_acc = sent_train(config, train_loader, model, sent_optimizer, criterion)\n",
    "    else:\n",
    "        if epoch_idx % 2 != 0:\n",
    "            train_loss, train_acc = spk_train(config, train_loader, model, spk_optimizer, criterion)\n",
    "        else:\n",
    "            train_loss, train_acc = sent_train(config, train_loader, model, sent_optimizer, criterion)\n",
    "            \n",
    "    \n",
    "\n",
    "#     validation code\n",
    "    val_loss, val_acc = val(config, val_loader, model, criterion)\n",
    "    \n",
    "    print(\"epoch #{}, train accuracy: {}\".format(epoch_idx, train_acc))\n",
    "    print(\"epoch #{}, val accuracy: {}\".format(epoch_idx, val_acc))\n",
    "\n",
    "#     evaluate best_metric\n",
    "    if not config['no_eer']:\n",
    "        # eer validation code\n",
    "        eer, label, score = sv_test(config, sv_loader, model, trial)\n",
    "        print(\"epoch #{}, sv eer: {}\".format(epoch_idx, eer))\n",
    "    \n",
    "    scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().state_dict(), open(\"gcommand_ResNet34_v1_mtl_lamb0.3.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV_Test\n",
    "\n",
    "equal_sent and diff_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.sv_score.score_utils import embeds_utterance\n",
    "\n",
    "def sv_test(config, sv_loader, model, trial):\n",
    "    embeddings, _ = embeds_utterance(config, sv_loader, model, lda=None)\n",
    "    sim_matrix = F.cosine_similarity(\n",
    "            embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)\n",
    "    cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]\n",
    "    score_vector = sim_matrix[cord].numpy()\n",
    "    label_vector = np.array(trial.label)\n",
    "    fpr, tpr, thres = roc_curve(\n",
    "            label_vector, score_vector, pos_label=1)\n",
    "    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "\n",
    "    return eer, label_vector, score_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_sent_trial = trial[trial.equal_command]\n",
    "diff_sent_trial = trial[~trial.equal_command]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "equal_sent_eer, _, _ = sv_test(config, sv_loader, model, equal_sent_trial)\n",
    "diff_sent_eer, _, _ = sv_test(config, sv_loader, model, diff_sent_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcommand_ResNet34_v1_mtl_lamb0.1.pt\n",
    "print(f\"equal: {equal_sent_eer}\\ndiff: {diff_sent_eer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['lamb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
