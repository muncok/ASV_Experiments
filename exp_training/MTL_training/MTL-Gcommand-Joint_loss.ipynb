{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint loss를 계산해서 한번에 end-to-end하는 방식\n",
    "그러나... 현재로서는 성능 변화가 별로없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "sys.path.append('../sv_system/')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "args = easydict.EasyDict(dict(dataset=\"gcommand_equal30_wav\",\n",
    "                              input_frames=100, splice_frames=[20, 100], stride_frames=1, input_format='fbank',\n",
    "                              cuda=True,\n",
    "                              lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=True,\n",
    "                              batch_size=128,\n",
    "                              arch=\"ResNet34\", loss=\"softmax\",\n",
    "                              n_epochs=50,\n",
    "                              lamb=0.3\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_si_df = pd.read_pickle(\"../dataset/dataframes/gcommand/equal_num_30spk/equal_num_30spk_si.pkl\")\n",
    "gc_sv_df = pd.read_pickle(\"../dataset/dataframes/gcommand/equal_num_30spk/equal_num_30spk_sv.pkl\")\n",
    "\n",
    "unique_spks = gc_si_df.spk.unique().tolist()\n",
    "unique_sents = gc_si_df.sent.unique().tolist()\n",
    "\n",
    "gc_si_df['sent_label'] = gc_si_df.sent.apply(lambda x: unique_sents.index(x))\n",
    "gc_sv_df['sent_label'] = gc_sv_df.sent.apply(lambda x: unique_sents.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch.utils.data as data\n",
    "from sv_system.data.manage_audio import preprocess_audio\n",
    "from collections  import OrderedDict\n",
    "\n",
    "class mtlSpeechDataset(data.Dataset):\n",
    "    def __init__(self, config, data, set_type):\n",
    "        super().__init__()\n",
    "        self.set_type = set_type\n",
    "        # data\n",
    "        self.audio_files = list(data.keys())\n",
    "        self.spk_labels = list(zip(*data.values()))[0]\n",
    "        self.sent_labels = list(zip(*data.values()))[1]\n",
    "\n",
    "        # input audio config\n",
    "        self.input_samples = config[\"input_samples\"]\n",
    "        self.input_format = config[\"input_format\"]\n",
    "        self.input_clip = config[\"input_clip\"]\n",
    "        \n",
    "        # input feature config\n",
    "        self.n_dct = config[\"n_dct_filters\"]\n",
    "        self.n_mels = config[\"n_mels\"]\n",
    "        self.filters = librosa.filters.dct(config[\"n_dct_filters\"], config[\"n_mels\"])\n",
    "        self.window_size = config[\"window_size\"]\n",
    "        self.window_stride =  config[\"window_stride\"]\n",
    "        self.data_folder = config[\"data_folder\"]\n",
    "\n",
    "        if set_type == \"train\":\n",
    "            self.random_clip = True\n",
    "        else:\n",
    "            self.random_clip = False\n",
    "\n",
    "    def preprocess(self, example):\n",
    "        in_len = self.input_samples\n",
    "        data = librosa.core.load(example, sr=16000)[0]\n",
    "        # input clipping\n",
    "        if self.input_clip:\n",
    "            if len(data) > in_len:\n",
    "                if self.random_clip:\n",
    "                    start_sample = np.random.randint(0, len(data) - in_len)\n",
    "                else:\n",
    "                    start_sample = 0\n",
    "                data = data[start_sample:start_sample+in_len]\n",
    "            else:\n",
    "                data = np.pad(data, (0, max(0, in_len - len(data))), \"constant\")\n",
    "\n",
    "        # audio to input feature\n",
    "        # mfcc hyper-parameters are hard coded.\n",
    "        input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
    "        return input_feature.unsqueeze(0)\n",
    "\n",
    "    @classmethod\n",
    "    def read_df(cls, config, df, set_type):\n",
    "        files = df.file.tolist()\n",
    "        if config['dataset'] == 'voxc12_mfcc':\n",
    "            files = [f.replace('-', '_') for f in files]\n",
    "            \n",
    "        spk_labels = df.label.tolist()\n",
    "        sent_labels = df.sent_label.tolist()\n",
    "        \n",
    "        samples = OrderedDict(zip(files, zip(spk_labels, sent_labels)))\n",
    "        dataset = cls(config, samples, set_type,)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])), \\\n",
    "                    self.spk_labels[index], self.sent_labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spk_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.data_utils import find_dataset, find_trial\n",
    "\n",
    "_, datasets = find_dataset(config, basedir='../')\n",
    "trial = find_trial(config, basedir='../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mtlSpeechDataset.read_df(config, gc_si_df, \"train\")\n",
    "datasets[0] = train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_loaders\n",
    "\n",
    "dataloaders = init_loaders(config, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.model.ResNet34 import ResNet34\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet34_v1(ResNet34):\n",
    "    \"\"\"\n",
    "        additional fc layer before output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, config, inplanes=16, n_labels1=1000, n_labels2=1000, fc_dims=None):\n",
    "        super().__init__(config, inplanes, 10)\n",
    "\n",
    "        extractor_output_dim = 8*inplanes\n",
    "        if not fc_dims:\n",
    "            fc_dims = extractor_output_dim\n",
    "\n",
    "        fc = [nn.Linear(extractor_output_dim,fc_dims),\n",
    "                  nn.ReLU(inplace=True)]\n",
    "\n",
    "        self.fc = nn.Sequential(*fc)\n",
    "        \n",
    "        self.classifier_1 = nn.Linear(fc_dims, n_labels1) # for spks\n",
    "        self.classifier_2 = nn.Linear(fc_dims, n_labels2) # for sents\n",
    "    \n",
    "    def embed(self, x):\n",
    "        x = self.extractor(x)\n",
    "        x = F.avg_pool2d(x,x.shape[-2:])\n",
    "        x = x.view(x.size(0), -1)\n",
    "        feat = self.fc(x)\n",
    "        \n",
    "        return feat\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.embed(x)\n",
    "        \n",
    "        out1 = self.classifier_1(feat)\n",
    "        out2 = self.classifier_2(feat)\n",
    "        \n",
    "        return out1, out2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.model.tdnnModel import gTDNN, st_pool_layer\n",
    "\n",
    "class tdnn_xvector(gTDNN):\n",
    "    \"\"\"xvector architecture\"\"\"\n",
    "    def __init__(self, config, n_labels_spk, n_labels_sent):\n",
    "        super(tdnn_xvector, self).__init__(config, n_labels_spk)\n",
    "        inDim = config['input_dim']\n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Conv1d(inDim, 512, stride=1, dilation=1, kernel_size=5),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=3, kernel_size=3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=4, kernel_size=3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(512, 512, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.spk_seg = nn.Sequential(\n",
    "            nn.Conv1d(512, 1500, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(True),\n",
    "            st_pool_layer(),\n",
    "            nn.Linear(3000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            ####################\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, n_labels_spk),\n",
    "        )\n",
    "        \n",
    "        self.sent_seg = nn.Sequential(\n",
    "            nn.Conv1d(512, 1500, stride=1, dilation=1, kernel_size=1),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(True),\n",
    "            st_pool_layer(),\n",
    "            nn.Linear(3000, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            ##################3333333##\n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, n_labels_sent),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        # (batch, time, freq) -> (batch, freq, time)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.extractor(x)\n",
    "        x = self.spk_seg(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def sent_out(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0,2,1)\n",
    "        feat = self.extractor(x)\n",
    "        out_sent = self.sent_seg(feat)\n",
    "        \n",
    "        return out_sent\n",
    "    \n",
    "    def spk_out(self, x):\n",
    "        feat = self.embed(x)\n",
    "        out_spk = self.classifier(feat)\n",
    "\n",
    "        return out_spk\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spk_out(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tdnn_xvector(config,  len(unique_spks), len(unique_sents))\n",
    "model = ResNet34_v1(config, n_labels1=len(unique_spks), n_labels2=len(unique_sents), fc_dims=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config[\"no_cuda\"]:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sv_system.train.train_utils import set_seed, find_optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion, joint_optimizer = find_optimizer(config, model)\n",
    "scheduler = ReduceLROnPlateau(joint_optimizer, 'min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_eer']:\n",
    "    train_loader, val_loader, test_loader, sv_loader = dataloaders\n",
    "else:\n",
    "    train_loader, val_loader, test_loader = dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtl trai\n",
    "from tqdm import tqdm_notebook\n",
    "from sv_system.train.train_utils import print_eval\n",
    "\n",
    "def joint_train(config, train_loader, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    loss_spk_sum = 0\n",
    "    loss_sent_sum = 0\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    lamb = config['lamb']\n",
    "    print_steps = (np.array([0.25, 0.5, 0.75, 1.0]) \\\n",
    "                    * len(train_loader)).astype(np.int64)\n",
    "\n",
    "    splice_frames = config['splice_frames']\n",
    "    if len(splice_frames) > 1:\n",
    "        splice_frames_ = np.random.randint(splice_frames[0], splice_frames[1])\n",
    "    else:\n",
    "        splice_frames_ = splice_frames[-1]\n",
    "\n",
    "    for batch_idx, (X, y_spk, y_sent) in enumerate(train_loader):\n",
    "        # X.shape is (batch, channel, time, bank)\n",
    "        X = X.narrow(2, 0, splice_frames_)\n",
    "        if not config[\"no_cuda\"]:\n",
    "            X = X.cuda()\n",
    "            y_spk = y_spk.cuda()\n",
    "            y_sent = y_sent.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logit1, logit2 = model(X)\n",
    "        loss_spk = criterion(logit1, y_spk) * (1-lamb)\n",
    "        loss_sent = criterion(logit2, y_sent) * (lamb)\n",
    "        loss = loss_spk + loss_sent \n",
    "        loss_sum += loss.item()\n",
    "        loss_spk_sum += loss_spk.item()\n",
    "        loss_sent_sum += loss_sent.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.argmax(logit1, dim=1)\n",
    "        corrects += predicted.eq(y_spk).cpu().sum().float()\n",
    "        total += y_spk.size(0)\n",
    "        if batch_idx in print_steps:\n",
    "            print(\"train loss: {:.4f}, spk_loss: {:.4f}, sent_loss: {:.4f}, acc: {:.5f} \" \\\n",
    "                  .format(corrects/total, loss_sum/total,\n",
    "                          loss_spk_sum/total, loss_sent_sum/total))\n",
    "            \n",
    "    return loss_sum, corrects/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lamb: 0.3\n",
      "------------------------------\n",
      "train loss: 0.0049, spk_loss: 0.0465, sent_loss: 0.0392, acc: 0.00732 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-36:\n",
      "Process Process-45:\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-48:\n",
      "Process Process-34:\n",
      "Process Process-46:\n",
      "Process Process-41:\n",
      "Process Process-40:\n",
      "Process Process-42:\n",
      "Process Process-35:\n",
      "Process Process-39:\n",
      "Process Process-47:\n",
      "Process Process-44:\n",
      "Process Process-43:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-33:\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-38:\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-37:\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 67, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])),                     self.spk_labels[index], self.sent_labels[index]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 67, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])),                     self.spk_labels[index], self.sent_labels[index]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 49, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 67, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])),                     self.spk_labels[index], self.sent_labels[index]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 67, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])),                     self.spk_labels[index], self.sent_labels[index]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 67, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])),                     self.spk_labels[index], self.sent_labels[index]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 49, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 23, in preprocess_audio\n",
      "    data[data > 0] = np.log(data[data > 0])\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 67, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])),                     self.spk_labels[index], self.sent_labels[index]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/posixpath.py\", line 81, in join\n",
      "    sep = _get_sep(a)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 35, in preprocess\n",
      "    data = librosa.core.load(example, sr=16000)[0]\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 49, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 67, in __getitem__\n",
      "    return self.preprocess(os.path.join(self.data_folder, self.audio_files[index])),                     self.spk_labels[index], self.sent_labels[index]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 49, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/posixpath.py\", line 42, in _get_sep\n",
      "    if isinstance(path, bytes):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/audio.py\", line 112, in load\n",
      "    with audioread.audio_open(os.path.realpath(path)) as input_file:\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-89fbc0fa46c5>\", line 49, in preprocess\n",
      "    input_feature = preprocess_audio(data, self.n_mels, self.filters, self.input_format)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-adb447da6423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     train code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoint_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     validation code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-8207fa2103ad>\u001b[0m in \u001b[0;36mjoint_train\u001b[0;34m(config, train_loader, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mcorrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_spk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my_spk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprint_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"../sv_system/data/manage_audio.py\", line 23, in preprocess_audio\n",
      "    data[data > 0] = np.log(data[data > 0])\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1494, in melspectrogram\n",
      "    mel_basis = filters.mel(sr, n_fft, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/audioread/__init__.py\", line 80, in audio_open\n",
      "    return rawread.RawAudioFile(path)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 274, in mel\n",
      "    weights[i] = np.maximum(0, np.minimum(lower, upper))\n",
      "  File \"../sv_system/data/manage_audio.py\", line 22, in preprocess_audio\n",
      "    data = librosa.feature.melspectrogram(data, sr=16000, n_mels=n_mels, hop_length=160, n_fft=480, fmin=20, fmax=4000)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/filters.py\", line 274, in mel\n",
      "    weights[i] = np.maximum(0, np.minimum(lower, upper))\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/audioread/rawread.py\", line 74, in __init__\n",
      "    self._file = wave.open(self._fh)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/feature/spectral.py\", line 1491, in melspectrogram\n",
      "    power=power)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/wave.py\", line 499, in open\n",
      "    return Wave_read(f)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/spectrum.py\", line 1531, in _spectrogram\n",
      "    S = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length))**power\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/wave.py\", line 163, in __init__\n",
      "    self.initfp(f)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/librosa/core/spectrum.py\", line 184, in stft\n",
      "    axis=0)[:stft_matrix.shape[0]]\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/wave.py\", line 138, in initfp\n",
      "    chunk = Chunk(self._file, bigendian = 0)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/scipy/fftpack/basic.py\", line 285, in fft\n",
      "    tmp = work_function(tmp,n,1,0,overwrite_x)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/chunk.py\", line 52, in __init__\n",
      "    def __init__(self, file, align=True, bigendian=True, inclheader=False):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sv_system.train.si_train import val, sv_test\n",
    "\n",
    "print(\"lamb: {}\".format(config['lamb']))\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "#     curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    \n",
    "#     #lr_scheduling\n",
    "#     idx = 0\n",
    "#     while(epoch_idx >= config['lr_schedule'][idx]):\n",
    "#     # use new lr from schedule epoch not a next epoch\n",
    "#         idx += 1\n",
    "#         if idx == len(config['lr_schedule']):\n",
    "#             break\n",
    "#     curr_lr = config['lrs'][idx]\n",
    "#     optimizer.state_dict()['param_groups'][0]['lr'] = curr_lr\n",
    "#     print(\"curr_lr: {}\".format(curr_lr))\n",
    "\n",
    "#     train code\n",
    "    train_loss, train_acc = joint_train(config, train_loader, model, joint_optimizer, criterion)\n",
    "\n",
    "#     validation code\n",
    "    val_loss, val_acc = val(config, val_loader, model, criterion)\n",
    "    \n",
    "    print(\"epoch #{}, train accuracy: {}\".format(epoch_idx, train_acc))\n",
    "    print(\"epoch #{}, val accuracy: {}\".format(epoch_idx, val_acc))\n",
    "\n",
    "#     evaluate best_metric\n",
    "    if not config['no_eer']:\n",
    "        # eer validation code\n",
    "        eer, label, score = sv_test(config, sv_loader, model, trial)\n",
    "        print(\"epoch #{}, sv eer: {}\".format(epoch_idx, eer))\n",
    "    \n",
    "    scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.cpu().state_dict(), open(\"gcommand_ResNet34_v1_mtl_lamb0.3.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV_Test\n",
    "\n",
    "equal_sent and diff_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.sv_score.score_utils import embeds_utterance\n",
    "\n",
    "def sv_test(config, sv_loader, model, trial):\n",
    "    embeddings, _ = embeds_utterance(config, sv_loader, model, lda=None)\n",
    "    sim_matrix = F.cosine_similarity(\n",
    "            embeddings.unsqueeze(1), embeddings.unsqueeze(0), dim=2)\n",
    "    cord = [trial.enrolment_id.tolist(), trial.test_id.tolist()]\n",
    "    score_vector = sim_matrix[cord].numpy()\n",
    "    label_vector = np.array(trial.label)\n",
    "    fpr, tpr, thres = roc_curve(\n",
    "            label_vector, score_vector, pos_label=1)\n",
    "    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
    "\n",
    "    return eer, label_vector, score_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_sent_trial = trial[trial.equal_command]\n",
    "diff_sent_trial = trial[~trial.equal_command]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "equal_sent_eer, _, _ = sv_test(config, sv_loader, model, equal_sent_trial)\n",
    "diff_sent_eer, _, _ = sv_test(config, sv_loader, model, diff_sent_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcommand_ResNet34_v1_mtl_lamb0.1.pt\n",
    "print(f\"equal: {equal_sent_eer}\\ndiff: {diff_sent_eer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['lamb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
