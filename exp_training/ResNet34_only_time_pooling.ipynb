{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet34_only_time_pooling\n",
    "\n",
    "kaldi의 tdnn_xvector 구조를 똑같이 구현하려고한다.\n",
    "\n",
    "목표는 utterance-level xvector의 generative한 특성을 살릴 수 있을까 알고싶은 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "# datasets\n",
    "# voxc1_fbank_xvector\n",
    "# gcommand_fbank_xvector\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\",\n",
    "                              input_frames=800, splice_frames=[300, 800], stride_frames=1, input_format='fbank',\n",
    "                              cuda=True,\n",
    "                              lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False,\n",
    "                              batch_size=128,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                              n_epochs=50\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.data_utils import find_dataset, find_trial\n",
    "\n",
    "_, datasets = find_dataset(config, basedir='../')\n",
    "trial = find_trial(config, basedir='../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_loaders\n",
    "\n",
    "dataloaders = init_loaders(config, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sv_system.model.model_utils import find_model\n",
    "# model = find_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.model.ResNet34 import ResNet34\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet34_time_pool(ResNet34):\n",
    "    \"\"\"\n",
    "        additional fc layer before output layer\n",
    "    \"\"\"\n",
    "    def __init__(self, config, inplanes=16, n_labels=1000, fc_dims=None):\n",
    "        super().__init__(config, inplanes, n_labels)\n",
    "\n",
    "        extractor_output_dim = 8*inplanes * 9\n",
    "        if not fc_dims:\n",
    "            fc_dims = extractor_output_dim\n",
    "\n",
    "        classifier = [nn.Linear(extractor_output_dim,\n",
    "            fc_dims),\n",
    "            nn.ReLU(inplace=True)]\n",
    "\n",
    "        loss_type = config[\"loss\"]\n",
    "        if loss_type == \"angular\":\n",
    "            classifier.append(AngleLinear(fc_dims, n_labels))\n",
    "        elif loss_type == \"softmax\":\n",
    "            classifier.append(nn.Linear(fc_dims, n_labels))\n",
    "        else:\n",
    "            print(\"not implemented loss\")\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.classifier = nn.Sequential(*classifier)\n",
    "        \n",
    "    def embed(self, x):\n",
    "        x = self.extractor(x)\n",
    "        x = F.avg_pool2d(x,(x.shape[-2],1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.embed(x)\n",
    "        out = self.classifier(feat)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet34_time_pool(config, inplanes=16, n_labels=config['n_labels'], fc_dims=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet34_time_pool(\n",
       "  (extractor): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Linear(in_features=512, out_features=1211, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.train.train_utils import set_seed, find_optimizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "criterion, optimizer = find_optimizer(config, model)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config['no_eer']:\n",
    "    train_loader, val_loader, test_loader, sv_loader = dataloaders\n",
    "else:\n",
    "    train_loader, val_loader, test_loader = dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += torch.sum(torch.eq(torch.argmax(logit, dim=1), y)).item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "                  .format(batch_idx+1, len(train_loader), loss_sum /(batch_idx+1),)\n",
    "                 )\n",
    "        acc = n_corrects / total\n",
    "\n",
    "    acc = n_corrects / total\n",
    "    return loss_sum, acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 6.925580\n",
      "Batch 200/1042\t Loss 6.728000\n",
      "Batch 300/1042\t Loss 6.536262\n",
      "Batch 400/1042\t Loss 6.359204\n",
      "Batch 500/1042\t Loss 6.179141\n",
      "Batch 600/1042\t Loss 6.017569\n",
      "Batch 700/1042\t Loss 5.856726\n",
      "Batch 800/1042\t Loss 5.704844\n",
      "Batch 900/1042\t Loss 5.557880\n",
      "Batch 1000/1042\t Loss 5.418602\n",
      "epoch #0, val accuracy: 0.07166020572185516\n",
      "epoch #0, sv eer: 0.19337663720583537\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 3.809774\n",
      "Batch 200/1042\t Loss 3.730361\n",
      "Batch 300/1042\t Loss 3.660816\n",
      "Batch 400/1042\t Loss 3.584737\n",
      "Batch 500/1042\t Loss 3.508593\n",
      "Batch 600/1042\t Loss 3.421987\n",
      "Batch 700/1042\t Loss 3.342123\n",
      "Batch 800/1042\t Loss 3.266431\n",
      "Batch 900/1042\t Loss 3.196370\n",
      "Batch 1000/1042\t Loss 3.129464\n",
      "epoch #1, val accuracy: 0.16760022938251495\n",
      "epoch #1, sv eer: 0.16180385475455222\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 2.284419\n",
      "Batch 200/1042\t Loss 2.250874\n",
      "Batch 300/1042\t Loss 2.199199\n",
      "Batch 400/1042\t Loss 2.160383\n",
      "Batch 500/1042\t Loss 2.131195\n",
      "Batch 600/1042\t Loss 2.098691\n",
      "Batch 700/1042\t Loss 2.065554\n",
      "Batch 800/1042\t Loss 2.029144\n",
      "Batch 900/1042\t Loss 1.995095\n",
      "Batch 1000/1042\t Loss 1.965226\n",
      "epoch #2, val accuracy: 0.34465548396110535\n",
      "epoch #2, sv eer: 0.13800447236716004\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.491348\n",
      "Batch 200/1042\t Loss 1.484908\n",
      "Batch 300/1042\t Loss 1.480664\n",
      "Batch 400/1042\t Loss 1.464399\n",
      "Batch 500/1042\t Loss 1.449762\n",
      "Batch 600/1042\t Loss 1.430053\n",
      "Batch 700/1042\t Loss 1.415971\n",
      "Batch 800/1042\t Loss 1.398543\n",
      "Batch 900/1042\t Loss 1.379044\n",
      "Batch 1000/1042\t Loss 1.363839\n",
      "epoch #3, val accuracy: 0.4074081778526306\n",
      "epoch #3, sv eer: 0.141039292940049\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 1.034833\n",
      "Batch 200/1042\t Loss 1.051398\n",
      "Batch 300/1042\t Loss 1.055977\n",
      "Batch 400/1042\t Loss 1.045428\n",
      "Batch 500/1042\t Loss 1.036656\n",
      "Batch 600/1042\t Loss 1.032484\n",
      "Batch 700/1042\t Loss 1.025675\n",
      "Batch 800/1042\t Loss 1.019022\n",
      "Batch 900/1042\t Loss 1.009693\n",
      "Batch 1000/1042\t Loss 1.000503\n",
      "epoch #4, val accuracy: 0.5151827931404114\n",
      "epoch #4, sv eer: 0.13560856138856353\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.773333\n",
      "Batch 200/1042\t Loss 0.789407\n",
      "Batch 300/1042\t Loss 0.783710\n",
      "Batch 400/1042\t Loss 0.783068\n",
      "Batch 500/1042\t Loss 0.784822\n",
      "Batch 600/1042\t Loss 0.782831\n",
      "Batch 700/1042\t Loss 0.776846\n",
      "Batch 800/1042\t Loss 0.773999\n",
      "Batch 900/1042\t Loss 0.775709\n",
      "Batch 1000/1042\t Loss 0.769337\n",
      "epoch #5, val accuracy: 0.537272572517395\n",
      "epoch #5, sv eer: 0.13017782983707804\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.597777\n",
      "Batch 200/1042\t Loss 0.607962\n",
      "Batch 300/1042\t Loss 0.617608\n",
      "Batch 400/1042\t Loss 0.620285\n",
      "Batch 500/1042\t Loss 0.624020\n",
      "Batch 600/1042\t Loss 0.622742\n",
      "Batch 700/1042\t Loss 0.622292\n",
      "Batch 800/1042\t Loss 0.623297\n",
      "Batch 900/1042\t Loss 0.625100\n",
      "Batch 1000/1042\t Loss 0.625837\n",
      "epoch #6, val accuracy: 0.503937840461731\n",
      "epoch #6, sv eer: 0.137951229900969\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.461332\n",
      "Batch 200/1042\t Loss 0.485850\n",
      "Batch 300/1042\t Loss 0.497131\n",
      "Batch 400/1042\t Loss 0.506903\n",
      "Batch 500/1042\t Loss 0.508157\n",
      "Batch 600/1042\t Loss 0.510755\n",
      "Batch 700/1042\t Loss 0.516676\n",
      "Batch 800/1042\t Loss 0.520834\n",
      "Batch 900/1042\t Loss 0.520826\n",
      "Batch 1000/1042\t Loss 0.520435\n",
      "epoch #7, val accuracy: 0.6000673770904541\n",
      "epoch #7, sv eer: 0.13310616547758491\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.418917\n",
      "Batch 200/1042\t Loss 0.412166\n",
      "Batch 300/1042\t Loss 0.421378\n",
      "Batch 400/1042\t Loss 0.434475\n",
      "Batch 500/1042\t Loss 0.441017\n",
      "Batch 600/1042\t Loss 0.442221\n",
      "Batch 700/1042\t Loss 0.444657\n",
      "Batch 800/1042\t Loss 0.447353\n",
      "Batch 900/1042\t Loss 0.451386\n",
      "Batch 1000/1042\t Loss 0.453305\n",
      "epoch #8, val accuracy: 0.5836211442947388\n",
      "epoch #8, sv eer: 0.12874028324992012\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.355150\n",
      "Batch 200/1042\t Loss 0.362462\n",
      "Batch 300/1042\t Loss 0.370032\n",
      "Batch 400/1042\t Loss 0.378292\n",
      "Batch 500/1042\t Loss 0.379236\n",
      "Batch 600/1042\t Loss 0.385105\n",
      "Batch 700/1042\t Loss 0.390660\n",
      "Batch 800/1042\t Loss 0.393095\n",
      "Batch 900/1042\t Loss 0.396496\n",
      "Batch 1000/1042\t Loss 0.395644\n",
      "epoch #9, val accuracy: 0.6369398832321167\n",
      "epoch #9, sv eer: 0.13049728463422425\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.310746\n",
      "Batch 200/1042\t Loss 0.328743\n",
      "Batch 300/1042\t Loss 0.333320\n",
      "Batch 400/1042\t Loss 0.338068\n",
      "Batch 500/1042\t Loss 0.346003\n",
      "Batch 600/1042\t Loss 0.351485\n",
      "Batch 700/1042\t Loss 0.354500\n",
      "Batch 800/1042\t Loss 0.357713\n",
      "Batch 900/1042\t Loss 0.361757\n",
      "Batch 1000/1042\t Loss 0.363502\n",
      "epoch #10, val accuracy: 0.5562668442726135\n",
      "epoch #10, sv eer: 0.1413587477371952\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.303812\n",
      "Batch 200/1042\t Loss 0.306353\n",
      "Batch 300/1042\t Loss 0.304812\n",
      "Batch 400/1042\t Loss 0.309340\n",
      "Batch 500/1042\t Loss 0.317131\n",
      "Batch 600/1042\t Loss 0.320318\n",
      "Batch 700/1042\t Loss 0.326071\n",
      "Batch 800/1042\t Loss 0.326878\n",
      "Batch 900/1042\t Loss 0.331389\n",
      "Batch 1000/1042\t Loss 0.332742\n",
      "epoch #11, val accuracy: 0.6086800694465637\n",
      "epoch #11, sv eer: 0.13534234905760834\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.283733\n",
      "Batch 200/1042\t Loss 0.282491\n",
      "Batch 300/1042\t Loss 0.285392\n",
      "Batch 400/1042\t Loss 0.291397\n",
      "Batch 500/1042\t Loss 0.292228\n",
      "Batch 600/1042\t Loss 0.296071\n",
      "Batch 700/1042\t Loss 0.299291\n",
      "Batch 800/1042\t Loss 0.305617\n",
      "Batch 900/1042\t Loss 0.309715\n",
      "Batch 1000/1042\t Loss 0.311958\n",
      "epoch #12, val accuracy: 0.5335031747817993\n",
      "epoch #12, sv eer: 0.1311894366947077\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.243867\n",
      "Batch 200/1042\t Loss 0.245049\n",
      "Batch 300/1042\t Loss 0.263586\n",
      "Batch 400/1042\t Loss 0.265447\n",
      "Batch 500/1042\t Loss 0.269036\n",
      "Batch 600/1042\t Loss 0.271773\n",
      "Batch 700/1042\t Loss 0.279836\n",
      "Batch 800/1042\t Loss 0.281482\n",
      "Batch 900/1042\t Loss 0.286188\n",
      "Batch 1000/1042\t Loss 0.289637\n",
      "epoch #13, val accuracy: 0.6382876038551331\n",
      "epoch #13, sv eer: 0.13896283675859866\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.221263\n",
      "Batch 200/1042\t Loss 0.222665\n",
      "Batch 300/1042\t Loss 0.230640\n",
      "Batch 400/1042\t Loss 0.237235\n",
      "Batch 500/1042\t Loss 0.241531\n",
      "Batch 600/1042\t Loss 0.249800\n",
      "Batch 700/1042\t Loss 0.257255\n",
      "Batch 800/1042\t Loss 0.260753\n",
      "Batch 900/1042\t Loss 0.266098\n",
      "Batch 1000/1042\t Loss 0.273374\n",
      "epoch #14, val accuracy: 0.6564605832099915\n",
      "epoch #14, sv eer: 0.12890001064849324\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.200883\n",
      "Batch 200/1042\t Loss 0.210339\n",
      "Batch 300/1042\t Loss 0.227689\n",
      "Batch 400/1042\t Loss 0.228713\n",
      "Batch 500/1042\t Loss 0.239002\n",
      "Batch 600/1042\t Loss 0.244464\n",
      "Batch 700/1042\t Loss 0.247468\n",
      "Batch 800/1042\t Loss 0.252537\n",
      "Batch 900/1042\t Loss 0.256459\n",
      "Batch 1000/1042\t Loss 0.259489\n",
      "epoch #15, val accuracy: 0.6372767686843872\n",
      "epoch #15, sv eer: 0.13395804493664146\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.218084\n",
      "Batch 200/1042\t Loss 0.211814\n",
      "Batch 300/1042\t Loss 0.216577\n",
      "Batch 400/1042\t Loss 0.225798\n",
      "Batch 500/1042\t Loss 0.230564\n",
      "Batch 600/1042\t Loss 0.236461\n",
      "Batch 700/1042\t Loss 0.241899\n",
      "Batch 800/1042\t Loss 0.243927\n",
      "Batch 900/1042\t Loss 0.247329\n",
      "Batch 1000/1042\t Loss 0.250428\n",
      "epoch #16, val accuracy: 0.6743597984313965\n",
      "epoch #16, sv eer: 0.12314982429986157\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.212090\n",
      "Batch 200/1042\t Loss 0.207848\n",
      "Batch 300/1042\t Loss 0.215016\n",
      "Batch 400/1042\t Loss 0.223262\n",
      "Batch 500/1042\t Loss 0.223478\n",
      "Batch 600/1042\t Loss 0.229421\n",
      "Batch 700/1042\t Loss 0.231409\n",
      "Batch 800/1042\t Loss 0.237175\n",
      "Batch 900/1042\t Loss 0.242520\n",
      "Batch 1000/1042\t Loss 0.243633\n",
      "epoch #17, val accuracy: 0.633001983165741\n",
      "epoch #17, sv eer: 0.13289319561282079\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.168473\n",
      "Batch 200/1042\t Loss 0.177616\n",
      "Batch 300/1042\t Loss 0.193557\n",
      "Batch 400/1042\t Loss 0.199606\n",
      "Batch 500/1042\t Loss 0.207744\n",
      "Batch 600/1042\t Loss 0.210027\n",
      "Batch 700/1042\t Loss 0.216772\n",
      "Batch 800/1042\t Loss 0.221392\n",
      "Batch 900/1042\t Loss 0.224302\n",
      "Batch 1000/1042\t Loss 0.225998\n",
      "epoch #18, val accuracy: 0.6293800473213196\n",
      "epoch #18, sv eer: 0.13299968054520286\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.188838\n",
      "Batch 200/1042\t Loss 0.193656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/1042\t Loss 0.192905\n",
      "Batch 400/1042\t Loss 0.199508\n",
      "Batch 500/1042\t Loss 0.206493\n",
      "Batch 600/1042\t Loss 0.210825\n",
      "Batch 700/1042\t Loss 0.211645\n",
      "Batch 800/1042\t Loss 0.217223\n",
      "Batch 900/1042\t Loss 0.222864\n",
      "Batch 1000/1042\t Loss 0.225701\n",
      "epoch #19, val accuracy: 0.6495956778526306\n",
      "epoch #19, sv eer: 0.126930039399425\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.167281\n",
      "Batch 200/1042\t Loss 0.170226\n",
      "Batch 300/1042\t Loss 0.181675\n",
      "Batch 400/1042\t Loss 0.190420\n",
      "Batch 500/1042\t Loss 0.191184\n",
      "Batch 600/1042\t Loss 0.199532\n",
      "Batch 700/1042\t Loss 0.203335\n",
      "Batch 800/1042\t Loss 0.208071\n",
      "Batch 900/1042\t Loss 0.210603\n",
      "Batch 1000/1042\t Loss 0.216963\n",
      "epoch #20, val accuracy: 0.6596192121505737\n",
      "epoch #20, sv eer: 0.12565222021084016\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.172383\n",
      "Batch 200/1042\t Loss 0.169364\n",
      "Batch 300/1042\t Loss 0.176494\n",
      "Batch 400/1042\t Loss 0.177937\n",
      "Batch 500/1042\t Loss 0.184656\n",
      "Batch 600/1042\t Loss 0.186327\n",
      "Batch 700/1042\t Loss 0.195737\n",
      "Batch 800/1042\t Loss 0.199538\n",
      "Batch 900/1042\t Loss 0.202230\n",
      "Batch 1000/1042\t Loss 0.207109\n",
      "epoch #21, val accuracy: 0.6708852648735046\n",
      "epoch #21, sv eer: 0.1286870407837291\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.142540\n",
      "Batch 200/1042\t Loss 0.156695\n",
      "Batch 300/1042\t Loss 0.169818\n",
      "Batch 400/1042\t Loss 0.172083\n",
      "Batch 500/1042\t Loss 0.177195\n",
      "Batch 600/1042\t Loss 0.184235\n",
      "Batch 700/1042\t Loss 0.192186\n",
      "Batch 800/1042\t Loss 0.194610\n",
      "Batch 900/1042\t Loss 0.198698\n",
      "Batch 1000/1042\t Loss 0.203024\n",
      "epoch #22, val accuracy: 0.6721066236495972\n",
      "epoch #22, sv eer: 0.13928229155574487\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.178364\n",
      "Batch 200/1042\t Loss 0.172551\n",
      "Batch 300/1042\t Loss 0.174321\n",
      "Batch 400/1042\t Loss 0.180374\n",
      "Batch 500/1042\t Loss 0.188265\n",
      "Batch 600/1042\t Loss 0.190314\n",
      "Batch 700/1042\t Loss 0.198050\n",
      "Batch 800/1042\t Loss 0.199892\n",
      "Batch 900/1042\t Loss 0.201133\n",
      "Batch 1000/1042\t Loss 0.204386\n",
      "epoch #23, val accuracy: 0.6749284267425537\n",
      "epoch #23, sv eer: 0.12708976679799808\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.153979\n",
      "Batch 200/1042\t Loss 0.161046\n",
      "Batch 300/1042\t Loss 0.162268\n",
      "Batch 400/1042\t Loss 0.169198\n",
      "Batch 500/1042\t Loss 0.169135\n",
      "Batch 600/1042\t Loss 0.176385\n",
      "Batch 700/1042\t Loss 0.180339\n",
      "Batch 800/1042\t Loss 0.184738\n",
      "Batch 900/1042\t Loss 0.190226\n",
      "Batch 1000/1042\t Loss 0.193481\n",
      "epoch #24, val accuracy: 0.7095266580581665\n",
      "epoch #24, sv eer: 0.13550207645618145\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.183943\n",
      "Batch 200/1042\t Loss 0.188672\n",
      "Batch 300/1042\t Loss 0.178822\n",
      "Batch 400/1042\t Loss 0.176688\n",
      "Batch 500/1042\t Loss 0.178709\n",
      "Batch 600/1042\t Loss 0.178898\n",
      "Batch 700/1042\t Loss 0.184627\n",
      "Batch 800/1042\t Loss 0.188928\n",
      "Batch 900/1042\t Loss 0.190999\n",
      "Batch 1000/1042\t Loss 0.193618\n",
      "epoch #25, val accuracy: 0.6336337327957153\n",
      "epoch #25, sv eer: 0.12905973804706633\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.142767\n",
      "Batch 200/1042\t Loss 0.142988\n",
      "Batch 300/1042\t Loss 0.149732\n",
      "Batch 400/1042\t Loss 0.149223\n",
      "Batch 500/1042\t Loss 0.158144\n",
      "Batch 600/1042\t Loss 0.160014\n",
      "Batch 700/1042\t Loss 0.166560\n",
      "Batch 800/1042\t Loss 0.174355\n",
      "Batch 900/1042\t Loss 0.177994\n",
      "Batch 1000/1042\t Loss 0.180090\n",
      "epoch #26, val accuracy: 0.7028090953826904\n",
      "epoch #26, sv eer: 0.12421467362368224\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.151641\n",
      "Batch 200/1042\t Loss 0.157515\n",
      "Batch 300/1042\t Loss 0.160505\n",
      "Batch 400/1042\t Loss 0.165599\n",
      "Batch 500/1042\t Loss 0.173308\n",
      "Batch 600/1042\t Loss 0.176923\n",
      "Batch 700/1042\t Loss 0.182640\n",
      "Batch 800/1042\t Loss 0.184399\n",
      "Batch 900/1042\t Loss 0.185609\n",
      "Batch 1000/1042\t Loss 0.187914\n",
      "epoch #27, val accuracy: 0.6160714626312256\n",
      "epoch #27, sv eer: 0.13379831753806837\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.140207\n",
      "Batch 200/1042\t Loss 0.148385\n",
      "Batch 300/1042\t Loss 0.148649\n",
      "Batch 400/1042\t Loss 0.154526\n",
      "Batch 500/1042\t Loss 0.158399\n",
      "Batch 600/1042\t Loss 0.160800\n",
      "Batch 700/1042\t Loss 0.166179\n",
      "Batch 800/1042\t Loss 0.170684\n",
      "Batch 900/1042\t Loss 0.172205\n",
      "Batch 1000/1042\t Loss 0.175965\n",
      "epoch #28, val accuracy: 0.687626302242279\n",
      "epoch #28, sv eer: 0.13374507507187733\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.149187\n",
      "Batch 200/1042\t Loss 0.158488\n",
      "Batch 300/1042\t Loss 0.163510\n",
      "Batch 400/1042\t Loss 0.163741\n",
      "Batch 500/1042\t Loss 0.171782\n",
      "Batch 600/1042\t Loss 0.173057\n",
      "Batch 700/1042\t Loss 0.173552\n",
      "Batch 800/1042\t Loss 0.175905\n",
      "Batch 900/1042\t Loss 0.178823\n",
      "Batch 1000/1042\t Loss 0.179791\n",
      "epoch #29, val accuracy: 0.6862154603004456\n",
      "epoch #29, sv eer: 0.12645085720370566\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.161439\n",
      "Batch 200/1042\t Loss 0.158084\n",
      "Batch 300/1042\t Loss 0.147608\n",
      "Batch 400/1042\t Loss 0.147878\n",
      "Batch 500/1042\t Loss 0.157961\n",
      "Batch 600/1042\t Loss 0.158933\n",
      "Batch 700/1042\t Loss 0.164623\n",
      "Batch 800/1042\t Loss 0.170170\n",
      "Batch 900/1042\t Loss 0.173015\n",
      "Batch 1000/1042\t Loss 0.177519\n",
      "epoch #30, val accuracy: 0.7088106274604797\n",
      "epoch #30, sv eer: 0.12948567777659462\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.179591\n",
      "Batch 200/1042\t Loss 0.167254\n",
      "Batch 300/1042\t Loss 0.165823\n",
      "Batch 400/1042\t Loss 0.172758\n",
      "Batch 500/1042\t Loss 0.170220\n",
      "Batch 600/1042\t Loss 0.173668\n",
      "Batch 700/1042\t Loss 0.176146\n",
      "Batch 800/1042\t Loss 0.180382\n",
      "Batch 900/1042\t Loss 0.182376\n",
      "Batch 1000/1042\t Loss 0.184117\n",
      "epoch #31, val accuracy: 0.6088696122169495\n",
      "epoch #31, sv eer: 0.1293259503780215\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.142815\n",
      "Batch 200/1042\t Loss 0.139476\n",
      "Batch 300/1042\t Loss 0.142290\n",
      "Batch 400/1042\t Loss 0.152549\n",
      "Batch 500/1042\t Loss 0.159288\n",
      "Batch 600/1042\t Loss 0.159946\n",
      "Batch 700/1042\t Loss 0.160184\n",
      "Batch 800/1042\t Loss 0.162206\n",
      "Batch 900/1042\t Loss 0.164515\n",
      "Batch 1000/1042\t Loss 0.167806\n",
      "epoch #32, val accuracy: 0.7040515542030334\n",
      "epoch #32, sv eer: 0.12714300926418912\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.134391\n",
      "Batch 200/1042\t Loss 0.136740\n",
      "Batch 300/1042\t Loss 0.144462\n",
      "Batch 400/1042\t Loss 0.144266\n",
      "Batch 500/1042\t Loss 0.146709\n",
      "Batch 600/1042\t Loss 0.156558\n",
      "Batch 700/1042\t Loss 0.156067\n",
      "Batch 800/1042\t Loss 0.156857\n",
      "Batch 900/1042\t Loss 0.161538\n",
      "Batch 1000/1042\t Loss 0.165759\n",
      "epoch #33, val accuracy: 0.7019036412239075\n",
      "epoch #33, sv eer: 0.1346501969971249\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.128609\n",
      "Batch 200/1042\t Loss 0.119512\n",
      "Batch 300/1042\t Loss 0.127957\n",
      "Batch 400/1042\t Loss 0.138536\n",
      "Batch 500/1042\t Loss 0.142323\n",
      "Batch 600/1042\t Loss 0.148137\n",
      "Batch 700/1042\t Loss 0.148107\n",
      "Batch 800/1042\t Loss 0.151649\n",
      "Batch 900/1042\t Loss 0.155375\n",
      "Batch 1000/1042\t Loss 0.158814\n",
      "epoch #34, val accuracy: 0.703251302242279\n",
      "epoch #34, sv eer: 0.12043445852411884\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.146785\n",
      "Batch 200/1042\t Loss 0.139815\n",
      "Batch 300/1042\t Loss 0.139789\n",
      "Batch 400/1042\t Loss 0.139819\n",
      "Batch 500/1042\t Loss 0.144657\n",
      "Batch 600/1042\t Loss 0.149823\n",
      "Batch 700/1042\t Loss 0.152802\n",
      "Batch 800/1042\t Loss 0.157462\n",
      "Batch 900/1042\t Loss 0.162255\n",
      "Batch 1000/1042\t Loss 0.165364\n",
      "epoch #35, val accuracy: 0.7043463587760925\n",
      "epoch #35, sv eer: 0.12453412842082845\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.135516\n",
      "Batch 200/1042\t Loss 0.130652\n",
      "Batch 300/1042\t Loss 0.134559\n",
      "Batch 400/1042\t Loss 0.133867\n",
      "Batch 500/1042\t Loss 0.138053\n",
      "Batch 600/1042\t Loss 0.143937\n",
      "Batch 700/1042\t Loss 0.148372\n",
      "Batch 800/1042\t Loss 0.150180\n",
      "Batch 900/1042\t Loss 0.154800\n",
      "Batch 1000/1042\t Loss 0.158248\n",
      "epoch #36, val accuracy: 0.6968918442726135\n",
      "epoch #36, sv eer: 0.12740922159514428\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.127713\n",
      "Batch 200/1042\t Loss 0.126835\n",
      "Batch 300/1042\t Loss 0.126622\n",
      "Batch 400/1042\t Loss 0.128527\n",
      "Batch 500/1042\t Loss 0.136175\n",
      "Batch 600/1042\t Loss 0.138655\n",
      "Batch 700/1042\t Loss 0.143766\n",
      "Batch 800/1042\t Loss 0.148223\n",
      "Batch 900/1042\t Loss 0.151434\n",
      "Batch 1000/1042\t Loss 0.155567\n",
      "epoch #37, val accuracy: 0.6769921183586121\n",
      "epoch #37, sv eer: 0.12975189010754978\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.136140\n",
      "Batch 200/1042\t Loss 0.134640\n",
      "Batch 300/1042\t Loss 0.136301\n",
      "Batch 400/1042\t Loss 0.137407\n",
      "Batch 500/1042\t Loss 0.143250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600/1042\t Loss 0.147488\n",
      "Batch 700/1042\t Loss 0.153435\n",
      "Batch 800/1042\t Loss 0.154090\n",
      "Batch 900/1042\t Loss 0.156282\n",
      "Batch 1000/1042\t Loss 0.157529\n",
      "epoch #38, val accuracy: 0.6742123961448669\n",
      "epoch #38, sv eer: 0.1296454051751677\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.135388\n",
      "Batch 200/1042\t Loss 0.129423\n",
      "Batch 300/1042\t Loss 0.132951\n",
      "Batch 400/1042\t Loss 0.134802\n",
      "Batch 500/1042\t Loss 0.136828\n",
      "Batch 600/1042\t Loss 0.143628\n",
      "Batch 700/1042\t Loss 0.146745\n",
      "Batch 800/1042\t Loss 0.149981\n",
      "Batch 900/1042\t Loss 0.150873\n",
      "Batch 1000/1042\t Loss 0.154479\n",
      "epoch #39, val accuracy: 0.6676634550094604\n",
      "epoch #39, sv eer: 0.13182834628900011\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.168544\n",
      "Batch 200/1042\t Loss 0.153246\n",
      "Batch 300/1042\t Loss 0.155489\n",
      "Batch 400/1042\t Loss 0.149845\n",
      "Batch 500/1042\t Loss 0.149067\n",
      "Batch 600/1042\t Loss 0.156104\n",
      "Batch 700/1042\t Loss 0.159256\n",
      "Batch 800/1042\t Loss 0.159665\n",
      "Batch 900/1042\t Loss 0.161150\n",
      "Batch 1000/1042\t Loss 0.161582\n",
      "epoch #40, val accuracy: 0.6907218098640442\n",
      "epoch #40, sv eer: 0.1275689489937174\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.112964\n",
      "Batch 200/1042\t Loss 0.121589\n",
      "Batch 300/1042\t Loss 0.121720\n",
      "Batch 400/1042\t Loss 0.125165\n",
      "Batch 500/1042\t Loss 0.128590\n",
      "Batch 600/1042\t Loss 0.134032\n",
      "Batch 700/1042\t Loss 0.140718\n",
      "Batch 800/1042\t Loss 0.145129\n",
      "Batch 900/1042\t Loss 0.148299\n",
      "Batch 1000/1042\t Loss 0.151150\n",
      "epoch #41, val accuracy: 0.6485006809234619\n",
      "epoch #41, sv eer: 0.13305292301139388\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.113375\n",
      "Batch 200/1042\t Loss 0.120484\n",
      "Batch 300/1042\t Loss 0.122315\n",
      "Batch 400/1042\t Loss 0.128053\n",
      "Batch 500/1042\t Loss 0.135300\n",
      "Batch 600/1042\t Loss 0.137629\n",
      "Batch 700/1042\t Loss 0.141950\n",
      "Batch 800/1042\t Loss 0.143296\n",
      "Batch 900/1042\t Loss 0.145733\n",
      "Batch 1000/1042\t Loss 0.149343\n",
      "epoch #42, val accuracy: 0.6605668663978577\n",
      "epoch #42, sv eer: 0.13331913534234907\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.146480\n",
      "Batch 200/1042\t Loss 0.130748\n",
      "Batch 300/1042\t Loss 0.130229\n",
      "Batch 400/1042\t Loss 0.131228\n",
      "Batch 500/1042\t Loss 0.137479\n",
      "Batch 600/1042\t Loss 0.142364\n",
      "Batch 700/1042\t Loss 0.147434\n",
      "Batch 800/1042\t Loss 0.150023\n",
      "Batch 900/1042\t Loss 0.152387\n",
      "Batch 1000/1042\t Loss 0.153837\n",
      "epoch #43, val accuracy: 0.7046622633934021\n",
      "epoch #43, sv eer: 0.13225428601852837\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.121302\n",
      "Batch 200/1042\t Loss 0.119013\n",
      "Batch 300/1042\t Loss 0.122688\n",
      "Batch 400/1042\t Loss 0.126666\n",
      "Batch 500/1042\t Loss 0.126100\n",
      "Batch 600/1042\t Loss 0.126726\n",
      "Batch 700/1042\t Loss 0.130353\n",
      "Batch 800/1042\t Loss 0.133092\n",
      "Batch 900/1042\t Loss 0.139585\n",
      "Batch 1000/1042\t Loss 0.144451\n",
      "epoch #44, val accuracy: 0.6907850503921509\n",
      "epoch #44, sv eer: 0.13102970929613458\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.127810\n",
      "Batch 200/1042\t Loss 0.121831\n",
      "Batch 300/1042\t Loss 0.127349\n",
      "Batch 400/1042\t Loss 0.129117\n",
      "Batch 500/1042\t Loss 0.132955\n",
      "Batch 600/1042\t Loss 0.141044\n",
      "Batch 700/1042\t Loss 0.140591\n",
      "Batch 800/1042\t Loss 0.141344\n",
      "Batch 900/1042\t Loss 0.144117\n",
      "Batch 1000/1042\t Loss 0.146675\n",
      "epoch #45, val accuracy: 0.6769921183586121\n",
      "epoch #45, sv eer: 0.13853689702907038\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.141538\n",
      "Batch 200/1042\t Loss 0.129040\n",
      "Batch 300/1042\t Loss 0.127154\n",
      "Batch 400/1042\t Loss 0.130181\n",
      "Batch 500/1042\t Loss 0.130172\n",
      "Batch 600/1042\t Loss 0.133251\n",
      "Batch 700/1042\t Loss 0.137752\n",
      "Batch 800/1042\t Loss 0.141926\n",
      "Batch 900/1042\t Loss 0.144462\n",
      "Batch 1000/1042\t Loss 0.147502\n",
      "epoch #46, val accuracy: 0.6567975282669067\n",
      "epoch #46, sv eer: 0.13757853263763178\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.136591\n",
      "Batch 200/1042\t Loss 0.119088\n",
      "Batch 300/1042\t Loss 0.126870\n",
      "Batch 400/1042\t Loss 0.132038\n",
      "Batch 500/1042\t Loss 0.134938\n",
      "Batch 600/1042\t Loss 0.138041\n",
      "Batch 700/1042\t Loss 0.142753\n",
      "Batch 800/1042\t Loss 0.146144\n",
      "Batch 900/1042\t Loss 0.147421\n",
      "Batch 1000/1042\t Loss 0.148257\n",
      "epoch #47, val accuracy: 0.7267099022865295\n",
      "epoch #47, sv eer: 0.1268235544670429\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.115251\n",
      "Batch 200/1042\t Loss 0.119472\n",
      "Batch 300/1042\t Loss 0.123788\n",
      "Batch 400/1042\t Loss 0.125567\n",
      "Batch 500/1042\t Loss 0.129071\n",
      "Batch 600/1042\t Loss 0.132736\n",
      "Batch 700/1042\t Loss 0.136529\n",
      "Batch 800/1042\t Loss 0.138257\n",
      "Batch 900/1042\t Loss 0.144010\n",
      "Batch 1000/1042\t Loss 0.146687\n",
      "epoch #48, val accuracy: 0.685941755771637\n",
      "epoch #48, sv eer: 0.12400170375891811\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1042\t Loss 0.123464\n",
      "Batch 200/1042\t Loss 0.115022\n",
      "Batch 300/1042\t Loss 0.110221\n",
      "Batch 400/1042\t Loss 0.115546\n",
      "Batch 500/1042\t Loss 0.122227\n",
      "Batch 600/1042\t Loss 0.126742\n",
      "Batch 700/1042\t Loss 0.131211\n",
      "Batch 800/1042\t Loss 0.133670\n",
      "Batch 900/1042\t Loss 0.136775\n",
      "Batch 1000/1042\t Loss 0.139353\n",
      "epoch #49, val accuracy: 0.7055466771125793\n",
      "epoch #49, sv eer: 0.12874028324992012\n"
     ]
    }
   ],
   "source": [
    "from sv_system.train.si_train import val, sv_test\n",
    "\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "\n",
    "#     train code\n",
    "    train_loss, train_acc = train(model)\n",
    "\n",
    "#     validation code\n",
    "    val_loss, val_acc = val(config, val_loader, model, criterion)\n",
    "    print(\"epoch #{}, val accuracy: {}\".format(epoch_idx, val_acc))\n",
    "\n",
    "#     evaluate best_metric\n",
    "    if not config['no_eer']:\n",
    "        # eer validation code\n",
    "        eer, label, score = sv_test(config, sv_loader, model, trial)\n",
    "        print(\"epoch #{}, sv eer: {}\".format(epoch_idx, eer))\n",
    "    \n",
    "    scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
