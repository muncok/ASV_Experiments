{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV Live Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extractor loaded from test.pth\n"
     ]
    }
   ],
   "source": [
    "from resNet34Models import ResNet34, ResNet34_v1\n",
    "\n",
    "config = dict(\n",
    "    loss=\"softmax\",\n",
    "    gpu_no=[0], no_cuda=True\n",
    ")\n",
    "\n",
    "model = ResNet34_v1(config, inplanes=32)\n",
    "model.load_extractor(\"test.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Speaker Verification System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from manage_audio import preprocess_audio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class sv_system():\n",
    "    def __init__(self, model, n_dims=40, feat_type='fbank'):\n",
    "        self.speaker_models = dict()\n",
    "        self.dct_filters = librosa.filters.dct(n_filters=n_dims, n_input=n_dims)\n",
    "        self.model = model\n",
    "        self.n_dims = n_dims\n",
    "        self.feat_type = feat_type\n",
    "        \n",
    "        # for test\n",
    "        self.speaker_models = self._random_speaker_model()\n",
    "    \n",
    "    def enrol(self, wav, spk_name):\n",
    "        feat = self._wav2feat(wav)\n",
    "        dvector = self._extract_dvector(feat).squeeze()\n",
    "        if spk_name not in self.speaker_models:\n",
    "            self.speaker_models[spk_name] = [dvector]\n",
    "        else:\n",
    "            self.speaker_models[spk_name] += dvector\n",
    "        \n",
    "    \n",
    "    def _extract_dvector(self, feat):\n",
    "        \"\"\"\n",
    "            dvector: ndarray\n",
    "        \"\"\"\n",
    "        if feat.dim() == 2:\n",
    "            feat = feat.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "        dvector = self.model.embed(feat).detach().to('cpu').numpy()\n",
    "        \n",
    "        return dvector\n",
    "    \n",
    "    def _wav2feat(self, wav):\n",
    "        wav_data = librosa.core.load(wav, sr=16000)[0]\n",
    "        feat = preprocess_audio(wav_data, n_mels=self.n_dims, \n",
    "                    dct_filters=self.dct_filters, in_feature=self.feat_type)\n",
    "        \n",
    "        return feat\n",
    "        \n",
    "    def _random_speaker_model(self):\n",
    "        random_dvector = np.random.rand(4, self.model.embed_dim)\n",
    "        random_speakers = [\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "        random_speaker_models = dict.fromkeys(random_speakers)\n",
    "\n",
    "        for i, key in enumerate(random_speaker_models.keys()):\n",
    "            random_speaker_models[key] = [random_dvector[i]]*2\n",
    "        \n",
    "        return random_speaker_models\n",
    "    \n",
    "    def verify(self, wav):\n",
    "        feat = self._wav2feat(wav)\n",
    "        test_dvector = self._extract_dvector(feat)\n",
    "        # order keep?\n",
    "        # averaging all dvectors for each speaker\n",
    "        avg_speaker_models = np.stack([np.mean(v, axis=0) for v in self.speaker_models.values()], \n",
    "                                      axis=0)\n",
    "\n",
    "        score = F.cosine_similarity(torch.from_numpy(avg_speaker_models).float(), \n",
    "                                    torch.from_numpy(test_dvector).float(), dim=1)\n",
    "        threshold = 0.5\n",
    "        pred_speaker = list(self.speaker_models.keys())[torch.argmax(score)]\n",
    "        if max(score) > threshold:\n",
    "            print(\"Accepted as {}\".format(pred_speaker))\n",
    "        else:\n",
    "            print(\"Reject\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sv_system = sv_system(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sv_system.enrol(\"../dataset/gcommand/gcommand_wav/bird/00f0204f_nohash_0.wav\", 'new2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification &  Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted as new2\n"
     ]
    }
   ],
   "source": [
    "test_sv_system.verify(\"../dataset/gcommand/gcommand_wav/bird/02e85b60_nohash_2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
