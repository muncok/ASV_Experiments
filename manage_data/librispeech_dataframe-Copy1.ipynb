{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-clean-360 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "libri_train360_dir = \"/dataset/SV_sets/librispeech/LibriSpeech/train-clean-360/\"\n",
    "\n",
    "train360_wav_list = glob.glob(os.path.join(libri_train360_dir, '*', '*', '*.wav'))\n",
    "\n",
    "train360_clean_df = pd.DataFrame(train360_wav_list, columns=['wav'])\n",
    "train360_clean_df = train360_clean_df.assign(id=train360_clean_df.wav.apply(lambda x: x.split(\"/\")[-1][:-9]))\n",
    "train360_clean_df = train360_clean_df.assign(spk=train360_clean_df.wav.apply(lambda x: x.split(\"/\")[-1].split(\"-\")[0]))\n",
    "train360_clean_df = train360_clean_df.set_index(\"id\")\n",
    "train360_clean_df = train360_clean_df.sort_index()\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean-360//wav.scp\", \"w\") as f:\n",
    "    for idx, row in train360_clean_df.iterrows():\n",
    "        f.write(idx+' '+row.wav+'\\n')\n",
    "\n",
    "spks = train360_clean_df.spk.unique().tolist()\n",
    "spk2utt_dict = dict()\n",
    "for spk in spks:\n",
    "    spk2utt_dict[spk] = ' '.join(train360_clean_df[train360_clean_df.spk == spk].index.tolist())\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean-360//spk2utt\", \"w\") as f:\n",
    "    for spk, utt in spk2utt_dict.items():\n",
    "        f.write(spk+\" \"+utt+\"\\n\")\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean-360/utt2spk\", \"w\") as f:\n",
    "    for idx, row in train360_clean_df.iterrows():\n",
    "        f.write(idx+\" \"+row.spk+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-clean-100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "libri_train100_dir = \"/dataset/SV_sets/librispeech/LibriSpeech/train-clean-100/\"\n",
    "\n",
    "train100_wav_list = glob.glob(os.path.join(libri_train100_dir, '*', '*', '*.wav'))\n",
    "\n",
    "train100_clean_df = pd.DataFrame(train100_wav_list, columns=['wav'])\n",
    "train100_clean_df = train100_clean_df.assign(id=train100_clean_df.wav.apply(lambda x: x.split(\"/\")[-1][:-9]))\n",
    "train100_clean_df = train100_clean_df.assign(spk=train100_clean_df.wav.apply(lambda x: x.split(\"/\")[-1].split(\"-\")[0]))\n",
    "train100_clean_df = train100_clean_df.set_index(\"id\")\n",
    "train100_clean_df = train100_clean_df.sort_index()\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean-100//wav.scp\", \"w\") as f:\n",
    "    for idx, row in train100_clean_df.iterrows():\n",
    "        f.write(idx+' '+row.wav+'\\n')\n",
    "\n",
    "spks = train100_clean_df.spk.unique().tolist()\n",
    "spk2utt_dict = dict()\n",
    "for spk in spks:\n",
    "    spk2utt_dict[spk] = ' '.join(train100_clean_df[train100_clean_df.spk == spk].index.tolist())\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean-100//spk2utt\", \"w\") as f:\n",
    "    for spk, utt in spk2utt_dict.items():\n",
    "        f.write(spk+\" \"+utt+\"\\n\")\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean-100/utt2spk\", \"w\") as f:\n",
    "    for idx, row in train100_clean_df.iterrows():\n",
    "        f.write(idx+\" \"+row.spk+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_df = pd.concat([train100_clean_df, train360_clean_df])\n",
    "train_clean_df = train_clean_df.sort_index()\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean/wav.scp\", \"w\") as f:\n",
    "    for idx, row in train_clean_df.iterrows():\n",
    "        f.write(idx+' '+row.wav+'\\n')\n",
    "\n",
    "spks = train_clean_df.spk.unique().tolist()\n",
    "spk2utt_dict = dict()\n",
    "for spk in spks:\n",
    "    spk2utt_dict[spk] = ' '.join(train_clean_df[train_clean_df.spk == spk].index.tolist())\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean//spk2utt\", \"w\") as f:\n",
    "    for spk, utt in spk2utt_dict.items():\n",
    "        f.write(spk+\" \"+utt+\"\\n\")\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean/utt2spk\", \"w\") as f:\n",
    "    for idx, row in train_clean_df.iterrows():\n",
    "        f.write(idx+\" \"+row.spk+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_df.columns = ['file', 'spk']\n",
    "train_clean_df['file'] = train_clean_df.file.apply(lambda x: x.rstrip(\"-norm.wav\"))\n",
    "train_clean_df['file'] = train_clean_df.file.apply(lambda x: re.sub(\"train-clean-...\", \"train-clean\", x))\n",
    "train_clean_df.to_pickle(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean/train_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dev-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "libri_dev_clean_dir = \"/dataset/SV_sets/librispeech/LibriSpeech/dev-clean/1272/128104/\"\n",
    "\n",
    "dev_wav_list = glob.glob(os.path.join(libri_dev_clean_dir, '*', '*', '*.wav'))\n",
    "\n",
    "dev_clean_df = pd.DataFrame(dev_wav_list, columns=['wav'])\n",
    "dev_clean_df = dev_clean_df.assign(id=dev_clean_df.wav.apply(lambda x: x.split(\"/\")[-1][:-9]))\n",
    "dev_clean_df = dev_clean_df.assign(spk=dev_clean_df.wav.apply(lambda x: x.split(\"/\")[-1].split(\"-\")[0]))\n",
    "dev_clean_df = dev_clean_df.set_index(\"id\")\n",
    "dev_clean_df = dev_clean_df.sort_index()\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/dev-clean/wav.scp\", \"w\") as f:\n",
    "    for idx, row in dev_clean_df.iterrows():\n",
    "        f.write(idx+' '+row.wav+'\\n')\n",
    "\n",
    "spks = dev_clean_df.spk.unique().tolist()\n",
    "spk2utt_dict = dict()\n",
    "for spk in spks:\n",
    "    spk2utt_dict[spk] = ' '.join(dev_clean_df[dev_clean_df.spk == spk].index.tolist())\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/dev-clean/spk2utt\", \"w\") as f:\n",
    "    for spk, utt in spk2utt_dict.items():\n",
    "        f.write(spk+\" \"+utt+\"\\n\")\n",
    "\n",
    "with open(\"/dataset/SV_sets/librispeech/LibriSpeech/dev-clean/utt2spk\", \"w\") as f:\n",
    "    for idx, row in dev_clean_df.iterrows():\n",
    "        f.write(idx+\" \"+row.spk+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_clean_df.columns = ['file', 'spk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_clean_df['file'] = dev_clean_df.file.apply(lambda x: x.rstrip(\"-norm.wav\"))\n",
    "dev_clean_df.to_pickle(\"/dataset/SV_sets/librispeech/LibriSpeech/dev-clean/dev_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embeds to files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key2df(keys, delimeter=\"-\"):\n",
    "    key_df = pd.DataFrame(keys, columns=['key'])\n",
    "    key_df['spk'] = key_df.key.apply(lambda x: x.split(delimeter)[0])\n",
    "    key_df['session'] = key_df.key.apply(lambda x: x.split(delimeter)[1])\n",
    "    key_df['label'] = key_df.groupby('spk').ngroup()\n",
    "    key_df['idx'] = range(len(key_df))\n",
    "    key_df = key_df.set_index('key')\n",
    "    \n",
    "    key_df['idx'] = range(len(key_df))\n",
    "    id2idx = key_df.idx.to_dict()\n",
    "    idx2id = {v:k for k,v in id2idx.items()}\n",
    "\n",
    "    return key_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeds = np.load(\"/dataset/SV_sets/librispeech/librispeech_embeds/train_embeds.npy\")\n",
    "dev_embeds = np.load(\"/dataset/SV_sets/librispeech/librispeech_embeds/dev_embeds.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys = pickle.load(open(\"/dataset/SV_sets/librispeech/librispeech_embeds/train_keys.pkl\", \"rb\"))\n",
    "dev_keys = pickle.load(open(\"/dataset/SV_sets/librispeech/librispeech_embeds/dev_keys.pkl\", \"rb\"))\n",
    "train_df = key2df(train_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "train_embed_mean = train_embeds.mean(0)\n",
    "centered_train_embeds = train_embeds - train_embed_mean.reshape(1, -1)\n",
    "centered_dev_embeds = dev_embeds - train_embed_mean.reshape(1, -1)\n",
    "\n",
    "clf = LDA(solver='svd', n_components=200)\n",
    "clf.fit(centered_train_embeds, train_df.label)\n",
    "\n",
    "lda_train_embeds = clf.transform(centered_train_embeds)\n",
    "lda_dev_embeds = clf.transform(centered_dev_embeds)\n",
    "\n",
    "# length normalization\n",
    "ln_lda_train_embeds = lda_train_embeds * np.sqrt(lda_train_embeds.shape[1]) / \\\n",
    "                                           np.linalg.norm(lda_train_embeds, axis=1, keepdims=True)\n",
    "ln_lda_dev_embeds = lda_dev_embeds * np.sqrt(lda_dev_embeds.shape[1]) / \\\n",
    "                                           np.linalg.norm(lda_dev_embeds, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/dataset/SV_sets/librispeech/librispeech_embeds/ln_lda_train_embeds.npy\", ln_lda_train_embeds)\n",
    "np.save(\"/dataset/SV_sets/librispeech/librispeech_embeds/ln_lda_dev_embeds.npy\", ln_lda_dev_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_embed_path = [os.path.join(\"/dataset/SV_sets/librispeech/LibriSpeech/dev-clean/\", \n",
    "                               key.split('-')[0], key.split('-')[1], key+\"-xvec.npy\")\n",
    "                               for key in dev_keys]\n",
    "\n",
    "for path, embed in zip(dev_embed_path, ln_lda_dev_embeds):\n",
    "    np.save(path, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embed_path = [os.path.join(\"/dataset/SV_sets/librispeech/LibriSpeech/train-clean/\", \n",
    "                               key.split('-')[0], key.split('-')[1], key+\"-xvec.npy\")\n",
    "                               for key in train_keys]\n",
    "\n",
    "for path, embed in zip(train_embed_path, ln_lda_train_embeds):\n",
    "    np.save(path, embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
