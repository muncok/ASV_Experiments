{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\", \n",
    "                              data_folder=\"/dataset/SV_sets/voxceleb12/feats/fbank64_vad/\",\n",
    "                              input_frames=400, splice_frames=[200, 400], stride_frames=1, \n",
    "                              input_format='fbank', input_dim=65, random_clip=True,\n",
    "                              n_epochs=200, lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False, batch_size=128,\n",
    "                              gpu_no=[0], cuda=True, num_workers=4,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.feat_dataset import FeatDataset\n",
    "dev_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_si.csv\")\n",
    "dev_df = dev_df.sample(frac=1.0)\n",
    "dev_train_df = dev_df[dev_df.set == 'train']\n",
    "dev_val_df = dev_df[dev_df.set == 'val']\n",
    "eval_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_eval.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train_dataset = FeatDataset.read_df(config, dev_train_df, 'train')\n",
    "dev_val_dataset = FeatDataset.read_df(config, dev_val_df, 'test')\n",
    "eval_dataset = FeatDataset.read_df(config, eval_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_default_loader \n",
    "dev_train_dataloader = init_default_loader(config, dev_train_dataset, shuffle=True, var_len=False) \n",
    "dev_val_dataloader = init_default_loader(config, dev_val_dataset, shuffle=False, var_len=False) \n",
    "eval_dataloader = init_default_loader(config, eval_dataset, shuffle=False, var_len=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnn_models import tdnn_xvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = tdnn_xvector(config, 512, n_labels=len(dev_df.label.unique()))\n",
    "# saved_model = torch.load(\"trained_models/vox1_dev_tdnn_xvector_held_out.pt\")\n",
    "# model.load_state_dict(saved_model)\n",
    "\n",
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(\"logs/baseline_voxc1_si\")\n",
    "model_path = \"trained_models/voxc1_si_tdnn_xvector.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/1047\t Loss 0.051042\n",
      "Batch 200/1047\t Loss 0.046820\n",
      "Batch 300/1047\t Loss 0.043741\n",
      "Batch 400/1047\t Loss 0.041190\n",
      "Batch 500/1047\t Loss 0.039033\n",
      "Batch 600/1047\t Loss 0.037154\n",
      "Batch 700/1047\t Loss 0.035498\n",
      "Batch 800/1047\t Loss 0.033987\n",
      "Batch 900/1047\t Loss 0.032641\n",
      "Batch 1000/1047\t Loss 0.031436\n",
      "epoch #0, train loss: 0.0309, train acc: 0.2422\n",
      "epoch #0, val loss: 0.0377, val acc: 0.1332\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.017070\n",
      "Batch 200/1047\t Loss 0.016758\n",
      "Batch 300/1047\t Loss 0.016217\n",
      "Batch 400/1047\t Loss 0.015779\n",
      "Batch 500/1047\t Loss 0.015351\n",
      "Batch 600/1047\t Loss 0.014940\n",
      "Batch 700/1047\t Loss 0.014498\n",
      "Batch 800/1047\t Loss 0.014117\n",
      "Batch 900/1047\t Loss 0.013739\n",
      "Batch 1000/1047\t Loss 0.013397\n",
      "epoch #1, train loss: 0.0133, train acc: 0.5958\n",
      "epoch #1, val loss: 0.0291, val acc: 0.2820\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.007998\n",
      "Batch 200/1047\t Loss 0.007977\n",
      "Batch 300/1047\t Loss 0.007981\n",
      "Batch 400/1047\t Loss 0.007843\n",
      "Batch 500/1047\t Loss 0.007816\n",
      "Batch 600/1047\t Loss 0.007744\n",
      "Batch 700/1047\t Loss 0.007633\n",
      "Batch 800/1047\t Loss 0.007523\n",
      "Batch 900/1047\t Loss 0.007428\n",
      "Batch 1000/1047\t Loss 0.007342\n",
      "epoch #2, train loss: 0.0073, train acc: 0.7634\n",
      "epoch #2, val loss: 0.0220, val acc: 0.4088\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.004750\n",
      "Batch 200/1047\t Loss 0.004710\n",
      "Batch 300/1047\t Loss 0.004709\n",
      "Batch 400/1047\t Loss 0.004738\n",
      "Batch 500/1047\t Loss 0.004737\n",
      "Batch 600/1047\t Loss 0.004753\n",
      "Batch 700/1047\t Loss 0.004750\n",
      "Batch 800/1047\t Loss 0.004744\n",
      "Batch 900/1047\t Loss 0.004725\n",
      "Batch 1000/1047\t Loss 0.004705\n",
      "epoch #3, train loss: 0.0047, train acc: 0.8417\n",
      "epoch #3, val loss: 0.0190, val acc: 0.4749\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.003027\n",
      "Batch 200/1047\t Loss 0.003002\n",
      "Batch 300/1047\t Loss 0.003078\n",
      "Batch 400/1047\t Loss 0.003113\n",
      "Batch 500/1047\t Loss 0.003162\n",
      "Batch 600/1047\t Loss 0.003228\n",
      "Batch 700/1047\t Loss 0.003258\n",
      "Batch 800/1047\t Loss 0.003276\n",
      "Batch 900/1047\t Loss 0.003295\n",
      "Batch 1000/1047\t Loss 0.003300\n",
      "epoch #4, train loss: 0.0033, train acc: 0.8859\n",
      "epoch #4, val loss: 0.0188, val acc: 0.4976\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.002268\n",
      "Batch 200/1047\t Loss 0.002257\n",
      "Batch 300/1047\t Loss 0.002302\n",
      "Batch 400/1047\t Loss 0.002331\n",
      "Batch 500/1047\t Loss 0.002365\n",
      "Batch 600/1047\t Loss 0.002383\n",
      "Batch 700/1047\t Loss 0.002418\n",
      "Batch 800/1047\t Loss 0.002454\n",
      "Batch 900/1047\t Loss 0.002481\n",
      "Batch 1000/1047\t Loss 0.002493\n",
      "epoch #5, train loss: 0.0025, train acc: 0.9138\n",
      "epoch #5, val loss: 0.0146, val acc: 0.5884\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001857\n",
      "Batch 200/1047\t Loss 0.001821\n",
      "Batch 300/1047\t Loss 0.001823\n",
      "Batch 400/1047\t Loss 0.001842\n",
      "Batch 500/1047\t Loss 0.001865\n",
      "Batch 600/1047\t Loss 0.001892\n",
      "Batch 700/1047\t Loss 0.001919\n",
      "Batch 800/1047\t Loss 0.001949\n",
      "Batch 900/1047\t Loss 0.001987\n",
      "Batch 1000/1047\t Loss 0.002012\n",
      "epoch #6, train loss: 0.0020, train acc: 0.9299\n",
      "epoch #6, val loss: 0.0180, val acc: 0.5181\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001561\n",
      "Batch 200/1047\t Loss 0.001521\n",
      "Batch 300/1047\t Loss 0.001521\n",
      "Batch 400/1047\t Loss 0.001539\n",
      "Batch 500/1047\t Loss 0.001561\n",
      "Batch 600/1047\t Loss 0.001588\n",
      "Batch 700/1047\t Loss 0.001602\n",
      "Batch 800/1047\t Loss 0.001635\n",
      "Batch 900/1047\t Loss 0.001677\n",
      "Batch 1000/1047\t Loss 0.001710\n",
      "epoch #7, train loss: 0.0017, train acc: 0.9401\n",
      "epoch #7, val loss: 0.0147, val acc: 0.5946\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001365\n",
      "Batch 200/1047\t Loss 0.001346\n",
      "Batch 300/1047\t Loss 0.001300\n",
      "Batch 400/1047\t Loss 0.001328\n",
      "Batch 500/1047\t Loss 0.001356\n",
      "Batch 600/1047\t Loss 0.001420\n",
      "Batch 700/1047\t Loss 0.001455\n",
      "Batch 800/1047\t Loss 0.001486\n",
      "Batch 900/1047\t Loss 0.001520\n",
      "Batch 1000/1047\t Loss 0.001535\n",
      "epoch #8, train loss: 0.0015, train acc: 0.9471\n",
      "epoch #8, val loss: 0.0139, val acc: 0.6042\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001193\n",
      "Batch 200/1047\t Loss 0.001205\n",
      "Batch 300/1047\t Loss 0.001210\n",
      "Batch 400/1047\t Loss 0.001221\n",
      "Batch 500/1047\t Loss 0.001248\n",
      "Batch 600/1047\t Loss 0.001255\n",
      "Batch 700/1047\t Loss 0.001262\n",
      "Batch 800/1047\t Loss 0.001294\n",
      "Batch 900/1047\t Loss 0.001338\n",
      "Batch 1000/1047\t Loss 0.001373\n",
      "epoch #9, train loss: 0.0014, train acc: 0.9526\n",
      "epoch #9, val loss: 0.0150, val acc: 0.5796\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001181\n",
      "Batch 200/1047\t Loss 0.001166\n",
      "Batch 300/1047\t Loss 0.001156\n",
      "Batch 400/1047\t Loss 0.001160\n",
      "Batch 500/1047\t Loss 0.001177\n",
      "Batch 600/1047\t Loss 0.001208\n",
      "Batch 700/1047\t Loss 0.001215\n",
      "Batch 800/1047\t Loss 0.001247\n",
      "Batch 900/1047\t Loss 0.001276\n",
      "Batch 1000/1047\t Loss 0.001303\n",
      "epoch #10, train loss: 0.0013, train acc: 0.9561\n",
      "epoch #10, val loss: 0.0158, val acc: 0.5697\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001010\n",
      "Batch 200/1047\t Loss 0.001008\n",
      "Batch 300/1047\t Loss 0.001047\n",
      "Batch 400/1047\t Loss 0.001058\n",
      "Batch 500/1047\t Loss 0.001071\n",
      "Batch 600/1047\t Loss 0.001084\n",
      "Batch 700/1047\t Loss 0.001095\n",
      "Batch 800/1047\t Loss 0.001113\n",
      "Batch 900/1047\t Loss 0.001149\n",
      "Batch 1000/1047\t Loss 0.001169\n",
      "epoch #11, train loss: 0.0012, train acc: 0.9608\n",
      "epoch #11, val loss: 0.0135, val acc: 0.6167\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001037\n",
      "Batch 200/1047\t Loss 0.000976\n",
      "Batch 300/1047\t Loss 0.000952\n",
      "Batch 400/1047\t Loss 0.000964\n",
      "Batch 500/1047\t Loss 0.000982\n",
      "Batch 600/1047\t Loss 0.001006\n",
      "Batch 700/1047\t Loss 0.001027\n",
      "Batch 800/1047\t Loss 0.001060\n",
      "Batch 900/1047\t Loss 0.001084\n",
      "Batch 1000/1047\t Loss 0.001126\n",
      "epoch #12, train loss: 0.0011, train acc: 0.9621\n",
      "epoch #12, val loss: 0.0131, val acc: 0.6313\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000992\n",
      "Batch 200/1047\t Loss 0.000982\n",
      "Batch 300/1047\t Loss 0.000942\n",
      "Batch 400/1047\t Loss 0.000948\n",
      "Batch 500/1047\t Loss 0.000971\n",
      "Batch 600/1047\t Loss 0.000996\n",
      "Batch 700/1047\t Loss 0.001011\n",
      "Batch 800/1047\t Loss 0.001045\n",
      "Batch 900/1047\t Loss 0.001077\n",
      "Batch 1000/1047\t Loss 0.001113\n",
      "epoch #13, train loss: 0.0011, train acc: 0.9625\n",
      "epoch #13, val loss: 0.0136, val acc: 0.6162\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000819\n",
      "Batch 200/1047\t Loss 0.000807\n",
      "Batch 300/1047\t Loss 0.000855\n",
      "Batch 400/1047\t Loss 0.000883\n",
      "Batch 500/1047\t Loss 0.000883\n",
      "Batch 600/1047\t Loss 0.000909\n",
      "Batch 700/1047\t Loss 0.000947\n",
      "Batch 800/1047\t Loss 0.000978\n",
      "Batch 900/1047\t Loss 0.001014\n",
      "Batch 1000/1047\t Loss 0.001043\n",
      "epoch #14, train loss: 0.0011, train acc: 0.9651\n",
      "epoch #14, val loss: 0.0138, val acc: 0.6147\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000968\n",
      "Batch 200/1047\t Loss 0.000934\n",
      "Batch 300/1047\t Loss 0.000911\n",
      "Batch 400/1047\t Loss 0.000912\n",
      "Batch 500/1047\t Loss 0.000912\n",
      "Batch 600/1047\t Loss 0.000923\n",
      "Batch 700/1047\t Loss 0.000943\n",
      "Batch 800/1047\t Loss 0.000969\n",
      "Batch 900/1047\t Loss 0.000988\n",
      "Batch 1000/1047\t Loss 0.001023\n",
      "epoch #15, train loss: 0.0010, train acc: 0.9659\n",
      "epoch #15, val loss: 0.0151, val acc: 0.5843\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000948\n",
      "Batch 200/1047\t Loss 0.000921\n",
      "Batch 300/1047\t Loss 0.000907\n",
      "Batch 400/1047\t Loss 0.000924\n",
      "Batch 500/1047\t Loss 0.000954\n",
      "Batch 600/1047\t Loss 0.000977\n",
      "Batch 700/1047\t Loss 0.000990\n",
      "Batch 800/1047\t Loss 0.001024\n",
      "Batch 900/1047\t Loss 0.001058\n",
      "Batch 1000/1047\t Loss 0.001101\n",
      "epoch #16, train loss: 0.0011, train acc: 0.9630\n",
      "epoch #16, val loss: 0.0134, val acc: 0.6184\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001029\n",
      "Batch 200/1047\t Loss 0.000921\n",
      "Batch 300/1047\t Loss 0.000889\n",
      "Batch 400/1047\t Loss 0.000876\n",
      "Batch 500/1047\t Loss 0.000881\n",
      "Batch 600/1047\t Loss 0.000889\n",
      "Batch 700/1047\t Loss 0.000900\n",
      "Batch 800/1047\t Loss 0.000916\n",
      "Batch 900/1047\t Loss 0.000927\n",
      "Batch 1000/1047\t Loss 0.000946\n",
      "epoch #17, train loss: 0.0010, train acc: 0.9696\n",
      "epoch #17, val loss: 0.0118, val acc: 0.6634\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000875\n",
      "Batch 200/1047\t Loss 0.000791\n",
      "Batch 300/1047\t Loss 0.000795\n",
      "Batch 400/1047\t Loss 0.000828\n",
      "Batch 500/1047\t Loss 0.000836\n",
      "Batch 600/1047\t Loss 0.000841\n",
      "Batch 700/1047\t Loss 0.000850\n",
      "Batch 800/1047\t Loss 0.000872\n",
      "Batch 900/1047\t Loss 0.000899\n",
      "Batch 1000/1047\t Loss 0.000915\n",
      "epoch #18, train loss: 0.0009, train acc: 0.9700\n",
      "epoch #18, val loss: 0.0130, val acc: 0.6339\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000863\n",
      "Batch 200/1047\t Loss 0.000816\n",
      "Batch 300/1047\t Loss 0.000808\n",
      "Batch 400/1047\t Loss 0.000803\n",
      "Batch 500/1047\t Loss 0.000812\n",
      "Batch 600/1047\t Loss 0.000815\n",
      "Batch 700/1047\t Loss 0.000841\n",
      "Batch 800/1047\t Loss 0.000869\n",
      "Batch 900/1047\t Loss 0.000897\n",
      "Batch 1000/1047\t Loss 0.000916\n",
      "epoch #19, train loss: 0.0009, train acc: 0.9708\n",
      "epoch #19, val loss: 0.0128, val acc: 0.6391\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000849\n",
      "Batch 200/1047\t Loss 0.000804\n",
      "Batch 300/1047\t Loss 0.000800\n",
      "Batch 400/1047\t Loss 0.000795\n",
      "Batch 500/1047\t Loss 0.000789\n",
      "Batch 600/1047\t Loss 0.000816\n",
      "Batch 700/1047\t Loss 0.000821\n",
      "Batch 800/1047\t Loss 0.000846\n",
      "Batch 900/1047\t Loss 0.000864\n",
      "Batch 1000/1047\t Loss 0.000890\n",
      "epoch #20, train loss: 0.0009, train acc: 0.9715\n",
      "epoch #20, val loss: 0.0129, val acc: 0.6374\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000760\n",
      "Batch 200/1047\t Loss 0.000751\n",
      "Batch 300/1047\t Loss 0.000758\n",
      "Batch 400/1047\t Loss 0.000765\n",
      "Batch 500/1047\t Loss 0.000772\n",
      "Batch 600/1047\t Loss 0.000781\n",
      "Batch 700/1047\t Loss 0.000794\n",
      "Batch 800/1047\t Loss 0.000817\n",
      "Batch 900/1047\t Loss 0.000839\n",
      "Batch 1000/1047\t Loss 0.000872\n",
      "epoch #21, train loss: 0.0009, train acc: 0.9720\n",
      "epoch #21, val loss: 0.0126, val acc: 0.6384\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000808\n",
      "Batch 200/1047\t Loss 0.000774\n",
      "Batch 300/1047\t Loss 0.000808\n",
      "Batch 400/1047\t Loss 0.000806\n",
      "Batch 500/1047\t Loss 0.000807\n",
      "Batch 600/1047\t Loss 0.000820\n",
      "Batch 700/1047\t Loss 0.000818\n",
      "Batch 800/1047\t Loss 0.000824\n",
      "Batch 900/1047\t Loss 0.000841\n",
      "Batch 1000/1047\t Loss 0.000862\n",
      "epoch #22, train loss: 0.0009, train acc: 0.9724\n",
      "epoch #22, val loss: 0.0158, val acc: 0.5741\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000813\n",
      "Batch 200/1047\t Loss 0.000738\n",
      "Batch 300/1047\t Loss 0.000719\n",
      "Batch 400/1047\t Loss 0.000730\n",
      "Batch 500/1047\t Loss 0.000743\n",
      "Batch 600/1047\t Loss 0.000754\n",
      "Batch 700/1047\t Loss 0.000763\n",
      "Batch 800/1047\t Loss 0.000766\n",
      "Batch 900/1047\t Loss 0.000777\n",
      "Batch 1000/1047\t Loss 0.000800\n",
      "epoch #23, train loss: 0.0008, train acc: 0.9756\n",
      "epoch #23, val loss: 0.0112, val acc: 0.6750\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000733\n",
      "Batch 200/1047\t Loss 0.000639\n",
      "Batch 300/1047\t Loss 0.000642\n",
      "Batch 400/1047\t Loss 0.000634\n",
      "Batch 500/1047\t Loss 0.000651\n",
      "Batch 600/1047\t Loss 0.000659\n",
      "Batch 700/1047\t Loss 0.000683\n",
      "Batch 800/1047\t Loss 0.000711\n",
      "Batch 900/1047\t Loss 0.000744\n",
      "Batch 1000/1047\t Loss 0.000780\n",
      "epoch #24, train loss: 0.0008, train acc: 0.9753\n",
      "epoch #24, val loss: 0.0143, val acc: 0.6034\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000784\n",
      "Batch 200/1047\t Loss 0.000756\n",
      "Batch 300/1047\t Loss 0.000742\n",
      "Batch 400/1047\t Loss 0.000728\n",
      "Batch 500/1047\t Loss 0.000739\n",
      "Batch 600/1047\t Loss 0.000762\n",
      "Batch 700/1047\t Loss 0.000776\n",
      "Batch 800/1047\t Loss 0.000797\n",
      "Batch 900/1047\t Loss 0.000815\n",
      "Batch 1000/1047\t Loss 0.000838\n",
      "epoch #25, train loss: 0.0008, train acc: 0.9736\n",
      "epoch #25, val loss: 0.0133, val acc: 0.6223\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000712\n",
      "Batch 200/1047\t Loss 0.000669\n",
      "Batch 300/1047\t Loss 0.000659\n",
      "Batch 400/1047\t Loss 0.000679\n",
      "Batch 500/1047\t Loss 0.000712\n",
      "Batch 600/1047\t Loss 0.000728\n",
      "Batch 700/1047\t Loss 0.000740\n",
      "Batch 800/1047\t Loss 0.000755\n",
      "Batch 900/1047\t Loss 0.000785\n",
      "Batch 1000/1047\t Loss 0.000817\n",
      "epoch #26, train loss: 0.0008, train acc: 0.9741\n",
      "epoch #26, val loss: 0.0126, val acc: 0.6403\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000696\n",
      "Batch 200/1047\t Loss 0.000645\n",
      "Batch 300/1047\t Loss 0.000658\n",
      "Batch 400/1047\t Loss 0.000663\n",
      "Batch 500/1047\t Loss 0.000683\n",
      "Batch 600/1047\t Loss 0.000691\n",
      "Batch 700/1047\t Loss 0.000708\n",
      "Batch 800/1047\t Loss 0.000724\n",
      "Batch 900/1047\t Loss 0.000743\n",
      "Batch 1000/1047\t Loss 0.000761\n",
      "epoch #27, train loss: 0.0008, train acc: 0.9767\n",
      "epoch #27, val loss: 0.0117, val acc: 0.6609\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000711\n",
      "Batch 200/1047\t Loss 0.000662\n",
      "Batch 300/1047\t Loss 0.000645\n",
      "Batch 400/1047\t Loss 0.000658\n",
      "Batch 500/1047\t Loss 0.000672\n",
      "Batch 600/1047\t Loss 0.000699\n",
      "Batch 700/1047\t Loss 0.000727\n",
      "Batch 800/1047\t Loss 0.000744\n",
      "Batch 900/1047\t Loss 0.000759\n",
      "Batch 1000/1047\t Loss 0.000785\n",
      "epoch #28, train loss: 0.0008, train acc: 0.9756\n",
      "epoch #28, val loss: 0.0122, val acc: 0.6506\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000726\n",
      "Batch 200/1047\t Loss 0.000679\n",
      "Batch 300/1047\t Loss 0.000692\n",
      "Batch 400/1047\t Loss 0.000694\n",
      "Batch 500/1047\t Loss 0.000724\n",
      "Batch 600/1047\t Loss 0.000736\n",
      "Batch 700/1047\t Loss 0.000742\n",
      "Batch 800/1047\t Loss 0.000756\n",
      "Batch 900/1047\t Loss 0.000775\n",
      "Batch 1000/1047\t Loss 0.000794\n",
      "epoch #29, train loss: 0.0008, train acc: 0.9755\n",
      "epoch #29, val loss: 0.0114, val acc: 0.6636\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000633\n",
      "Batch 200/1047\t Loss 0.000605\n",
      "Batch 300/1047\t Loss 0.000600\n",
      "Batch 400/1047\t Loss 0.000592\n",
      "Batch 500/1047\t Loss 0.000582\n",
      "Batch 600/1047\t Loss 0.000591\n",
      "Batch 700/1047\t Loss 0.000607\n",
      "Batch 800/1047\t Loss 0.000626\n",
      "Batch 900/1047\t Loss 0.000647\n",
      "Batch 1000/1047\t Loss 0.000670\n",
      "epoch #30, train loss: 0.0007, train acc: 0.9807\n",
      "epoch #30, val loss: 0.0111, val acc: 0.6811\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000604\n",
      "Batch 200/1047\t Loss 0.000591\n",
      "Batch 300/1047\t Loss 0.000581\n",
      "Batch 400/1047\t Loss 0.000595\n",
      "Batch 500/1047\t Loss 0.000613\n",
      "Batch 600/1047\t Loss 0.000637\n",
      "Batch 700/1047\t Loss 0.000656\n",
      "Batch 800/1047\t Loss 0.000672\n",
      "Batch 900/1047\t Loss 0.000686\n",
      "Batch 1000/1047\t Loss 0.000710\n",
      "epoch #31, train loss: 0.0007, train acc: 0.9789\n",
      "epoch #31, val loss: 0.0121, val acc: 0.6507\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000789\n",
      "Batch 200/1047\t Loss 0.000735\n",
      "Batch 300/1047\t Loss 0.000710\n",
      "Batch 400/1047\t Loss 0.000713\n",
      "Batch 500/1047\t Loss 0.000729\n",
      "Batch 600/1047\t Loss 0.000767\n",
      "Batch 700/1047\t Loss 0.000791\n",
      "Batch 800/1047\t Loss 0.000826\n",
      "Batch 900/1047\t Loss 0.000855\n",
      "Batch 1000/1047\t Loss 0.000896\n",
      "epoch #32, train loss: 0.0009, train acc: 0.9719\n",
      "epoch #32, val loss: 0.0139, val acc: 0.6096\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000891\n",
      "Batch 200/1047\t Loss 0.000801\n",
      "Batch 300/1047\t Loss 0.000804\n",
      "Batch 400/1047\t Loss 0.000791\n",
      "Batch 500/1047\t Loss 0.000785\n",
      "Batch 600/1047\t Loss 0.000776\n",
      "Batch 700/1047\t Loss 0.000778\n",
      "Batch 800/1047\t Loss 0.000797\n",
      "Batch 900/1047\t Loss 0.000818\n",
      "Batch 1000/1047\t Loss 0.000825\n",
      "epoch #33, train loss: 0.0008, train acc: 0.9743\n",
      "epoch #33, val loss: 0.0126, val acc: 0.6374\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000387\n",
      "Batch 200/1047\t Loss 0.000327\n",
      "Batch 300/1047\t Loss 0.000282\n",
      "Batch 400/1047\t Loss 0.000259\n",
      "Batch 500/1047\t Loss 0.000238\n",
      "Batch 600/1047\t Loss 0.000223\n",
      "Batch 700/1047\t Loss 0.000209\n",
      "Batch 800/1047\t Loss 0.000201\n",
      "Batch 900/1047\t Loss 0.000192\n",
      "Batch 1000/1047\t Loss 0.000185\n",
      "epoch #34, train loss: 0.0002, train acc: 0.9965\n",
      "epoch #34, val loss: 0.0054, val acc: 0.8327\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000088\n",
      "Batch 200/1047\t Loss 0.000095\n",
      "Batch 300/1047\t Loss 0.000093\n",
      "Batch 400/1047\t Loss 0.000092\n",
      "Batch 500/1047\t Loss 0.000091\n",
      "Batch 600/1047\t Loss 0.000090\n",
      "Batch 700/1047\t Loss 0.000089\n",
      "Batch 800/1047\t Loss 0.000089\n",
      "Batch 900/1047\t Loss 0.000087\n",
      "Batch 1000/1047\t Loss 0.000086\n",
      "epoch #35, train loss: 0.0001, train acc: 0.9990\n",
      "epoch #35, val loss: 0.0051, val acc: 0.8420\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000065\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #36, train loss: 0.0001, train acc: 0.9995\n",
      "epoch #36, val loss: 0.0050, val acc: 0.8467\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000058\n",
      "Batch 200/1047\t Loss 0.000062\n",
      "Batch 300/1047\t Loss 0.000063\n",
      "Batch 400/1047\t Loss 0.000063\n",
      "Batch 500/1047\t Loss 0.000065\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000065\n",
      "Batch 800/1047\t Loss 0.000065\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #37, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #37, val loss: 0.0049, val acc: 0.8508\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000062\n",
      "Batch 200/1047\t Loss 0.000062\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000064\n",
      "Batch 500/1047\t Loss 0.000065\n",
      "Batch 600/1047\t Loss 0.000065\n",
      "Batch 700/1047\t Loss 0.000064\n",
      "Batch 800/1047\t Loss 0.000065\n",
      "Batch 900/1047\t Loss 0.000065\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #38, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #38, val loss: 0.0048, val acc: 0.8554\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000061\n",
      "Batch 200/1047\t Loss 0.000062\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000065\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #39, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #39, val loss: 0.0048, val acc: 0.8564\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000069\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000070\n",
      "epoch #40, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #40, val loss: 0.0048, val acc: 0.8593\n",
      "------------------------------\n",
      "curr_lr: 0.004000000000000001\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #41, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #41, val loss: 0.0047, val acc: 0.8610\n",
      "------------------------------\n",
      "curr_lr: 0.004000000000000001\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #42, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #42, val loss: 0.0047, val acc: 0.8616\n",
      "------------------------------\n",
      "curr_lr: 0.004000000000000001\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #43, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #43, val loss: 0.0047, val acc: 0.8607\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #44, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #44, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000070\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #45, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #45, val loss: 0.0047, val acc: 0.8611\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #46, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #46, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #47, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #47, val loss: 0.0047, val acc: 0.8607\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000064\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #48, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #48, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #49, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #49, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #50, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #50, val loss: 0.0047, val acc: 0.8619\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000072\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #51, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #51, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000064\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000065\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #52, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #52, val loss: 0.0047, val acc: 0.8611\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #53, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #53, val loss: 0.0047, val acc: 0.8617\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #54, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #54, val loss: 0.0047, val acc: 0.8623\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000070\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #55, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #55, val loss: 0.0047, val acc: 0.8612\n",
      "------------------------------\n",
      "curr_lr: 1.2800000000000007e-06\n",
      "Batch 100/1047\t Loss 0.000070\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000071\n",
      "Batch 500/1047\t Loss 0.000070\n",
      "Batch 600/1047\t Loss 0.000070\n",
      "Batch 700/1047\t Loss 0.000070\n",
      "Batch 800/1047\t Loss 0.000070\n",
      "Batch 900/1047\t Loss 0.000070\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #56, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #56, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.2800000000000007e-06\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #57, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #57, val loss: 0.0047, val acc: 0.8619\n",
      "------------------------------\n",
      "curr_lr: 1.2800000000000007e-06\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000071\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #58, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #58, val loss: 0.0047, val acc: 0.8624\n",
      "------------------------------\n",
      "curr_lr: 2.560000000000001e-07\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #59, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #59, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 2.560000000000001e-07\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #60, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #60, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 2.560000000000001e-07\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000064\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #61, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #61, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 5.120000000000003e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #62, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #62, val loss: 0.0047, val acc: 0.8616\n",
      "------------------------------\n",
      "curr_lr: 5.120000000000003e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #63, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #63, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 5.120000000000003e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #64, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #64, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #65, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #65, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #66, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #66, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #67, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #67, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #68, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #68, val loss: 0.0047, val acc: 0.8629\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #69, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #69, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #70, train loss: 0.0001, train acc: 0.9998\n",
      "epoch #70, val loss: 0.0047, val acc: 0.8607\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #71, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #71, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #72, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #72, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #73, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #73, val loss: 0.0047, val acc: 0.8610\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #74, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #74, val loss: 0.0047, val acc: 0.8609\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #75, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #75, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #76, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #76, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #77, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #77, val loss: 0.0047, val acc: 0.8616\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #78, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #78, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #79, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #79, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #80, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #80, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #81, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #81, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #82, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #82, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #83, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #83, val loss: 0.0047, val acc: 0.8623\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #84, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #84, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #85, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #85, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #86, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #86, val loss: 0.0047, val acc: 0.8623\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #87, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #87, val loss: 0.0047, val acc: 0.8613\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #88, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #88, val loss: 0.0047, val acc: 0.8610\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #89, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #89, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000064\n",
      "Batch 300/1047\t Loss 0.000064\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000065\n",
      "Batch 600/1047\t Loss 0.000065\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #90, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #90, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000070\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #91, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #91, val loss: 0.0047, val acc: 0.8627\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #92, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #92, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #93, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #93, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #94, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #94, val loss: 0.0047, val acc: 0.8627\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #95, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #95, val loss: 0.0047, val acc: 0.8616\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000070\n",
      "Batch 500/1047\t Loss 0.000070\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #96, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #96, val loss: 0.0047, val acc: 0.8617\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000065\n",
      "Batch 500/1047\t Loss 0.000065\n",
      "Batch 600/1047\t Loss 0.000065\n",
      "Batch 700/1047\t Loss 0.000065\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #97, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #97, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #98, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #98, val loss: 0.0047, val acc: 0.8617\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #99, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #99, val loss: 0.0047, val acc: 0.8610\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #100, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #100, val loss: 0.0047, val acc: 0.8617\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000065\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #101, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #101, val loss: 0.0047, val acc: 0.8619\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #102, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #102, val loss: 0.0047, val acc: 0.8615\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000073\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #103, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #103, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #104, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #104, val loss: 0.0047, val acc: 0.8606\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #105, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #105, val loss: 0.0047, val acc: 0.8609\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #106, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #106, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000072\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #107, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #107, val loss: 0.0047, val acc: 0.8619\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #108, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #108, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #109, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #109, val loss: 0.0047, val acc: 0.8615\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #110, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #110, val loss: 0.0047, val acc: 0.8612\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #111, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #111, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #112, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #112, val loss: 0.0047, val acc: 0.8609\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000073\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #113, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #113, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #114, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #114, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #115, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #115, val loss: 0.0046, val acc: 0.8624\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #116, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #116, val loss: 0.0047, val acc: 0.8616\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000069\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #117, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #117, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #118, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #118, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #119, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #119, val loss: 0.0047, val acc: 0.8612\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #120, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #120, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #121, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #121, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000069\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #122, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #122, val loss: 0.0047, val acc: 0.8626\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #123, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #123, val loss: 0.0047, val acc: 0.8623\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000064\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000064\n",
      "Batch 500/1047\t Loss 0.000065\n",
      "Batch 600/1047\t Loss 0.000065\n",
      "Batch 700/1047\t Loss 0.000065\n",
      "Batch 800/1047\t Loss 0.000065\n",
      "Batch 900/1047\t Loss 0.000065\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #124, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #124, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #125, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #125, val loss: 0.0047, val acc: 0.8627\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #126, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #126, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #127, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #127, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #128, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #128, val loss: 0.0047, val acc: 0.8624\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #129, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #129, val loss: 0.0047, val acc: 0.8627\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #130, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #130, val loss: 0.0047, val acc: 0.8615\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #131, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #131, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #132, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #132, val loss: 0.0047, val acc: 0.8617\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #133, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #133, val loss: 0.0047, val acc: 0.8617\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #134, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #134, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #135, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #135, val loss: 0.0047, val acc: 0.8615\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000071\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #136, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #136, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #137, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #137, val loss: 0.0047, val acc: 0.8612\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000065\n",
      "Batch 600/1047\t Loss 0.000065\n",
      "Batch 700/1047\t Loss 0.000065\n",
      "Batch 800/1047\t Loss 0.000065\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #138, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #138, val loss: 0.0047, val acc: 0.8627\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #139, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #139, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #140, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #140, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000069\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #141, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #141, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #142, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #142, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000063\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #143, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #143, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #144, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #144, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #145, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #145, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #146, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #146, val loss: 0.0047, val acc: 0.8633\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000069\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #147, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #147, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000064\n",
      "Batch 400/1047\t Loss 0.000064\n",
      "Batch 500/1047\t Loss 0.000064\n",
      "Batch 600/1047\t Loss 0.000064\n",
      "Batch 700/1047\t Loss 0.000065\n",
      "Batch 800/1047\t Loss 0.000065\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #148, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #148, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #149, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #149, val loss: 0.0047, val acc: 0.8602\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #150, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #150, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #151, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #151, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #152, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #152, val loss: 0.0046, val acc: 0.8626\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #153, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #153, val loss: 0.0047, val acc: 0.8610\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #154, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #154, val loss: 0.0047, val acc: 0.8624\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #155, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #155, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #156, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #156, val loss: 0.0047, val acc: 0.8629\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #157, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #157, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #158, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #158, val loss: 0.0047, val acc: 0.8615\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000065\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #159, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #159, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #160, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #160, val loss: 0.0047, val acc: 0.8624\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #161, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #161, val loss: 0.0047, val acc: 0.8612\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000064\n",
      "Batch 400/1047\t Loss 0.000064\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #162, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #162, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #163, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #163, val loss: 0.0047, val acc: 0.8628\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #164, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #164, val loss: 0.0047, val acc: 0.8628\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000069\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #165, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #165, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #166, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #166, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #167, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #167, val loss: 0.0047, val acc: 0.8615\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #168, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #168, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000064\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000064\n",
      "Batch 500/1047\t Loss 0.000065\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #169, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #169, val loss: 0.0047, val acc: 0.8629\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000070\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #170, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #170, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #171, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #171, val loss: 0.0047, val acc: 0.8625\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000065\n",
      "Batch 800/1047\t Loss 0.000065\n",
      "Batch 900/1047\t Loss 0.000065\n",
      "Batch 1000/1047\t Loss 0.000065\n",
      "epoch #172, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #172, val loss: 0.0047, val acc: 0.8613\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000070\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000069\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #173, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #173, val loss: 0.0047, val acc: 0.8616\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #174, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #174, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #175, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #175, val loss: 0.0047, val acc: 0.8624\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000068\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #176, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #176, val loss: 0.0047, val acc: 0.8617\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #177, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #177, val loss: 0.0047, val acc: 0.8613\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #178, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #178, val loss: 0.0047, val acc: 0.8621\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #179, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #179, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000064\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #180, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #180, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #181, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #181, val loss: 0.0047, val acc: 0.8629\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #182, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #182, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #183, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #183, val loss: 0.0047, val acc: 0.8607\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000070\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000070\n",
      "Batch 500/1047\t Loss 0.000070\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000069\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #184, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #184, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #185, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #185, val loss: 0.0047, val acc: 0.8617\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #186, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #186, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #187, train loss: 0.0001, train acc: 1.0000\n",
      "epoch #187, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #188, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #188, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #189, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #189, val loss: 0.0047, val acc: 0.8611\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #190, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #190, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000071\n",
      "Batch 200/1047\t Loss 0.000069\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #191, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #191, val loss: 0.0047, val acc: 0.8619\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000065\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000068\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #192, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #192, val loss: 0.0047, val acc: 0.8610\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000069\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000068\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #193, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #193, val loss: 0.0047, val acc: 0.8614\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000067\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #194, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #194, val loss: 0.0047, val acc: 0.8610\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000070\n",
      "Batch 200/1047\t Loss 0.000072\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000068\n",
      "Batch 1000/1047\t Loss 0.000069\n",
      "epoch #195, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #195, val loss: 0.0047, val acc: 0.8622\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000067\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000066\n",
      "Batch 900/1047\t Loss 0.000066\n",
      "Batch 1000/1047\t Loss 0.000066\n",
      "epoch #196, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #196, val loss: 0.0047, val acc: 0.8623\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000064\n",
      "Batch 200/1047\t Loss 0.000066\n",
      "Batch 300/1047\t Loss 0.000066\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000067\n",
      "Batch 600/1047\t Loss 0.000067\n",
      "Batch 700/1047\t Loss 0.000067\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #197, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #197, val loss: 0.0047, val acc: 0.8618\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000074\n",
      "Batch 200/1047\t Loss 0.000070\n",
      "Batch 300/1047\t Loss 0.000070\n",
      "Batch 400/1047\t Loss 0.000069\n",
      "Batch 500/1047\t Loss 0.000069\n",
      "Batch 600/1047\t Loss 0.000069\n",
      "Batch 700/1047\t Loss 0.000068\n",
      "Batch 800/1047\t Loss 0.000068\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #198, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #198, val loss: 0.0047, val acc: 0.8620\n",
      "------------------------------\n",
      "curr_lr: 1.0240000000000006e-08\n",
      "Batch 100/1047\t Loss 0.000068\n",
      "Batch 200/1047\t Loss 0.000065\n",
      "Batch 300/1047\t Loss 0.000065\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000066\n",
      "Batch 700/1047\t Loss 0.000066\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000067\n",
      "Batch 1000/1047\t Loss 0.000067\n",
      "epoch #199, train loss: 0.0001, train acc: 0.9999\n",
      "epoch #199, val loss: 0.0047, val acc: 0.8622\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "from sklearn.metrics import roc_curve\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(reduce=True)\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=2)\n",
    "step_scheduler = MultiStepLR(optimizer, [30], 0.1)\n",
    "\n",
    "for epoch_idx in range(0, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    \n",
    "    n_sel_samples = 0\n",
    "# =============== train code #===============\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "                  .format(batch_idx+1, len(dev_train_dataloader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "    plateau_scheduler.step(train_loss)\n",
    "    \n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))\n",
    "    writer.add_scalar(\"train/loss\", train_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"train/acc\", train_acc, epoch_idx+1)\n",
    "    \n",
    "\n",
    "#=============== dev_val code #===============\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    val_loss = loss_sum / total\n",
    "    val_acc = n_corrects / total\n",
    "    \n",
    "    \n",
    "    print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))\n",
    "    writer.add_scalar(\"val/loss\", val_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"val/acc\", val_acc, epoch_idx+1)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, Fr features\n",
    "fr_feats = []\n",
    "model.eval()\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    fr_feat = model.fr_feat(X).cpu().detach()\n",
    "    fr_feats.append(fr_feat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Freeze Model & FineTurne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "freezed_params = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param_name not in ['classifier.2.weight', 'classifier.2.bias']:\n",
    "        freezed_params.append(param)\n",
    "        \n",
    "for param in freezed_params:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #0, train loss: 0.0219, train acc: 0.9818\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #1, train loss: 0.0217, train acc: 0.9818\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #2, train loss: 0.0214, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #3, train loss: 0.0210, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #4, train loss: 0.0206, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #5, train loss: 0.0202, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #6, train loss: 0.0198, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #7, train loss: 0.0194, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #8, train loss: 0.0190, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #9, train loss: 0.0186, train acc: 0.9848\n"
     ]
    }
   ],
   "source": [
    "# =============== fine_tune code #===============\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "for epoch_idx in range(0, 10):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(held_out_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    #         if (batch_idx+1) % 1000 == 0:\n",
    "    #             print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "    #                   .format(batch_idx+1, len(si_loader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "\n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73438e0851f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#=============== dev_val code #===============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#=============== dev_val code #===============\n",
    "model.eval()\n",
    "loss_sum = 0\n",
    "n_corrects = 0\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    logit = model(X)\n",
    "    loss = criterion(logit, y)\n",
    "    loss_sum += loss.item()\n",
    "    n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "    total += y.size(0)\n",
    "val_loss = loss_sum / total\n",
    "val_acc = n_corrects / total\n",
    "print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "global_mean = si_embeds.mean(0)\n",
    "clf = LDA(solver='svd', n_components=200)\n",
    "clf.fit(si_embeds - global_mean, si_key_df.label)\n",
    "\n",
    "si_embeds = clf.transform(si_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "sv_embeds = clf.transform(sv_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "si_dataset, embed_dim, n_labels = embedToDataset(si_embeds.reshape(-1,200), si_key_df)\n",
    "sv_dataset, _, _ = embedToDataset(sv_embeds, sv_key_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
