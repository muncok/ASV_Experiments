{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden1_size = 1024\n",
    "hidden2_size = 1024\n",
    "hidden3_size = 1024\n",
    "output_size = 10\n",
    "\n",
    "initial_enable = True\n",
    "\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "num_epochs = 200\n",
    "batch_size_for_train = 128\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='../dataset',\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.1307,),(0.3081,))])\n",
    "                           )\n",
    "\n",
    "test_dataset = dsets.MNIST(root='../dataset',\n",
    "                           train=False,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                          transforms.Normalize((0.1307,),(0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SPLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_dataset, initial_prob):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.initial_prob = initial_prob\n",
    "        self.distrib = torch.distributions.bernoulli.Bernoulli(probs=self.initial_prob)\n",
    "        self.mask_tensor = self.distrib.sample(sample_shape=torch.Size([self.__len__()]))\n",
    "#        self.mask_tensor.requires_grad_(True)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data, target = self.original_dataset[index]\n",
    "        not_masked = self.mask_tensor[index]\n",
    "        \n",
    "        return data, target, not_masked\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.original_dataset.__len__()\n",
    "    \n",
    "    def m_len(self):\n",
    "        return self.mask_tensor.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset_spl = SPLDataset(train_dataset, initial_prob=0.01)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_spl,\n",
    "                                           batch_size=batch_size_for_train,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "# https://github.com/ufoym/imbalanced-dataset-sampler\n",
    "sampler = torch.utils.data.RandomSampler(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class splMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(splMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden1_size)\n",
    "        self.relu1 = nn.ReLU(hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden2_size)\n",
    "        self.relu2 = nn.ReLU(hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, hidden3_size)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden3_size)\n",
    "        self.relu3 = nn.ReLU(hidden3_size)\n",
    "        self.fc_out = nn.Linear(hidden3_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splMLP(\n",
       "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU(inplace)\n",
       "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU(inplace)\n",
       "  (fc3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU(inplace)\n",
       "  (fc_out): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = splMLP()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_entropy(logits):\n",
    "    softmax_prob = nn.Softmax(dim=1)\n",
    "    probs = softmax_prob(logits)\n",
    "    log_probs = torch.log2(probs)\n",
    "    sha_entropy = torch.mul(-probs, log_probs).sum(dim=1)\n",
    "    \n",
    "    avg_sha_entropy = sha_entropy.mean(dim=0)\n",
    "    var_sha_entropy = sha_entropy.var(dim=0)\n",
    "    \n",
    "    return avg_sha_entropy, var_sha_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_save(model):\n",
    "    model_saved_dir = './saved_models/'\n",
    "    model_name = model.__class__.__name__\n",
    "    model_path = model_saved_dir + model_name + '.pkl'\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(\"Best model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labels_distrib = {str(c):[] for c in range(10)}\n",
    "labels_distrib = dict(zip([str(c) for c in range(10)] ,[[0 for _ in range(num_epochs)] for _ in range(10)]))\n",
    "\n",
    "def get_labels_distrib(mask_tensor, labels, epoch):\n",
    "    mask_tensor_temp = mask_tensor.to(device=device, dtype=torch.uint8)\n",
    "    selected_labels = labels[mask_tensor_temp].data.cpu().numpy()\n",
    "\n",
    "    for x in selected_labels:\n",
    "        labels_distrib[str(x)][epoch] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global k\n",
    "    net.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    output_entropy = 0\n",
    "    mask_k = 1. / k\n",
    "    datalen = 0\n",
    "    \n",
    "    grad_abs = 0\n",
    "    loss_abs = 0\n",
    "    \n",
    "    for i, (inputs, labels, mask_tensor) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,784)\n",
    "        inputs = inputs.to(device=device)\n",
    "        labels = labels.to(device=device)\n",
    "        mask_tensor = mask_tensor.to(device=device)\n",
    "        \n",
    "        logits = net(inputs) # logits = outputs\n",
    "        expected_logits, expected_labels = torch.max(logits, 1) # argmax(softmax) = expected\n",
    "        avg_outputs_entropy, var_outputs_entropy = cal_entropy(logits)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (expected_labels==labels).sum()\n",
    "        \n",
    "        original_loss = criterion(logits, labels)\n",
    "        if (epoch+1) == 1 and initial_enable:\n",
    "            pass\n",
    "        else:\n",
    "            mask_tensor_update = (original_loss > mask_k).to(device=device, dtype=torch.float32)\n",
    "            mask_tensor = mask_tensor_update\n",
    "        mask_loss = (mask_tensor.detach()*original_loss).mean(dim=0)\n",
    "        get_labels_distrib(mask_tensor, labels, epoch)\n",
    "        reg_loss = (-mask_k*mask_tensor).mean(dim=0)\n",
    "        spl_loss = mask_loss + reg_loss\n",
    "        total_loss += mask_loss.data\n",
    "        datalen += mask_tensor.sum(dim=0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        spl_loss.backward()\n",
    "#        grad_abs += logits.grad.detach().abs().sum()\n",
    "#        loss_abs += mask_loss.detach().abs().sum()\n",
    "#        print(grad_abs, loss_abs, net.fc1.weight.grad.detach().abs().sum())\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"<Normal Phase> Epoch [%d/%d], Step [%d/%d], Loss: %.8f\"\n",
    "                  %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, mask_loss.data))\n",
    "            \n",
    "    correct = torch.tensor(correct, dtype=torch.float32, device=device)\n",
    "    total = torch.tensor(total, dtype=torch.float32, device=device)\n",
    "    accuracy = 100.*correct/total\n",
    "\n",
    "    print(\"<Normal Phase> totally train at Epoch [%d/%d], Loss: %.6f, Train Acc.: %.4f\" \n",
    "          %(epoch+1, num_epochs, total_loss.data, accuracy))\n",
    "    print(\"**datalen=%d, k=%.6f\" %(datalen, k))\n",
    "    print(\"**datalen=%d\" %(train_dataset_spl.m_len()))\n",
    "    print(\"grad_abs, loss_abs, del_w_abs:\")\n",
    "    \n",
    "    \n",
    "    k = k * 1.05\n",
    "    \n",
    "    return accuracy.item(), total_loss, (avg_outputs_entropy.item(), var_outputs_entropy.item()), datalen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.view(-1,784)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = net(inputs)\n",
    "        expected_logits, expected_labels = torch.max(logits, 1)\n",
    "        avg_outputs_entropy, var_outputs_entropy = cal_entropy(logits)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (expected_labels == labels).sum()\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss = loss.mean(dim=0)\n",
    "        total_loss += loss.data\n",
    "\n",
    "    correct = torch.tensor(correct, dtype=torch.float32, device=device)\n",
    "    total = torch.tensor(total, dtype=torch.float32, device=device)\n",
    "    accuracy = 100.*correct/total\n",
    "    \n",
    "    print(\"<Test Phase> Accuacy of the network on the 10,000 test images:%.4f %%\" % (accuracy))\n",
    "        \n",
    "    return accuracy.item(), total_loss, (avg_outputs_entropy.item(), var_outputs_entropy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaeunl/anaconda3/envs/tensor/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_trace = dict()\n",
    "\n",
    "train_trace = dict()\n",
    "train_trace['loss']=[]\n",
    "train_trace['acc']=[]\n",
    "train_trace['logit']=[]\n",
    "train_trace['output_entropy']=dict()\n",
    "train_trace['output_entropy']['mean']=[]\n",
    "train_trace['output_entropy']['var']=[]\n",
    "train_trace['datalen_total']=[]\n",
    "train_trace['datalen_class']=labels_distrib\n",
    "\n",
    "validate_trace = dict()\n",
    "validate_trace['datalen']=[]\n",
    "\n",
    "test_trace = dict()\n",
    "test_trace['loss']=[]\n",
    "test_trace['acc']=[]\n",
    "test_trace['logit']=[]\n",
    "test_trace['output_entropy']=dict()\n",
    "test_trace['output_entropy']['mean']=[]\n",
    "test_trace['output_entropy']['var']=[]\n",
    "\n",
    "file_trace['train']=train_trace\n",
    "file_trace['validate']=validate_trace\n",
    "file_trace['test']=test_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Normal Phase> Epoch [1/200], Step [100/468], Loss: 0.00382417\n",
      "<Normal Phase> Epoch [1/200], Step [200/468], Loss: 0.00212226\n",
      "<Normal Phase> Epoch [1/200], Step [300/468], Loss: 0.00685871\n",
      "<Normal Phase> Epoch [1/200], Step [400/468], Loss: 0.00000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaeunl/anaconda3/envs/tensor/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Normal Phase> totally train at Epoch [1/200], Loss: 4.609850, Train Acc.: 75.1667\n",
      "**datalen=620, k=1.000000\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaeunl/anaconda3/envs/tensor/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Test Phase> Accuacy of the network on the 10,000 test images:87.7100 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [2/200], Step [100/468], Loss: 0.08374874\n",
      "<Normal Phase> Epoch [2/200], Step [200/468], Loss: 0.15255174\n",
      "<Normal Phase> Epoch [2/200], Step [300/468], Loss: 0.10310934\n",
      "<Normal Phase> Epoch [2/200], Step [400/468], Loss: 0.08227003\n",
      "<Normal Phase> totally train at Epoch [2/200], Loss: 47.950111, Train Acc.: 95.0067\n",
      "**datalen=4155, k=1.050000\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:96.7200 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [3/200], Step [100/468], Loss: 0.02379362\n",
      "<Normal Phase> Epoch [3/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [3/200], Step [300/468], Loss: 0.01671714\n",
      "<Normal Phase> Epoch [3/200], Step [400/468], Loss: 0.04439760\n",
      "<Normal Phase> totally train at Epoch [3/200], Loss: 17.698580, Train Acc.: 97.7900\n",
      "**datalen=1802, k=1.102500\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:97.4000 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [4/200], Step [100/468], Loss: 0.01481911\n",
      "<Normal Phase> Epoch [4/200], Step [200/468], Loss: 0.03242037\n",
      "<Normal Phase> Epoch [4/200], Step [300/468], Loss: 0.03084029\n",
      "<Normal Phase> Epoch [4/200], Step [400/468], Loss: 0.04734178\n",
      "<Normal Phase> totally train at Epoch [4/200], Loss: 11.024251, Train Acc.: 98.5883\n",
      "**datalen=1244, k=1.157625\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:97.7000 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [5/200], Step [100/468], Loss: 0.01685298\n",
      "<Normal Phase> Epoch [5/200], Step [200/468], Loss: 0.00689354\n",
      "<Normal Phase> Epoch [5/200], Step [300/468], Loss: 0.00700026\n",
      "<Normal Phase> Epoch [5/200], Step [400/468], Loss: 0.01414079\n",
      "<Normal Phase> totally train at Epoch [5/200], Loss: 8.557416, Train Acc.: 98.9200\n",
      "**datalen=1025, k=1.215506\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:97.8300 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [6/200], Step [100/468], Loss: 0.00653397\n",
      "<Normal Phase> Epoch [6/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [6/200], Step [300/468], Loss: 0.02644006\n",
      "<Normal Phase> Epoch [6/200], Step [400/468], Loss: 0.01523128\n",
      "<Normal Phase> totally train at Epoch [6/200], Loss: 7.199365, Train Acc.: 99.0717\n",
      "**datalen=913, k=1.276282\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:97.6300 %\n",
      "<Normal Phase> Epoch [7/200], Step [100/468], Loss: 0.01177685\n",
      "<Normal Phase> Epoch [7/200], Step [200/468], Loss: 0.00751992\n",
      "<Normal Phase> Epoch [7/200], Step [300/468], Loss: 0.01315280\n",
      "<Normal Phase> Epoch [7/200], Step [400/468], Loss: 0.02471455\n",
      "<Normal Phase> totally train at Epoch [7/200], Loss: 6.287140, Train Acc.: 99.2167\n",
      "**datalen=834, k=1.340096\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:97.9800 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [8/200], Step [100/468], Loss: 0.01288518\n",
      "<Normal Phase> Epoch [8/200], Step [200/468], Loss: 0.01231151\n",
      "<Normal Phase> Epoch [8/200], Step [300/468], Loss: 0.02092685\n",
      "<Normal Phase> Epoch [8/200], Step [400/468], Loss: 0.00559796\n",
      "<Normal Phase> totally train at Epoch [8/200], Loss: 5.435708, Train Acc.: 99.3500\n",
      "**datalen=748, k=1.407100\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:97.7700 %\n",
      "<Normal Phase> Epoch [9/200], Step [100/468], Loss: 0.01770143\n",
      "<Normal Phase> Epoch [9/200], Step [200/468], Loss: 0.01492929\n",
      "<Normal Phase> Epoch [9/200], Step [300/468], Loss: 0.00756209\n",
      "<Normal Phase> Epoch [9/200], Step [400/468], Loss: 0.00672202\n",
      "<Normal Phase> totally train at Epoch [9/200], Loss: 5.858735, Train Acc.: 99.3683\n",
      "**datalen=824, k=1.477455\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:97.9800 %\n",
      "<Normal Phase> Epoch [10/200], Step [100/468], Loss: 0.01997712\n",
      "<Normal Phase> Epoch [10/200], Step [200/468], Loss: 0.01093085\n",
      "<Normal Phase> Epoch [10/200], Step [300/468], Loss: 0.00655495\n",
      "<Normal Phase> Epoch [10/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [10/200], Loss: 4.817940, Train Acc.: 99.5050\n",
      "**datalen=715, k=1.551328\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.1400 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [11/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [11/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [11/200], Step [300/468], Loss: 0.01088384\n",
      "<Normal Phase> Epoch [11/200], Step [400/468], Loss: 0.01554060\n",
      "<Normal Phase> totally train at Epoch [11/200], Loss: 4.118031, Train Acc.: 99.6000\n",
      "**datalen=635, k=1.628895\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.1800 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [12/200], Step [100/468], Loss: 0.00541705\n",
      "<Normal Phase> Epoch [12/200], Step [200/468], Loss: 0.02390611\n",
      "<Normal Phase> Epoch [12/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [12/200], Step [400/468], Loss: 0.01329299\n",
      "<Normal Phase> totally train at Epoch [12/200], Loss: 4.345391, Train Acc.: 99.6017\n",
      "**datalen=681, k=1.710339\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.1300 %\n",
      "<Normal Phase> Epoch [13/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [13/200], Step [200/468], Loss: 0.01895509\n",
      "<Normal Phase> Epoch [13/200], Step [300/468], Loss: 0.01592908\n",
      "<Normal Phase> Epoch [13/200], Step [400/468], Loss: 0.02365076\n",
      "<Normal Phase> totally train at Epoch [13/200], Loss: 4.118188, Train Acc.: 99.6700\n",
      "**datalen=668, k=1.795856\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.0400 %\n",
      "<Normal Phase> Epoch [14/200], Step [100/468], Loss: 0.01107316\n",
      "<Normal Phase> Epoch [14/200], Step [200/468], Loss: 0.00477836\n",
      "<Normal Phase> Epoch [14/200], Step [300/468], Loss: 0.00418635\n",
      "<Normal Phase> Epoch [14/200], Step [400/468], Loss: 0.02022273\n",
      "<Normal Phase> totally train at Epoch [14/200], Loss: 3.574470, Train Acc.: 99.6933\n",
      "**datalen=599, k=1.885649\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.0300 %\n",
      "<Normal Phase> Epoch [15/200], Step [100/468], Loss: 0.00894994\n",
      "<Normal Phase> Epoch [15/200], Step [200/468], Loss: 0.00715127\n",
      "<Normal Phase> Epoch [15/200], Step [300/468], Loss: 0.00445836\n",
      "<Normal Phase> Epoch [15/200], Step [400/468], Loss: 0.01551089\n",
      "<Normal Phase> totally train at Epoch [15/200], Loss: 3.384568, Train Acc.: 99.7067\n",
      "**datalen=584, k=1.979932\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:97.8900 %\n",
      "<Normal Phase> Epoch [16/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [16/200], Step [200/468], Loss: 0.01128594\n",
      "<Normal Phase> Epoch [16/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [16/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [16/200], Loss: 2.743001, Train Acc.: 99.8183\n",
      "**datalen=505, k=2.078928\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2500 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [17/200], Step [100/468], Loss: 0.00416862\n",
      "<Normal Phase> Epoch [17/200], Step [200/468], Loss: 0.01713235\n",
      "<Normal Phase> Epoch [17/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [17/200], Step [400/468], Loss: 0.00748162\n",
      "<Normal Phase> totally train at Epoch [17/200], Loss: 2.818656, Train Acc.: 99.7767\n",
      "**datalen=522, k=2.182875\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.1400 %\n",
      "<Normal Phase> Epoch [18/200], Step [100/468], Loss: 0.00487672\n",
      "<Normal Phase> Epoch [18/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [18/200], Step [300/468], Loss: 0.00343727\n",
      "<Normal Phase> Epoch [18/200], Step [400/468], Loss: 0.00741040\n",
      "<Normal Phase> totally train at Epoch [18/200], Loss: 3.097808, Train Acc.: 99.7717\n",
      "**datalen=576, k=2.292018\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3000 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [19/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [19/200], Step [200/468], Loss: 0.00609736\n",
      "<Normal Phase> Epoch [19/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [19/200], Step [400/468], Loss: 0.00931968\n",
      "<Normal Phase> totally train at Epoch [19/200], Loss: 2.754498, Train Acc.: 99.8017\n",
      "**datalen=531, k=2.406619\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2600 %\n",
      "<Normal Phase> Epoch [20/200], Step [100/468], Loss: 0.00883254\n",
      "<Normal Phase> Epoch [20/200], Step [200/468], Loss: 0.00322654\n",
      "<Normal Phase> Epoch [20/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [20/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [20/200], Loss: 2.073863, Train Acc.: 99.8733\n",
      "**datalen=437, k=2.526950\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2100 %\n",
      "<Normal Phase> Epoch [21/200], Step [100/468], Loss: 0.00469301\n",
      "<Normal Phase> Epoch [21/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [21/200], Step [300/468], Loss: 0.00583822\n",
      "<Normal Phase> Epoch [21/200], Step [400/468], Loss: 0.02161944\n",
      "<Normal Phase> totally train at Epoch [21/200], Loss: 2.526721, Train Acc.: 99.8117\n",
      "**datalen=516, k=2.653298\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2500 %\n",
      "<Normal Phase> Epoch [22/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [22/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [22/200], Step [300/468], Loss: 0.00567858\n",
      "<Normal Phase> Epoch [22/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [22/200], Loss: 2.470166, Train Acc.: 99.8367\n",
      "**datalen=512, k=2.785963\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3400 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [23/200], Step [100/468], Loss: 0.01500988\n",
      "<Normal Phase> Epoch [23/200], Step [200/468], Loss: 0.00371263\n",
      "<Normal Phase> Epoch [23/200], Step [300/468], Loss: 0.00294136\n",
      "<Normal Phase> Epoch [23/200], Step [400/468], Loss: 0.00339048\n",
      "<Normal Phase> totally train at Epoch [23/200], Loss: 2.095169, Train Acc.: 99.8617\n",
      "**datalen=459, k=2.925261\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.0300 %\n",
      "<Normal Phase> Epoch [24/200], Step [100/468], Loss: 0.00364807\n",
      "<Normal Phase> Epoch [24/200], Step [200/468], Loss: 0.00304260\n",
      "<Normal Phase> Epoch [24/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [24/200], Step [400/468], Loss: 0.00792052\n",
      "<Normal Phase> totally train at Epoch [24/200], Loss: 2.091749, Train Acc.: 99.8883\n",
      "**datalen=474, k=3.071524\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3000 %\n",
      "<Normal Phase> Epoch [25/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [25/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [25/200], Step [300/468], Loss: 0.00337803\n",
      "<Normal Phase> Epoch [25/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [25/200], Loss: 1.543439, Train Acc.: 99.9317\n",
      "**datalen=377, k=3.225100\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2400 %\n",
      "<Normal Phase> Epoch [26/200], Step [100/468], Loss: 0.00411158\n",
      "<Normal Phase> Epoch [26/200], Step [200/468], Loss: 0.00814060\n",
      "<Normal Phase> Epoch [26/200], Step [300/468], Loss: 0.00317659\n",
      "<Normal Phase> Epoch [26/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [26/200], Loss: 2.016740, Train Acc.: 99.8633\n",
      "**datalen=466, k=3.386355\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3400 %\n",
      "<Normal Phase> Epoch [27/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [27/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [27/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [27/200], Step [400/468], Loss: 0.00313200\n",
      "<Normal Phase> totally train at Epoch [27/200], Loss: 1.632802, Train Acc.: 99.9183\n",
      "**datalen=410, k=3.555673\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3600 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [28/200], Step [100/468], Loss: 0.00222481\n",
      "<Normal Phase> Epoch [28/200], Step [200/468], Loss: 0.00315143\n",
      "<Normal Phase> Epoch [28/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [28/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [28/200], Loss: 1.128498, Train Acc.: 99.9417\n",
      "**datalen=313, k=3.733456\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4800 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [29/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [29/200], Step [200/468], Loss: 0.00317222\n",
      "<Normal Phase> Epoch [29/200], Step [300/468], Loss: 0.00388649\n",
      "<Normal Phase> Epoch [29/200], Step [400/468], Loss: 0.00828323\n",
      "<Normal Phase> totally train at Epoch [29/200], Loss: 1.474521, Train Acc.: 99.9183\n",
      "**datalen=386, k=3.920129\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2900 %\n",
      "<Normal Phase> Epoch [30/200], Step [100/468], Loss: 0.00462527\n",
      "<Normal Phase> Epoch [30/200], Step [200/468], Loss: 0.01035846\n",
      "<Normal Phase> Epoch [30/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [30/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [30/200], Loss: 1.830203, Train Acc.: 99.8967\n",
      "**datalen=454, k=4.116136\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [31/200], Step [100/468], Loss: 0.01232470\n",
      "<Normal Phase> Epoch [31/200], Step [200/468], Loss: 0.00513156\n",
      "<Normal Phase> Epoch [31/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [31/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [31/200], Loss: 1.605588, Train Acc.: 99.9083\n",
      "**datalen=416, k=4.321942\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4900 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [32/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [32/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [32/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [32/200], Step [400/468], Loss: 0.00189960\n",
      "<Normal Phase> totally train at Epoch [32/200], Loss: 0.867194, Train Acc.: 99.9583\n",
      "**datalen=278, k=4.538039\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4400 %\n",
      "<Normal Phase> Epoch [33/200], Step [100/468], Loss: 0.00314569\n",
      "<Normal Phase> Epoch [33/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [33/200], Step [300/468], Loss: 0.01388317\n",
      "<Normal Phase> Epoch [33/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [33/200], Loss: 1.533961, Train Acc.: 99.8867\n",
      "**datalen=389, k=4.764941\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4500 %\n",
      "<Normal Phase> Epoch [34/200], Step [100/468], Loss: 0.00486512\n",
      "<Normal Phase> Epoch [34/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [34/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [34/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [34/200], Loss: 1.094189, Train Acc.: 99.9367\n",
      "**datalen=336, k=5.003189\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3800 %\n",
      "<Normal Phase> Epoch [35/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [35/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [35/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [35/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [35/200], Loss: 1.176511, Train Acc.: 99.9433\n",
      "**datalen=363, k=5.253348\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4200 %\n",
      "<Normal Phase> Epoch [36/200], Step [100/468], Loss: 0.00254482\n",
      "<Normal Phase> Epoch [36/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [36/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [36/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [36/200], Loss: 1.021398, Train Acc.: 99.9533\n",
      "**datalen=335, k=5.516015\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5900 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [37/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [37/200], Step [200/468], Loss: 0.00136835\n",
      "<Normal Phase> Epoch [37/200], Step [300/468], Loss: 0.00283182\n",
      "<Normal Phase> Epoch [37/200], Step [400/468], Loss: 0.00338840\n",
      "<Normal Phase> totally train at Epoch [37/200], Loss: 1.000188, Train Acc.: 99.9500\n",
      "**datalen=330, k=5.791816\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4400 %\n",
      "<Normal Phase> Epoch [38/200], Step [100/468], Loss: 0.00302521\n",
      "<Normal Phase> Epoch [38/200], Step [200/468], Loss: 0.00208127\n",
      "<Normal Phase> Epoch [38/200], Step [300/468], Loss: 0.00986121\n",
      "<Normal Phase> Epoch [38/200], Step [400/468], Loss: 0.00569046\n",
      "<Normal Phase> totally train at Epoch [38/200], Loss: 1.323730, Train Acc.: 99.9183\n",
      "**datalen=402, k=6.081407\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3500 %\n",
      "<Normal Phase> Epoch [39/200], Step [100/468], Loss: 0.00136157\n",
      "<Normal Phase> Epoch [39/200], Step [200/468], Loss: 0.00175376\n",
      "<Normal Phase> Epoch [39/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [39/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [39/200], Loss: 1.310836, Train Acc.: 99.9100\n",
      "**datalen=403, k=6.385477\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4200 %\n",
      "<Normal Phase> Epoch [40/200], Step [100/468], Loss: 0.00132425\n",
      "<Normal Phase> Epoch [40/200], Step [200/468], Loss: 0.01773220\n",
      "<Normal Phase> Epoch [40/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [40/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [40/200], Loss: 1.003731, Train Acc.: 99.9417\n",
      "**datalen=336, k=6.704751\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4700 %\n",
      "<Normal Phase> Epoch [41/200], Step [100/468], Loss: 0.00118834\n",
      "<Normal Phase> Epoch [41/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [41/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [41/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [41/200], Loss: 0.791486, Train Acc.: 99.9567\n",
      "**datalen=288, k=7.039989\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4100 %\n",
      "<Normal Phase> Epoch [42/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [42/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [42/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [42/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [42/200], Loss: 0.814465, Train Acc.: 99.9517\n",
      "**datalen=315, k=7.391988\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4100 %\n",
      "<Normal Phase> Epoch [43/200], Step [100/468], Loss: 0.00231911\n",
      "<Normal Phase> Epoch [43/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [43/200], Step [300/468], Loss: 0.00317209\n",
      "<Normal Phase> Epoch [43/200], Step [400/468], Loss: 0.00101855\n",
      "<Normal Phase> totally train at Epoch [43/200], Loss: 1.013692, Train Acc.: 99.9450\n",
      "**datalen=325, k=7.761588\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5700 %\n",
      "<Normal Phase> Epoch [44/200], Step [100/468], Loss: 0.00178988\n",
      "<Normal Phase> Epoch [44/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [44/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [44/200], Step [400/468], Loss: 0.00149511\n",
      "<Normal Phase> totally train at Epoch [44/200], Loss: 0.794237, Train Acc.: 99.9683\n",
      "**datalen=346, k=8.149667\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [45/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [45/200], Step [200/468], Loss: 0.00160822\n",
      "<Normal Phase> Epoch [45/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [45/200], Step [400/468], Loss: 0.00247440\n",
      "<Normal Phase> totally train at Epoch [45/200], Loss: 0.564185, Train Acc.: 99.9700\n",
      "**datalen=253, k=8.557150\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [46/200], Step [100/468], Loss: 0.00231809\n",
      "<Normal Phase> Epoch [46/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [46/200], Step [300/468], Loss: 0.00177623\n",
      "<Normal Phase> Epoch [46/200], Step [400/468], Loss: 0.00159155\n",
      "<Normal Phase> totally train at Epoch [46/200], Loss: 0.746196, Train Acc.: 99.9650\n",
      "**datalen=295, k=8.985008\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [47/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [47/200], Step [200/468], Loss: 0.02852515\n",
      "<Normal Phase> Epoch [47/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [47/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [47/200], Loss: 1.107842, Train Acc.: 99.9300\n",
      "**datalen=377, k=9.434258\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4400 %\n",
      "<Normal Phase> Epoch [48/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [48/200], Step [200/468], Loss: 0.00120123\n",
      "<Normal Phase> Epoch [48/200], Step [300/468], Loss: 0.00084534\n",
      "<Normal Phase> Epoch [48/200], Step [400/468], Loss: 0.01189718\n",
      "<Normal Phase> totally train at Epoch [48/200], Loss: 0.897960, Train Acc.: 99.9450\n",
      "**datalen=350, k=9.905971\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4700 %\n",
      "<Normal Phase> Epoch [49/200], Step [100/468], Loss: 0.00384663\n",
      "<Normal Phase> Epoch [49/200], Step [200/468], Loss: 0.00424221\n",
      "<Normal Phase> Epoch [49/200], Step [300/468], Loss: 0.01528087\n",
      "<Normal Phase> Epoch [49/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [49/200], Loss: 0.702907, Train Acc.: 99.9567\n",
      "**datalen=290, k=10.401270\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5400 %\n",
      "<Normal Phase> Epoch [50/200], Step [100/468], Loss: 0.00110516\n",
      "<Normal Phase> Epoch [50/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [50/200], Step [300/468], Loss: 0.00153406\n",
      "<Normal Phase> Epoch [50/200], Step [400/468], Loss: 0.00531031\n",
      "<Normal Phase> totally train at Epoch [50/200], Loss: 0.519251, Train Acc.: 99.9817\n",
      "**datalen=257, k=10.921333\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4600 %\n",
      "<Normal Phase> Epoch [51/200], Step [100/468], Loss: 0.00567342\n",
      "<Normal Phase> Epoch [51/200], Step [200/468], Loss: 0.00115245\n",
      "<Normal Phase> Epoch [51/200], Step [300/468], Loss: 0.00206128\n",
      "<Normal Phase> Epoch [51/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [51/200], Loss: 0.590076, Train Acc.: 99.9633\n",
      "**datalen=272, k=11.467400\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5100 %\n",
      "<Normal Phase> Epoch [52/200], Step [100/468], Loss: 0.00073696\n",
      "<Normal Phase> Epoch [52/200], Step [200/468], Loss: 0.00258446\n",
      "<Normal Phase> Epoch [52/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [52/200], Step [400/468], Loss: 0.00169773\n",
      "<Normal Phase> totally train at Epoch [52/200], Loss: 0.697779, Train Acc.: 99.9650\n",
      "**datalen=320, k=12.040770\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3900 %\n",
      "<Normal Phase> Epoch [53/200], Step [100/468], Loss: 0.00094021\n",
      "<Normal Phase> Epoch [53/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [53/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [53/200], Step [400/468], Loss: 0.00261229\n",
      "<Normal Phase> totally train at Epoch [53/200], Loss: 1.103153, Train Acc.: 99.9217\n",
      "**datalen=406, k=12.642808\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5900 %\n",
      "<Normal Phase> Epoch [54/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [54/200], Step [200/468], Loss: 0.00137895\n",
      "<Normal Phase> Epoch [54/200], Step [300/468], Loss: 0.00105311\n",
      "<Normal Phase> Epoch [54/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [54/200], Loss: 0.715093, Train Acc.: 99.9567\n",
      "**datalen=333, k=13.274949\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3900 %\n",
      "<Normal Phase> Epoch [55/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [55/200], Step [200/468], Loss: 0.00099500\n",
      "<Normal Phase> Epoch [55/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [55/200], Step [400/468], Loss: 0.00086259\n",
      "<Normal Phase> totally train at Epoch [55/200], Loss: 0.547897, Train Acc.: 99.9650\n",
      "**datalen=271, k=13.938696\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5200 %\n",
      "<Normal Phase> Epoch [56/200], Step [100/468], Loss: 0.00645317\n",
      "<Normal Phase> Epoch [56/200], Step [200/468], Loss: 0.00058684\n",
      "<Normal Phase> Epoch [56/200], Step [300/468], Loss: 0.00218273\n",
      "<Normal Phase> Epoch [56/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [56/200], Loss: 0.467748, Train Acc.: 99.9783\n",
      "**datalen=238, k=14.635631\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5300 %\n",
      "<Normal Phase> Epoch [57/200], Step [100/468], Loss: 0.00052521\n",
      "<Normal Phase> Epoch [57/200], Step [200/468], Loss: 0.00052317\n",
      "<Normal Phase> Epoch [57/200], Step [300/468], Loss: 0.00107508\n",
      "<Normal Phase> Epoch [57/200], Step [400/468], Loss: 0.00165344\n",
      "<Normal Phase> totally train at Epoch [57/200], Loss: 0.492297, Train Acc.: 99.9800\n",
      "**datalen=242, k=15.367412\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4000 %\n",
      "<Normal Phase> Epoch [58/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [58/200], Step [200/468], Loss: 0.00121196\n",
      "<Normal Phase> Epoch [58/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [58/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [58/200], Loss: 0.268330, Train Acc.: 99.9883\n",
      "**datalen=204, k=16.135783\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4900 %\n",
      "<Normal Phase> Epoch [59/200], Step [100/468], Loss: 0.00334812\n",
      "<Normal Phase> Epoch [59/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [59/200], Step [300/468], Loss: 0.00653870\n",
      "<Normal Phase> Epoch [59/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [59/200], Loss: 0.954266, Train Acc.: 99.9450\n",
      "**datalen=371, k=16.942572\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4600 %\n",
      "<Normal Phase> Epoch [60/200], Step [100/468], Loss: 0.00046568\n",
      "<Normal Phase> Epoch [60/200], Step [200/468], Loss: 0.00099543\n",
      "<Normal Phase> Epoch [60/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [60/200], Step [400/468], Loss: 0.00078879\n",
      "<Normal Phase> totally train at Epoch [60/200], Loss: 0.883740, Train Acc.: 99.9500\n",
      "**datalen=402, k=17.789701\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4500 %\n",
      "<Normal Phase> Epoch [61/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [61/200], Step [200/468], Loss: 0.00048678\n",
      "<Normal Phase> Epoch [61/200], Step [300/468], Loss: 0.00203905\n",
      "<Normal Phase> Epoch [61/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [61/200], Loss: 0.504120, Train Acc.: 99.9733\n",
      "**datalen=325, k=18.679186\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4100 %\n",
      "<Normal Phase> Epoch [62/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [62/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [62/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [62/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [62/200], Loss: 0.458834, Train Acc.: 99.9717\n",
      "**datalen=279, k=19.613145\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3900 %\n",
      "<Normal Phase> Epoch [63/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [63/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [63/200], Step [300/468], Loss: 0.03392620\n",
      "<Normal Phase> Epoch [63/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [63/200], Loss: 0.324109, Train Acc.: 99.9817\n",
      "**datalen=205, k=20.593802\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4900 %\n",
      "<Normal Phase> Epoch [64/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [64/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [64/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [64/200], Step [400/468], Loss: 0.01092668\n",
      "<Normal Phase> totally train at Epoch [64/200], Loss: 0.638780, Train Acc.: 99.9500\n",
      "**datalen=331, k=21.623493\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4400 %\n",
      "<Normal Phase> Epoch [65/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [65/200], Step [200/468], Loss: 0.00725307\n",
      "<Normal Phase> Epoch [65/200], Step [300/468], Loss: 0.00045294\n",
      "<Normal Phase> Epoch [65/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [65/200], Loss: 0.752876, Train Acc.: 99.9533\n",
      "**datalen=359, k=22.704667\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5200 %\n",
      "<Normal Phase> Epoch [66/200], Step [100/468], Loss: 0.00617544\n",
      "<Normal Phase> Epoch [66/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [66/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [66/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [66/200], Loss: 0.335984, Train Acc.: 99.9850\n",
      "**datalen=227, k=23.839901\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6000 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [67/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [67/200], Step [200/468], Loss: 0.00044441\n",
      "<Normal Phase> Epoch [67/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [67/200], Step [400/468], Loss: 0.00165301\n",
      "<Normal Phase> totally train at Epoch [67/200], Loss: 0.517212, Train Acc.: 99.9700\n",
      "**datalen=295, k=25.031896\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2600 %\n",
      "<Normal Phase> Epoch [68/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [68/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [68/200], Step [300/468], Loss: 0.00149867\n",
      "<Normal Phase> Epoch [68/200], Step [400/468], Loss: 0.00113505\n",
      "<Normal Phase> totally train at Epoch [68/200], Loss: 0.625528, Train Acc.: 99.9600\n",
      "**datalen=349, k=26.283490\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [69/200], Step [100/468], Loss: 0.00092879\n",
      "<Normal Phase> Epoch [69/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [69/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [69/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [69/200], Loss: 0.385688, Train Acc.: 99.9783\n",
      "**datalen=265, k=27.597665\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4600 %\n",
      "<Normal Phase> Epoch [70/200], Step [100/468], Loss: 0.00055353\n",
      "<Normal Phase> Epoch [70/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [70/200], Step [300/468], Loss: 0.00433372\n",
      "<Normal Phase> Epoch [70/200], Step [400/468], Loss: 0.00029643\n",
      "<Normal Phase> totally train at Epoch [70/200], Loss: 0.670298, Train Acc.: 99.9567\n",
      "**datalen=341, k=28.977548\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4200 %\n",
      "<Normal Phase> Epoch [71/200], Step [100/468], Loss: 0.00028541\n",
      "<Normal Phase> Epoch [71/200], Step [200/468], Loss: 0.00125081\n",
      "<Normal Phase> Epoch [71/200], Step [300/468], Loss: 0.00066601\n",
      "<Normal Phase> Epoch [71/200], Step [400/468], Loss: 0.00075409\n",
      "<Normal Phase> totally train at Epoch [71/200], Loss: 0.564705, Train Acc.: 99.9650\n",
      "**datalen=393, k=30.426426\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4700 %\n",
      "<Normal Phase> Epoch [72/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [72/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [72/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [72/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [72/200], Loss: 0.210496, Train Acc.: 99.9917\n",
      "**datalen=212, k=31.947747\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6400 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [73/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [73/200], Step [200/468], Loss: 0.00028613\n",
      "<Normal Phase> Epoch [73/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [73/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [73/200], Loss: 0.250238, Train Acc.: 99.9850\n",
      "**datalen=225, k=33.545134\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6200 %\n",
      "<Normal Phase> Epoch [74/200], Step [100/468], Loss: 0.00203988\n",
      "<Normal Phase> Epoch [74/200], Step [200/468], Loss: 0.00254843\n",
      "<Normal Phase> Epoch [74/200], Step [300/468], Loss: 0.00463731\n",
      "<Normal Phase> Epoch [74/200], Step [400/468], Loss: 0.00030736\n",
      "<Normal Phase> totally train at Epoch [74/200], Loss: 0.746392, Train Acc.: 99.9650\n",
      "**datalen=365, k=35.222391\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5900 %\n",
      "<Normal Phase> Epoch [75/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [75/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [75/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [75/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [75/200], Loss: 0.140048, Train Acc.: 99.9983\n",
      "**datalen=207, k=36.983510\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6700 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [76/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [76/200], Step [200/468], Loss: 0.00051215\n",
      "<Normal Phase> Epoch [76/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [76/200], Step [400/468], Loss: 0.00160314\n",
      "<Normal Phase> totally train at Epoch [76/200], Loss: 0.295108, Train Acc.: 99.9783\n",
      "**datalen=249, k=38.832686\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6000 %\n",
      "<Normal Phase> Epoch [77/200], Step [100/468], Loss: 0.00202358\n",
      "<Normal Phase> Epoch [77/200], Step [200/468], Loss: 0.00021316\n",
      "<Normal Phase> Epoch [77/200], Step [300/468], Loss: 0.00020602\n",
      "<Normal Phase> Epoch [77/200], Step [400/468], Loss: 0.00024429\n",
      "<Normal Phase> totally train at Epoch [77/200], Loss: 0.663858, Train Acc.: 99.9617\n",
      "**datalen=426, k=40.774320\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4700 %\n",
      "<Normal Phase> Epoch [78/200], Step [100/468], Loss: 0.00020318\n",
      "<Normal Phase> Epoch [78/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [78/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [78/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [78/200], Loss: 0.305822, Train Acc.: 99.9883\n",
      "**datalen=284, k=42.813036\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6100 %\n",
      "<Normal Phase> Epoch [79/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [79/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [79/200], Step [300/468], Loss: 0.00032416\n",
      "<Normal Phase> Epoch [79/200], Step [400/468], Loss: 0.00038663\n",
      "<Normal Phase> totally train at Epoch [79/200], Loss: 0.307756, Train Acc.: 99.9800\n",
      "**datalen=262, k=44.953688\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5800 %\n",
      "<Normal Phase> Epoch [80/200], Step [100/468], Loss: 0.00184333\n",
      "<Normal Phase> Epoch [80/200], Step [200/468], Loss: 0.00031789\n",
      "<Normal Phase> Epoch [80/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [80/200], Step [400/468], Loss: 0.00027402\n",
      "<Normal Phase> totally train at Epoch [80/200], Loss: 0.900545, Train Acc.: 99.9450\n",
      "**datalen=473, k=47.201372\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7300 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [81/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [81/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [81/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [81/200], Step [400/468], Loss: 0.00225820\n",
      "<Normal Phase> totally train at Epoch [81/200], Loss: 0.529412, Train Acc.: 99.9650\n",
      "**datalen=363, k=49.561441\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [82/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [82/200], Step [200/468], Loss: 0.00026182\n",
      "<Normal Phase> Epoch [82/200], Step [300/468], Loss: 0.00106233\n",
      "<Normal Phase> Epoch [82/200], Step [400/468], Loss: 0.00020048\n",
      "<Normal Phase> totally train at Epoch [82/200], Loss: 0.184190, Train Acc.: 99.9933\n",
      "**datalen=263, k=52.039513\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [83/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [83/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [83/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [83/200], Step [400/468], Loss: 0.00068735\n",
      "<Normal Phase> totally train at Epoch [83/200], Loss: 0.100455, Train Acc.: 99.9950\n",
      "**datalen=180, k=54.641489\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6600 %\n",
      "<Normal Phase> Epoch [84/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [84/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [84/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [84/200], Step [400/468], Loss: 0.00110921\n",
      "<Normal Phase> totally train at Epoch [84/200], Loss: 0.101821, Train Acc.: 99.9933\n",
      "**datalen=153, k=57.373563\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4400 %\n",
      "<Normal Phase> Epoch [85/200], Step [100/468], Loss: 0.00106056\n",
      "<Normal Phase> Epoch [85/200], Step [200/468], Loss: 0.00034196\n",
      "<Normal Phase> Epoch [85/200], Step [300/468], Loss: 0.00127754\n",
      "<Normal Phase> Epoch [85/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [85/200], Loss: 1.104033, Train Acc.: 99.9200\n",
      "**datalen=584, k=60.242241\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5700 %\n",
      "<Normal Phase> Epoch [86/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [86/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [86/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [86/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [86/200], Loss: 0.189986, Train Acc.: 99.9950\n",
      "**datalen=297, k=63.254353\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6800 %\n",
      "<Normal Phase> Epoch [87/200], Step [100/468], Loss: 0.00040378\n",
      "<Normal Phase> Epoch [87/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [87/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [87/200], Step [400/468], Loss: 0.00777888\n",
      "<Normal Phase> totally train at Epoch [87/200], Loss: 0.172652, Train Acc.: 99.9933\n",
      "**datalen=216, k=66.417071\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6600 %\n",
      "<Normal Phase> Epoch [88/200], Step [100/468], Loss: 0.00018261\n",
      "<Normal Phase> Epoch [88/200], Step [200/468], Loss: 0.00080145\n",
      "<Normal Phase> Epoch [88/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [88/200], Step [400/468], Loss: 0.00080597\n",
      "<Normal Phase> totally train at Epoch [88/200], Loss: 0.788082, Train Acc.: 99.9517\n",
      "**datalen=444, k=69.737925\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6000 %\n",
      "<Normal Phase> Epoch [89/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [89/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [89/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [89/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [89/200], Loss: 0.245873, Train Acc.: 99.9900\n",
      "**datalen=299, k=73.224821\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5900 %\n",
      "<Normal Phase> Epoch [90/200], Step [100/468], Loss: 0.00020420\n",
      "<Normal Phase> Epoch [90/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [90/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [90/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [90/200], Loss: 0.379823, Train Acc.: 99.9800\n",
      "**datalen=281, k=76.886062\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7200 %\n",
      "<Normal Phase> Epoch [91/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [91/200], Step [200/468], Loss: 0.00026463\n",
      "<Normal Phase> Epoch [91/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [91/200], Step [400/468], Loss: 0.00014355\n",
      "<Normal Phase> totally train at Epoch [91/200], Loss: 0.163962, Train Acc.: 99.9967\n",
      "**datalen=219, k=80.730365\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6300 %\n",
      "<Normal Phase> Epoch [92/200], Step [100/468], Loss: 0.00012257\n",
      "<Normal Phase> Epoch [92/200], Step [200/468], Loss: 0.00017315\n",
      "<Normal Phase> Epoch [92/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [92/200], Step [400/468], Loss: 0.00040473\n",
      "<Normal Phase> totally train at Epoch [92/200], Loss: 0.275376, Train Acc.: 99.9817\n",
      "**datalen=240, k=84.766883\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6300 %\n",
      "<Normal Phase> Epoch [93/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [93/200], Step [200/468], Loss: 0.00008896\n",
      "<Normal Phase> Epoch [93/200], Step [300/468], Loss: 0.00063926\n",
      "<Normal Phase> Epoch [93/200], Step [400/468], Loss: 0.00201966\n",
      "<Normal Phase> totally train at Epoch [93/200], Loss: 0.859349, Train Acc.: 99.9450\n",
      "**datalen=554, k=89.005227\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4000 %\n",
      "<Normal Phase> Epoch [94/200], Step [100/468], Loss: 0.00011279\n",
      "<Normal Phase> Epoch [94/200], Step [200/468], Loss: 0.00037969\n",
      "<Normal Phase> Epoch [94/200], Step [300/468], Loss: 0.02816424\n",
      "<Normal Phase> Epoch [94/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [94/200], Loss: 0.444155, Train Acc.: 99.9867\n",
      "**datalen=518, k=93.455489\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6000 %\n",
      "<Normal Phase> Epoch [95/200], Step [100/468], Loss: 0.00012037\n",
      "<Normal Phase> Epoch [95/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [95/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [95/200], Step [400/468], Loss: 0.00008686\n",
      "<Normal Phase> totally train at Epoch [95/200], Loss: 0.138667, Train Acc.: 99.9900\n",
      "**datalen=282, k=98.128263\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6700 %\n",
      "<Normal Phase> Epoch [96/200], Step [100/468], Loss: 0.00025768\n",
      "<Normal Phase> Epoch [96/200], Step [200/468], Loss: 0.00067326\n",
      "<Normal Phase> Epoch [96/200], Step [300/468], Loss: 0.00054146\n",
      "<Normal Phase> Epoch [96/200], Step [400/468], Loss: 0.00062879\n",
      "<Normal Phase> totally train at Epoch [96/200], Loss: 0.271034, Train Acc.: 99.9833\n",
      "**datalen=342, k=103.034676\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5400 %\n",
      "<Normal Phase> Epoch [97/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [97/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [97/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [97/200], Step [400/468], Loss: 0.00016013\n",
      "<Normal Phase> totally train at Epoch [97/200], Loss: 0.285032, Train Acc.: 99.9850\n",
      "**datalen=324, k=108.186410\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5200 %\n",
      "<Normal Phase> Epoch [98/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [98/200], Step [200/468], Loss: 0.00040163\n",
      "<Normal Phase> Epoch [98/200], Step [300/468], Loss: 0.00265731\n",
      "<Normal Phase> Epoch [98/200], Step [400/468], Loss: 0.00016365\n",
      "<Normal Phase> totally train at Epoch [98/200], Loss: 0.474702, Train Acc.: 99.9667\n",
      "**datalen=368, k=113.595731\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7100 %\n",
      "<Normal Phase> Epoch [99/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [99/200], Step [200/468], Loss: 0.00006603\n",
      "<Normal Phase> Epoch [99/200], Step [300/468], Loss: 0.00036591\n",
      "<Normal Phase> Epoch [99/200], Step [400/468], Loss: 0.00013965\n",
      "<Normal Phase> totally train at Epoch [99/200], Loss: 0.204055, Train Acc.: 99.9900\n",
      "**datalen=342, k=119.275517\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7000 %\n",
      "<Normal Phase> Epoch [100/200], Step [100/468], Loss: 0.00016477\n",
      "<Normal Phase> Epoch [100/200], Step [200/468], Loss: 0.00039783\n",
      "<Normal Phase> Epoch [100/200], Step [300/468], Loss: 0.00113980\n",
      "<Normal Phase> Epoch [100/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [100/200], Loss: 0.169045, Train Acc.: 99.9950\n",
      "**datalen=286, k=125.239293\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6100 %\n",
      "<Normal Phase> Epoch [101/200], Step [100/468], Loss: 0.00087502\n",
      "<Normal Phase> Epoch [101/200], Step [200/468], Loss: 0.00052557\n",
      "<Normal Phase> Epoch [101/200], Step [300/468], Loss: 0.00963741\n",
      "<Normal Phase> Epoch [101/200], Step [400/468], Loss: 0.00021064\n",
      "<Normal Phase> totally train at Epoch [101/200], Loss: 0.201785, Train Acc.: 99.9883\n",
      "**datalen=300, k=131.501258\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5900 %\n",
      "<Normal Phase> Epoch [102/200], Step [100/468], Loss: 0.00032286\n",
      "<Normal Phase> Epoch [102/200], Step [200/468], Loss: 0.00011960\n",
      "<Normal Phase> Epoch [102/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [102/200], Step [400/468], Loss: 0.00009777\n",
      "<Normal Phase> totally train at Epoch [102/200], Loss: 0.369869, Train Acc.: 99.9783\n",
      "**datalen=497, k=138.076321\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2600 %\n",
      "<Normal Phase> Epoch [103/200], Step [100/468], Loss: 0.00072129\n",
      "<Normal Phase> Epoch [103/200], Step [200/468], Loss: 0.00008083\n",
      "<Normal Phase> Epoch [103/200], Step [300/468], Loss: 0.00072632\n",
      "<Normal Phase> Epoch [103/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [103/200], Loss: 0.699737, Train Acc.: 99.9617\n",
      "**datalen=617, k=144.980137\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4800 %\n",
      "<Normal Phase> Epoch [104/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [104/200], Step [200/468], Loss: 0.00010830\n",
      "<Normal Phase> Epoch [104/200], Step [300/468], Loss: 0.00695850\n",
      "<Normal Phase> Epoch [104/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [104/200], Loss: 0.271745, Train Acc.: 99.9900\n",
      "**datalen=412, k=152.229144\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5300 %\n",
      "<Normal Phase> Epoch [105/200], Step [100/468], Loss: 0.00011889\n",
      "<Normal Phase> Epoch [105/200], Step [200/468], Loss: 0.04667360\n",
      "<Normal Phase> Epoch [105/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [105/200], Step [400/468], Loss: 0.00033719\n",
      "<Normal Phase> totally train at Epoch [105/200], Loss: 0.378722, Train Acc.: 99.9817\n",
      "**datalen=352, k=159.840601\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6100 %\n",
      "<Normal Phase> Epoch [106/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [106/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [106/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [106/200], Step [400/468], Loss: 0.00010028\n",
      "<Normal Phase> totally train at Epoch [106/200], Loss: 0.127239, Train Acc.: 99.9967\n",
      "**datalen=260, k=167.832631\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6200 %\n",
      "<Normal Phase> Epoch [107/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [107/200], Step [200/468], Loss: 0.00010285\n",
      "<Normal Phase> Epoch [107/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [107/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [107/200], Loss: 0.266768, Train Acc.: 99.9883\n",
      "**datalen=365, k=176.224262\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5900 %\n",
      "<Normal Phase> Epoch [108/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [108/200], Step [200/468], Loss: 0.00178744\n",
      "<Normal Phase> Epoch [108/200], Step [300/468], Loss: 0.00006114\n",
      "<Normal Phase> Epoch [108/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [108/200], Loss: 0.252693, Train Acc.: 99.9867\n",
      "**datalen=413, k=185.035475\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [109/200], Step [100/468], Loss: 0.00039854\n",
      "<Normal Phase> Epoch [109/200], Step [200/468], Loss: 0.00008203\n",
      "<Normal Phase> Epoch [109/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [109/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [109/200], Loss: 0.842416, Train Acc.: 99.9450\n",
      "**datalen=691, k=194.287249\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [110/200], Step [100/468], Loss: 0.00155383\n",
      "<Normal Phase> Epoch [110/200], Step [200/468], Loss: 0.00030861\n",
      "<Normal Phase> Epoch [110/200], Step [300/468], Loss: 0.00017830\n",
      "<Normal Phase> Epoch [110/200], Step [400/468], Loss: 0.00006936\n",
      "<Normal Phase> totally train at Epoch [110/200], Loss: 0.321624, Train Acc.: 99.9850\n",
      "**datalen=551, k=204.001612\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6200 %\n",
      "<Normal Phase> Epoch [111/200], Step [100/468], Loss: 0.00007534\n",
      "<Normal Phase> Epoch [111/200], Step [200/468], Loss: 0.00004828\n",
      "<Normal Phase> Epoch [111/200], Step [300/468], Loss: 0.00003691\n",
      "<Normal Phase> Epoch [111/200], Step [400/468], Loss: 0.00038074\n",
      "<Normal Phase> totally train at Epoch [111/200], Loss: 0.156377, Train Acc.: 99.9950\n",
      "**datalen=362, k=214.201692\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [112/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [112/200], Step [200/468], Loss: 0.00008235\n",
      "<Normal Phase> Epoch [112/200], Step [300/468], Loss: 0.00148913\n",
      "<Normal Phase> Epoch [112/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [112/200], Loss: 0.192390, Train Acc.: 99.9867\n",
      "**datalen=312, k=224.911777\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [113/200], Step [100/468], Loss: 0.00007343\n",
      "<Normal Phase> Epoch [113/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [113/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [113/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [113/200], Loss: 0.033142, Train Acc.: 100.0000\n",
      "**datalen=203, k=236.157366\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5400 %\n",
      "<Normal Phase> Epoch [114/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [114/200], Step [200/468], Loss: 0.00008621\n",
      "<Normal Phase> Epoch [114/200], Step [300/468], Loss: 0.00064934\n",
      "<Normal Phase> Epoch [114/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [114/200], Loss: 0.191716, Train Acc.: 99.9833\n",
      "**datalen=218, k=247.965234\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3900 %\n",
      "<Normal Phase> Epoch [115/200], Step [100/468], Loss: 0.00024565\n",
      "<Normal Phase> Epoch [115/200], Step [200/468], Loss: 0.00009929\n",
      "<Normal Phase> Epoch [115/200], Step [300/468], Loss: 0.00344681\n",
      "<Normal Phase> Epoch [115/200], Step [400/468], Loss: 0.00109471\n",
      "<Normal Phase> totally train at Epoch [115/200], Loss: 1.120401, Train Acc.: 99.9117\n",
      "**datalen=1036, k=260.363496\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3300 %\n",
      "<Normal Phase> Epoch [116/200], Step [100/468], Loss: 0.00673312\n",
      "<Normal Phase> Epoch [116/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [116/200], Step [300/468], Loss: 0.00187752\n",
      "<Normal Phase> Epoch [116/200], Step [400/468], Loss: 0.00029768\n",
      "<Normal Phase> totally train at Epoch [116/200], Loss: 0.239932, Train Acc.: 99.9850\n",
      "**datalen=598, k=273.381671\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5900 %\n",
      "<Normal Phase> Epoch [117/200], Step [100/468], Loss: 0.00007410\n",
      "<Normal Phase> Epoch [117/200], Step [200/468], Loss: 0.00008703\n",
      "<Normal Phase> Epoch [117/200], Step [300/468], Loss: 0.00010910\n",
      "<Normal Phase> Epoch [117/200], Step [400/468], Loss: 0.00004902\n",
      "<Normal Phase> totally train at Epoch [117/200], Loss: 0.246278, Train Acc.: 99.9883\n",
      "**datalen=482, k=287.050754\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5000 %\n",
      "<Normal Phase> Epoch [118/200], Step [100/468], Loss: 0.00012306\n",
      "<Normal Phase> Epoch [118/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [118/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [118/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [118/200], Loss: 0.257389, Train Acc.: 99.9850\n",
      "**datalen=442, k=301.403292\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [119/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [119/200], Step [200/468], Loss: 0.00029960\n",
      "<Normal Phase> Epoch [119/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [119/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [119/200], Loss: 0.379716, Train Acc.: 99.9767\n",
      "**datalen=695, k=316.473456\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5200 %\n",
      "<Normal Phase> Epoch [120/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [120/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [120/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [120/200], Step [400/468], Loss: 0.00010421\n",
      "<Normal Phase> totally train at Epoch [120/200], Loss: 0.183790, Train Acc.: 99.9900\n",
      "**datalen=494, k=332.297129\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6500 %\n",
      "<Normal Phase> Epoch [121/200], Step [100/468], Loss: 0.00030298\n",
      "<Normal Phase> Epoch [121/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [121/200], Step [300/468], Loss: 0.00006314\n",
      "<Normal Phase> Epoch [121/200], Step [400/468], Loss: 0.00032198\n",
      "<Normal Phase> totally train at Epoch [121/200], Loss: 0.029159, Train Acc.: 100.0000\n",
      "**datalen=266, k=348.911986\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6500 %\n",
      "<Normal Phase> Epoch [122/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [122/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [122/200], Step [300/468], Loss: 0.00158094\n",
      "<Normal Phase> Epoch [122/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [122/200], Loss: 0.020740, Train Acc.: 100.0000\n",
      "**datalen=214, k=366.357585\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6900 %\n",
      "<Normal Phase> Epoch [123/200], Step [100/468], Loss: 0.00002363\n",
      "<Normal Phase> Epoch [123/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [123/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [123/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [123/200], Loss: 0.013766, Train Acc.: 100.0000\n",
      "**datalen=194, k=384.675464\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7100 %\n",
      "<Normal Phase> Epoch [124/200], Step [100/468], Loss: 0.00003583\n",
      "<Normal Phase> Epoch [124/200], Step [200/468], Loss: 0.00006771\n",
      "<Normal Phase> Epoch [124/200], Step [300/468], Loss: 0.00008310\n",
      "<Normal Phase> Epoch [124/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [124/200], Loss: 0.009355, Train Acc.: 100.0000\n",
      "**datalen=159, k=403.909237\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7200 %\n",
      "<Normal Phase> Epoch [125/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [125/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [125/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [125/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [125/200], Loss: 0.010897, Train Acc.: 100.0000\n",
      "**datalen=165, k=424.104699\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6800 %\n",
      "<Normal Phase> Epoch [126/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [126/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [126/200], Step [300/468], Loss: 0.00008311\n",
      "<Normal Phase> Epoch [126/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [126/200], Loss: 0.664412, Train Acc.: 99.9750\n",
      "**datalen=281, k=445.309934\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2700 %\n",
      "<Normal Phase> Epoch [127/200], Step [100/468], Loss: 0.00019874\n",
      "<Normal Phase> Epoch [127/200], Step [200/468], Loss: 0.00052090\n",
      "<Normal Phase> Epoch [127/200], Step [300/468], Loss: 0.00165432\n",
      "<Normal Phase> Epoch [127/200], Step [400/468], Loss: 0.00130273\n",
      "<Normal Phase> totally train at Epoch [127/200], Loss: 1.170554, Train Acc.: 99.9167\n",
      "**datalen=1351, k=467.575431\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5200 %\n",
      "<Normal Phase> Epoch [128/200], Step [100/468], Loss: 0.00151100\n",
      "<Normal Phase> Epoch [128/200], Step [200/468], Loss: 0.00058272\n",
      "<Normal Phase> Epoch [128/200], Step [300/468], Loss: 0.01764737\n",
      "<Normal Phase> Epoch [128/200], Step [400/468], Loss: 0.00009783\n",
      "<Normal Phase> totally train at Epoch [128/200], Loss: 0.778871, Train Acc.: 99.9500\n",
      "**datalen=964, k=490.954203\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5300 %\n",
      "<Normal Phase> Epoch [129/200], Step [100/468], Loss: 0.00004680\n",
      "<Normal Phase> Epoch [129/200], Step [200/468], Loss: 0.00001539\n",
      "<Normal Phase> Epoch [129/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [129/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [129/200], Loss: 0.093874, Train Acc.: 99.9983\n",
      "**datalen=575, k=515.501913\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6900 %\n",
      "<Normal Phase> Epoch [130/200], Step [100/468], Loss: 0.00012409\n",
      "<Normal Phase> Epoch [130/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [130/200], Step [300/468], Loss: 0.00001810\n",
      "<Normal Phase> Epoch [130/200], Step [400/468], Loss: 0.00002102\n",
      "<Normal Phase> totally train at Epoch [130/200], Loss: 0.169387, Train Acc.: 99.9917\n",
      "**datalen=491, k=541.277008\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7100 %\n",
      "<Normal Phase> Epoch [131/200], Step [100/468], Loss: 0.00008430\n",
      "<Normal Phase> Epoch [131/200], Step [200/468], Loss: 0.00038001\n",
      "<Normal Phase> Epoch [131/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [131/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [131/200], Loss: 0.065493, Train Acc.: 99.9967\n",
      "**datalen=423, k=568.340859\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7000 %\n",
      "<Normal Phase> Epoch [132/200], Step [100/468], Loss: 0.00022951\n",
      "<Normal Phase> Epoch [132/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [132/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [132/200], Step [400/468], Loss: 0.00001568\n",
      "<Normal Phase> totally train at Epoch [132/200], Loss: 0.027479, Train Acc.: 100.0000\n",
      "**datalen=340, k=596.757902\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7500 %\n",
      "Best model saved!\n",
      "<Normal Phase> Epoch [133/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [133/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [133/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [133/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [133/200], Loss: 0.267504, Train Acc.: 99.9900\n",
      "**datalen=340, k=626.595797\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6800 %\n",
      "<Normal Phase> Epoch [134/200], Step [100/468], Loss: 0.00006233\n",
      "<Normal Phase> Epoch [134/200], Step [200/468], Loss: 0.00010782\n",
      "<Normal Phase> Epoch [134/200], Step [300/468], Loss: 0.00042199\n",
      "<Normal Phase> Epoch [134/200], Step [400/468], Loss: 0.00016493\n",
      "<Normal Phase> totally train at Epoch [134/200], Loss: 1.019450, Train Acc.: 99.9350\n",
      "**datalen=1129, k=657.925587\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2000 %\n",
      "<Normal Phase> Epoch [135/200], Step [100/468], Loss: 0.00239498\n",
      "<Normal Phase> Epoch [135/200], Step [200/468], Loss: 0.00006864\n",
      "<Normal Phase> Epoch [135/200], Step [300/468], Loss: 0.00035938\n",
      "<Normal Phase> Epoch [135/200], Step [400/468], Loss: 0.00006611\n",
      "<Normal Phase> totally train at Epoch [135/200], Loss: 0.502638, Train Acc.: 99.9667\n",
      "**datalen=1139, k=690.821866\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6500 %\n",
      "<Normal Phase> Epoch [136/200], Step [100/468], Loss: 0.00022489\n",
      "<Normal Phase> Epoch [136/200], Step [200/468], Loss: 0.00035096\n",
      "<Normal Phase> Epoch [136/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [136/200], Step [400/468], Loss: 0.00001147\n",
      "<Normal Phase> totally train at Epoch [136/200], Loss: 0.092310, Train Acc.: 99.9967\n",
      "**datalen=629, k=725.362959\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6400 %\n",
      "<Normal Phase> Epoch [137/200], Step [100/468], Loss: 0.00001736\n",
      "<Normal Phase> Epoch [137/200], Step [200/468], Loss: 0.00001043\n",
      "<Normal Phase> Epoch [137/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [137/200], Step [400/468], Loss: 0.00001684\n",
      "<Normal Phase> totally train at Epoch [137/200], Loss: 0.028231, Train Acc.: 100.0000\n",
      "**datalen=462, k=761.631107\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6300 %\n",
      "<Normal Phase> Epoch [138/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [138/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [138/200], Step [300/468], Loss: 0.00005180\n",
      "<Normal Phase> Epoch [138/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [138/200], Loss: 0.022914, Train Acc.: 100.0000\n",
      "**datalen=397, k=799.712662\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6500 %\n",
      "<Normal Phase> Epoch [139/200], Step [100/468], Loss: 0.00001386\n",
      "<Normal Phase> Epoch [139/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [139/200], Step [300/468], Loss: 0.00001184\n",
      "<Normal Phase> Epoch [139/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [139/200], Loss: 0.011484, Train Acc.: 100.0000\n",
      "**datalen=310, k=839.698296\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7000 %\n",
      "<Normal Phase> Epoch [140/200], Step [100/468], Loss: 0.00002434\n",
      "<Normal Phase> Epoch [140/200], Step [200/468], Loss: 0.00001013\n",
      "<Normal Phase> Epoch [140/200], Step [300/468], Loss: 0.00001528\n",
      "<Normal Phase> Epoch [140/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [140/200], Loss: 0.010051, Train Acc.: 100.0000\n",
      "**datalen=282, k=881.683210\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6500 %\n",
      "<Normal Phase> Epoch [141/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [141/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [141/200], Step [300/468], Loss: 0.00548526\n",
      "<Normal Phase> Epoch [141/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [141/200], Loss: 1.041267, Train Acc.: 99.9500\n",
      "**datalen=780, k=925.767371\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.3400 %\n",
      "<Normal Phase> Epoch [142/200], Step [100/468], Loss: 0.02515057\n",
      "<Normal Phase> Epoch [142/200], Step [200/468], Loss: 0.00813969\n",
      "<Normal Phase> Epoch [142/200], Step [300/468], Loss: 0.00040635\n",
      "<Normal Phase> Epoch [142/200], Step [400/468], Loss: 0.00007946\n",
      "<Normal Phase> totally train at Epoch [142/200], Loss: 0.654970, Train Acc.: 99.9550\n",
      "**datalen=1270, k=972.055739\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5000 %\n",
      "<Normal Phase> Epoch [143/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [143/200], Step [200/468], Loss: 0.00003906\n",
      "<Normal Phase> Epoch [143/200], Step [300/468], Loss: 0.00005862\n",
      "<Normal Phase> Epoch [143/200], Step [400/468], Loss: 0.00146692\n",
      "<Normal Phase> totally train at Epoch [143/200], Loss: 0.120880, Train Acc.: 99.9967\n",
      "**datalen=802, k=1020.658526\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5900 %\n",
      "<Normal Phase> Epoch [144/200], Step [100/468], Loss: 0.00121760\n",
      "<Normal Phase> Epoch [144/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [144/200], Step [300/468], Loss: 0.00005689\n",
      "<Normal Phase> Epoch [144/200], Step [400/468], Loss: 0.00004999\n",
      "<Normal Phase> totally train at Epoch [144/200], Loss: 0.112480, Train Acc.: 99.9950\n",
      "**datalen=724, k=1071.691453\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6600 %\n",
      "<Normal Phase> Epoch [145/200], Step [100/468], Loss: 0.00001352\n",
      "<Normal Phase> Epoch [145/200], Step [200/468], Loss: 0.00002506\n",
      "<Normal Phase> Epoch [145/200], Step [300/468], Loss: 0.00000950\n",
      "<Normal Phase> Epoch [145/200], Step [400/468], Loss: 0.00268581\n",
      "<Normal Phase> totally train at Epoch [145/200], Loss: 0.213951, Train Acc.: 99.9817\n",
      "**datalen=726, k=1125.276025\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5200 %\n",
      "<Normal Phase> Epoch [146/200], Step [100/468], Loss: 0.00007026\n",
      "<Normal Phase> Epoch [146/200], Step [200/468], Loss: 0.00002228\n",
      "<Normal Phase> Epoch [146/200], Step [300/468], Loss: 0.00002161\n",
      "<Normal Phase> Epoch [146/200], Step [400/468], Loss: 0.00014894\n",
      "<Normal Phase> totally train at Epoch [146/200], Loss: 0.787658, Train Acc.: 99.9450\n",
      "**datalen=1229, k=1181.539827\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5600 %\n",
      "<Normal Phase> Epoch [147/200], Step [100/468], Loss: 0.00003739\n",
      "<Normal Phase> Epoch [147/200], Step [200/468], Loss: 0.00004635\n",
      "<Normal Phase> Epoch [147/200], Step [300/468], Loss: 0.00001132\n",
      "<Normal Phase> Epoch [147/200], Step [400/468], Loss: 0.00016999\n",
      "<Normal Phase> totally train at Epoch [147/200], Loss: 0.344338, Train Acc.: 99.9833\n",
      "**datalen=967, k=1240.616818\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6000 %\n",
      "<Normal Phase> Epoch [148/200], Step [100/468], Loss: 0.00005532\n",
      "<Normal Phase> Epoch [148/200], Step [200/468], Loss: 0.00029381\n",
      "<Normal Phase> Epoch [148/200], Step [300/468], Loss: 0.00501378\n",
      "<Normal Phase> Epoch [148/200], Step [400/468], Loss: 0.00010698\n",
      "<Normal Phase> totally train at Epoch [148/200], Loss: 0.115656, Train Acc.: 99.9950\n",
      "**datalen=820, k=1302.647659\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5100 %\n",
      "<Normal Phase> Epoch [149/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [149/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [149/200], Step [300/468], Loss: 0.00002146\n",
      "<Normal Phase> Epoch [149/200], Step [400/468], Loss: 0.00006221\n",
      "<Normal Phase> totally train at Epoch [149/200], Loss: 0.213005, Train Acc.: 99.9867\n",
      "**datalen=785, k=1367.780042\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6200 %\n",
      "<Normal Phase> Epoch [150/200], Step [100/468], Loss: 0.00000717\n",
      "<Normal Phase> Epoch [150/200], Step [200/468], Loss: 0.00115179\n",
      "<Normal Phase> Epoch [150/200], Step [300/468], Loss: 0.00009738\n",
      "<Normal Phase> Epoch [150/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [150/200], Loss: 0.154289, Train Acc.: 99.9850\n",
      "**datalen=745, k=1436.169044\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6400 %\n",
      "<Normal Phase> Epoch [151/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [151/200], Step [200/468], Loss: 0.00005631\n",
      "<Normal Phase> Epoch [151/200], Step [300/468], Loss: 0.00177307\n",
      "<Normal Phase> Epoch [151/200], Step [400/468], Loss: 0.00004301\n",
      "<Normal Phase> totally train at Epoch [151/200], Loss: 0.134838, Train Acc.: 99.9950\n",
      "**datalen=712, k=1507.977496\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5800 %\n",
      "<Normal Phase> Epoch [152/200], Step [100/468], Loss: 0.00003204\n",
      "<Normal Phase> Epoch [152/200], Step [200/468], Loss: 0.00007239\n",
      "<Normal Phase> Epoch [152/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [152/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [152/200], Loss: 0.022100, Train Acc.: 100.0000\n",
      "**datalen=567, k=1583.376371\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6000 %\n",
      "<Normal Phase> Epoch [153/200], Step [100/468], Loss: 0.00002685\n",
      "<Normal Phase> Epoch [153/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [153/200], Step [300/468], Loss: 0.00000706\n",
      "<Normal Phase> Epoch [153/200], Step [400/468], Loss: 0.00000522\n",
      "<Normal Phase> totally train at Epoch [153/200], Loss: 0.033363, Train Acc.: 100.0000\n",
      "**datalen=521, k=1662.545189\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5400 %\n",
      "<Normal Phase> Epoch [154/200], Step [100/468], Loss: 0.00000563\n",
      "<Normal Phase> Epoch [154/200], Step [200/468], Loss: 0.00000983\n",
      "<Normal Phase> Epoch [154/200], Step [300/468], Loss: 0.00007142\n",
      "<Normal Phase> Epoch [154/200], Step [400/468], Loss: 0.03364074\n",
      "<Normal Phase> totally train at Epoch [154/200], Loss: 0.449142, Train Acc.: 99.9683\n",
      "**datalen=857, k=1745.672449\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.2700 %\n",
      "<Normal Phase> Epoch [155/200], Step [100/468], Loss: 0.01245101\n",
      "<Normal Phase> Epoch [155/200], Step [200/468], Loss: 0.00677830\n",
      "<Normal Phase> Epoch [155/200], Step [300/468], Loss: 0.01207855\n",
      "<Normal Phase> Epoch [155/200], Step [400/468], Loss: 0.00015782\n",
      "<Normal Phase> totally train at Epoch [155/200], Loss: 1.128659, Train Acc.: 99.9267\n",
      "**datalen=1531, k=1832.956071\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5700 %\n",
      "<Normal Phase> Epoch [156/200], Step [100/468], Loss: 0.00001914\n",
      "<Normal Phase> Epoch [156/200], Step [200/468], Loss: 0.00001615\n",
      "<Normal Phase> Epoch [156/200], Step [300/468], Loss: 0.00010790\n",
      "<Normal Phase> Epoch [156/200], Step [400/468], Loss: 0.00004459\n",
      "<Normal Phase> totally train at Epoch [156/200], Loss: 0.128842, Train Acc.: 99.9933\n",
      "**datalen=924, k=1924.603875\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.7000 %\n",
      "<Normal Phase> Epoch [157/200], Step [100/468], Loss: 0.00010156\n",
      "<Normal Phase> Epoch [157/200], Step [200/468], Loss: 0.00028723\n",
      "<Normal Phase> Epoch [157/200], Step [300/468], Loss: 0.00000812\n",
      "<Normal Phase> Epoch [157/200], Step [400/468], Loss: 0.00002306\n",
      "<Normal Phase> totally train at Epoch [157/200], Loss: 0.029616, Train Acc.: 100.0000\n",
      "**datalen=683, k=2020.834069\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6700 %\n",
      "<Normal Phase> Epoch [158/200], Step [100/468], Loss: 0.00000615\n",
      "<Normal Phase> Epoch [158/200], Step [200/468], Loss: 0.00003050\n",
      "<Normal Phase> Epoch [158/200], Step [300/468], Loss: 0.00000368\n",
      "<Normal Phase> Epoch [158/200], Step [400/468], Loss: 0.00000454\n",
      "<Normal Phase> totally train at Epoch [158/200], Loss: 0.097923, Train Acc.: 99.9933\n",
      "**datalen=708, k=2121.875772\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6800 %\n",
      "<Normal Phase> Epoch [159/200], Step [100/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [159/200], Step [200/468], Loss: 0.00054091\n",
      "<Normal Phase> Epoch [159/200], Step [300/468], Loss: 0.00003863\n",
      "<Normal Phase> Epoch [159/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [159/200], Loss: 0.087147, Train Acc.: 99.9967\n",
      "**datalen=797, k=2227.969561\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5800 %\n",
      "<Normal Phase> Epoch [160/200], Step [100/468], Loss: 0.00008712\n",
      "<Normal Phase> Epoch [160/200], Step [200/468], Loss: 0.00006337\n",
      "<Normal Phase> Epoch [160/200], Step [300/468], Loss: 0.00028552\n",
      "<Normal Phase> Epoch [160/200], Step [400/468], Loss: 0.00006242\n",
      "<Normal Phase> totally train at Epoch [160/200], Loss: 0.681113, Train Acc.: 99.9550\n",
      "**datalen=1297, k=2339.368039\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6400 %\n",
      "<Normal Phase> Epoch [161/200], Step [100/468], Loss: 0.00002151\n",
      "<Normal Phase> Epoch [161/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [161/200], Step [300/468], Loss: 0.00003524\n",
      "<Normal Phase> Epoch [161/200], Step [400/468], Loss: 0.00115304\n",
      "<Normal Phase> totally train at Epoch [161/200], Loss: 0.362119, Train Acc.: 99.9833\n",
      "**datalen=1209, k=2456.336441\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5300 %\n",
      "<Normal Phase> Epoch [162/200], Step [100/468], Loss: 0.00000751\n",
      "<Normal Phase> Epoch [162/200], Step [200/468], Loss: 0.00000522\n",
      "<Normal Phase> Epoch [162/200], Step [300/468], Loss: 0.00221809\n",
      "<Normal Phase> Epoch [162/200], Step [400/468], Loss: 0.00001441\n",
      "<Normal Phase> totally train at Epoch [162/200], Loss: 0.431770, Train Acc.: 99.9700\n",
      "**datalen=1410, k=2579.153263\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6600 %\n",
      "<Normal Phase> Epoch [163/200], Step [100/468], Loss: 0.00000779\n",
      "<Normal Phase> Epoch [163/200], Step [200/468], Loss: 0.00044050\n",
      "<Normal Phase> Epoch [163/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [163/200], Step [400/468], Loss: 0.00029350\n",
      "<Normal Phase> totally train at Epoch [163/200], Loss: 0.123539, Train Acc.: 99.9917\n",
      "**datalen=1095, k=2708.110926\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5700 %\n",
      "<Normal Phase> Epoch [164/200], Step [100/468], Loss: 0.00039158\n",
      "<Normal Phase> Epoch [164/200], Step [200/468], Loss: 0.00013074\n",
      "<Normal Phase> Epoch [164/200], Step [300/468], Loss: 0.00003656\n",
      "<Normal Phase> Epoch [164/200], Step [400/468], Loss: 0.00000361\n",
      "<Normal Phase> totally train at Epoch [164/200], Loss: 0.029342, Train Acc.: 100.0000\n",
      "**datalen=851, k=2843.516472\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5800 %\n",
      "<Normal Phase> Epoch [165/200], Step [100/468], Loss: 0.00001203\n",
      "<Normal Phase> Epoch [165/200], Step [200/468], Loss: 0.00002037\n",
      "<Normal Phase> Epoch [165/200], Step [300/468], Loss: 0.00000276\n",
      "<Normal Phase> Epoch [165/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [165/200], Loss: 0.017703, Train Acc.: 100.0000\n",
      "**datalen=706, k=2985.692296\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6400 %\n",
      "<Normal Phase> Epoch [166/200], Step [100/468], Loss: 0.00000297\n",
      "<Normal Phase> Epoch [166/200], Step [200/468], Loss: 0.00000399\n",
      "<Normal Phase> Epoch [166/200], Step [300/468], Loss: 0.00000336\n",
      "<Normal Phase> Epoch [166/200], Step [400/468], Loss: 0.00001443\n",
      "<Normal Phase> totally train at Epoch [166/200], Loss: 0.016986, Train Acc.: 100.0000\n",
      "**datalen=633, k=3134.976910\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5800 %\n",
      "<Normal Phase> Epoch [167/200], Step [100/468], Loss: 0.00000322\n",
      "<Normal Phase> Epoch [167/200], Step [200/468], Loss: 0.00003795\n",
      "<Normal Phase> Epoch [167/200], Step [300/468], Loss: 0.00630338\n",
      "<Normal Phase> Epoch [167/200], Step [400/468], Loss: 0.00509445\n",
      "<Normal Phase> totally train at Epoch [167/200], Loss: 1.062171, Train Acc.: 99.9517\n",
      "**datalen=1435, k=3291.725756\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [168/200], Step [100/468], Loss: 0.00002826\n",
      "<Normal Phase> Epoch [168/200], Step [200/468], Loss: 0.00027430\n",
      "<Normal Phase> Epoch [168/200], Step [300/468], Loss: 0.00000526\n",
      "<Normal Phase> Epoch [168/200], Step [400/468], Loss: 0.00000950\n",
      "<Normal Phase> totally train at Epoch [168/200], Loss: 0.650522, Train Acc.: 99.9500\n",
      "**datalen=1742, k=3456.312044\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4200 %\n",
      "<Normal Phase> Epoch [169/200], Step [100/468], Loss: 0.00016117\n",
      "<Normal Phase> Epoch [169/200], Step [200/468], Loss: 0.00125695\n",
      "<Normal Phase> Epoch [169/200], Step [300/468], Loss: 0.00008808\n",
      "<Normal Phase> Epoch [169/200], Step [400/468], Loss: 0.00023452\n",
      "<Normal Phase> totally train at Epoch [169/200], Loss: 0.525844, Train Acc.: 99.9667\n",
      "**datalen=1772, k=3629.127646\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5000 %\n",
      "<Normal Phase> Epoch [170/200], Step [100/468], Loss: 0.00001815\n",
      "<Normal Phase> Epoch [170/200], Step [200/468], Loss: 0.00016973\n",
      "<Normal Phase> Epoch [170/200], Step [300/468], Loss: 0.00001667\n",
      "<Normal Phase> Epoch [170/200], Step [400/468], Loss: 0.00010834\n",
      "<Normal Phase> totally train at Epoch [170/200], Loss: 0.231221, Train Acc.: 99.9817\n",
      "**datalen=1404, k=3810.584028\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4900 %\n",
      "<Normal Phase> Epoch [171/200], Step [100/468], Loss: 0.00000528\n",
      "<Normal Phase> Epoch [171/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [171/200], Step [300/468], Loss: 0.00000294\n",
      "<Normal Phase> Epoch [171/200], Step [400/468], Loss: 0.00011277\n",
      "<Normal Phase> totally train at Epoch [171/200], Loss: 0.169644, Train Acc.: 99.9933\n",
      "**datalen=1332, k=4001.113230\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5600 %\n",
      "<Normal Phase> Epoch [172/200], Step [100/468], Loss: 0.00015505\n",
      "<Normal Phase> Epoch [172/200], Step [200/468], Loss: 0.00008281\n",
      "<Normal Phase> Epoch [172/200], Step [300/468], Loss: 0.00003242\n",
      "<Normal Phase> Epoch [172/200], Step [400/468], Loss: 0.00004127\n",
      "<Normal Phase> totally train at Epoch [172/200], Loss: 0.027137, Train Acc.: 100.0000\n",
      "**datalen=1086, k=4201.168891\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5600 %\n",
      "<Normal Phase> Epoch [173/200], Step [100/468], Loss: 0.00001027\n",
      "<Normal Phase> Epoch [173/200], Step [200/468], Loss: 0.00003763\n",
      "<Normal Phase> Epoch [173/200], Step [300/468], Loss: 0.00022075\n",
      "<Normal Phase> Epoch [173/200], Step [400/468], Loss: 0.00000494\n",
      "<Normal Phase> totally train at Epoch [173/200], Loss: 0.259987, Train Acc.: 99.9833\n",
      "**datalen=1118, k=4411.227336\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5700 %\n",
      "<Normal Phase> Epoch [174/200], Step [100/468], Loss: 0.00007982\n",
      "<Normal Phase> Epoch [174/200], Step [200/468], Loss: 0.00003960\n",
      "<Normal Phase> Epoch [174/200], Step [300/468], Loss: 0.00005347\n",
      "<Normal Phase> Epoch [174/200], Step [400/468], Loss: 0.00007278\n",
      "<Normal Phase> totally train at Epoch [174/200], Loss: 0.349345, Train Acc.: 99.9767\n",
      "**datalen=1386, k=4631.788703\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5400 %\n",
      "<Normal Phase> Epoch [175/200], Step [100/468], Loss: 0.00002426\n",
      "<Normal Phase> Epoch [175/200], Step [200/468], Loss: 0.00004024\n",
      "<Normal Phase> Epoch [175/200], Step [300/468], Loss: 0.00008421\n",
      "<Normal Phase> Epoch [175/200], Step [400/468], Loss: 0.00262864\n",
      "<Normal Phase> totally train at Epoch [175/200], Loss: 0.038611, Train Acc.: 99.9983\n",
      "**datalen=1071, k=4863.378138\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6000 %\n",
      "<Normal Phase> Epoch [176/200], Step [100/468], Loss: 0.00001422\n",
      "<Normal Phase> Epoch [176/200], Step [200/468], Loss: 0.00002274\n",
      "<Normal Phase> Epoch [176/200], Step [300/468], Loss: 0.00002184\n",
      "<Normal Phase> Epoch [176/200], Step [400/468], Loss: 0.00003610\n",
      "<Normal Phase> totally train at Epoch [176/200], Loss: 0.027585, Train Acc.: 100.0000\n",
      "**datalen=980, k=5106.547045\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [177/200], Step [100/468], Loss: 0.00001387\n",
      "<Normal Phase> Epoch [177/200], Step [200/468], Loss: 0.00002574\n",
      "<Normal Phase> Epoch [177/200], Step [300/468], Loss: 0.00000423\n",
      "<Normal Phase> Epoch [177/200], Step [400/468], Loss: 0.00000166\n",
      "<Normal Phase> totally train at Epoch [177/200], Loss: 0.049332, Train Acc.: 99.9983\n",
      "**datalen=928, k=5361.874397\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6700 %\n",
      "<Normal Phase> Epoch [178/200], Step [100/468], Loss: 0.00002663\n",
      "<Normal Phase> Epoch [178/200], Step [200/468], Loss: 0.00001500\n",
      "<Normal Phase> Epoch [178/200], Step [300/468], Loss: 0.01904809\n",
      "<Normal Phase> Epoch [178/200], Step [400/468], Loss: 0.00002396\n",
      "<Normal Phase> totally train at Epoch [178/200], Loss: 0.935566, Train Acc.: 99.9400\n",
      "**datalen=2019, k=5629.968117\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4100 %\n",
      "<Normal Phase> Epoch [179/200], Step [100/468], Loss: 0.00002282\n",
      "<Normal Phase> Epoch [179/200], Step [200/468], Loss: 0.00024987\n",
      "<Normal Phase> Epoch [179/200], Step [300/468], Loss: 0.00001656\n",
      "<Normal Phase> Epoch [179/200], Step [400/468], Loss: 0.00001092\n",
      "<Normal Phase> totally train at Epoch [179/200], Loss: 0.434606, Train Acc.: 99.9750\n",
      "**datalen=1632, k=5911.466522\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [180/200], Step [100/468], Loss: 0.00000406\n",
      "<Normal Phase> Epoch [180/200], Step [200/468], Loss: 0.00004301\n",
      "<Normal Phase> Epoch [180/200], Step [300/468], Loss: 0.00002032\n",
      "<Normal Phase> Epoch [180/200], Step [400/468], Loss: 0.00056327\n",
      "<Normal Phase> totally train at Epoch [180/200], Loss: 0.149585, Train Acc.: 99.9933\n",
      "**datalen=1315, k=6207.039849\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4800 %\n",
      "<Normal Phase> Epoch [181/200], Step [100/468], Loss: 0.00003261\n",
      "<Normal Phase> Epoch [181/200], Step [200/468], Loss: 0.00000852\n",
      "<Normal Phase> Epoch [181/200], Step [300/468], Loss: 0.00029077\n",
      "<Normal Phase> Epoch [181/200], Step [400/468], Loss: 0.00002048\n",
      "<Normal Phase> totally train at Epoch [181/200], Loss: 0.188313, Train Acc.: 99.9933\n",
      "**datalen=1402, k=6517.391841\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [182/200], Step [100/468], Loss: 0.00000532\n",
      "<Normal Phase> Epoch [182/200], Step [200/468], Loss: 0.00000282\n",
      "<Normal Phase> Epoch [182/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [182/200], Step [400/468], Loss: 0.00000481\n",
      "<Normal Phase> totally train at Epoch [182/200], Loss: 0.035208, Train Acc.: 100.0000\n",
      "**datalen=1134, k=6843.261433\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5400 %\n",
      "<Normal Phase> Epoch [183/200], Step [100/468], Loss: 0.00000801\n",
      "<Normal Phase> Epoch [183/200], Step [200/468], Loss: 0.00000221\n",
      "<Normal Phase> Epoch [183/200], Step [300/468], Loss: 0.00001678\n",
      "<Normal Phase> Epoch [183/200], Step [400/468], Loss: 0.00000225\n",
      "<Normal Phase> totally train at Epoch [183/200], Loss: 0.120103, Train Acc.: 99.9967\n",
      "**datalen=1121, k=7185.424505\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.6000 %\n",
      "<Normal Phase> Epoch [184/200], Step [100/468], Loss: 0.00001391\n",
      "<Normal Phase> Epoch [184/200], Step [200/468], Loss: 0.00809858\n",
      "<Normal Phase> Epoch [184/200], Step [300/468], Loss: 0.00002202\n",
      "<Normal Phase> Epoch [184/200], Step [400/468], Loss: 0.00028193\n",
      "<Normal Phase> totally train at Epoch [184/200], Loss: 0.377390, Train Acc.: 99.9733\n",
      "**datalen=1327, k=7544.695730\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5000 %\n",
      "<Normal Phase> Epoch [185/200], Step [100/468], Loss: 0.00000727\n",
      "<Normal Phase> Epoch [185/200], Step [200/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [185/200], Step [300/468], Loss: 0.00000277\n",
      "<Normal Phase> Epoch [185/200], Step [400/468], Loss: 0.00022797\n",
      "<Normal Phase> totally train at Epoch [185/200], Loss: 0.115693, Train Acc.: 99.9950\n",
      "**datalen=1547, k=7921.930516\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5700 %\n",
      "<Normal Phase> Epoch [186/200], Step [100/468], Loss: 0.00003141\n",
      "<Normal Phase> Epoch [186/200], Step [200/468], Loss: 0.00001733\n",
      "<Normal Phase> Epoch [186/200], Step [300/468], Loss: 0.00026352\n",
      "<Normal Phase> Epoch [186/200], Step [400/468], Loss: 0.00030543\n",
      "<Normal Phase> totally train at Epoch [186/200], Loss: 0.207394, Train Acc.: 99.9917\n",
      "**datalen=1286, k=8318.027042\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4200 %\n",
      "<Normal Phase> Epoch [187/200], Step [100/468], Loss: 0.00400930\n",
      "<Normal Phase> Epoch [187/200], Step [200/468], Loss: 0.00003526\n",
      "<Normal Phase> Epoch [187/200], Step [300/468], Loss: 0.00005138\n",
      "<Normal Phase> Epoch [187/200], Step [400/468], Loss: 0.00000000\n",
      "<Normal Phase> totally train at Epoch [187/200], Loss: 0.296282, Train Acc.: 99.9833\n",
      "**datalen=1728, k=8733.928394\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4500 %\n",
      "<Normal Phase> Epoch [188/200], Step [100/468], Loss: 0.00002077\n",
      "<Normal Phase> Epoch [188/200], Step [200/468], Loss: 0.00003608\n",
      "<Normal Phase> Epoch [188/200], Step [300/468], Loss: 0.00010128\n",
      "<Normal Phase> Epoch [188/200], Step [400/468], Loss: 0.00042338\n",
      "<Normal Phase> totally train at Epoch [188/200], Loss: 0.260686, Train Acc.: 99.9817\n",
      "**datalen=1546, k=9170.624814\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4300 %\n",
      "<Normal Phase> Epoch [189/200], Step [100/468], Loss: 0.00001244\n",
      "<Normal Phase> Epoch [189/200], Step [200/468], Loss: 0.00002888\n",
      "<Normal Phase> Epoch [189/200], Step [300/468], Loss: 0.00001043\n",
      "<Normal Phase> Epoch [189/200], Step [400/468], Loss: 0.00000656\n",
      "<Normal Phase> totally train at Epoch [189/200], Loss: 0.326401, Train Acc.: 99.9733\n",
      "**datalen=1892, k=9629.156055\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4200 %\n",
      "<Normal Phase> Epoch [190/200], Step [100/468], Loss: 0.00001115\n",
      "<Normal Phase> Epoch [190/200], Step [200/468], Loss: 0.00135726\n",
      "<Normal Phase> Epoch [190/200], Step [300/468], Loss: 0.00000528\n",
      "<Normal Phase> Epoch [190/200], Step [400/468], Loss: 0.00000722\n",
      "<Normal Phase> totally train at Epoch [190/200], Loss: 0.348841, Train Acc.: 99.9750\n",
      "**datalen=2045, k=10110.613857\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4400 %\n",
      "<Normal Phase> Epoch [191/200], Step [100/468], Loss: 0.00001330\n",
      "<Normal Phase> Epoch [191/200], Step [200/468], Loss: 0.00000999\n",
      "<Normal Phase> Epoch [191/200], Step [300/468], Loss: 0.00018523\n",
      "<Normal Phase> Epoch [191/200], Step [400/468], Loss: 0.00005373\n",
      "<Normal Phase> totally train at Epoch [191/200], Loss: 0.155657, Train Acc.: 99.9950\n",
      "**datalen=1612, k=10616.144550\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4700 %\n",
      "<Normal Phase> Epoch [192/200], Step [100/468], Loss: 0.00003142\n",
      "<Normal Phase> Epoch [192/200], Step [200/468], Loss: 0.00000534\n",
      "<Normal Phase> Epoch [192/200], Step [300/468], Loss: 0.00418665\n",
      "<Normal Phase> Epoch [192/200], Step [400/468], Loss: 0.00004543\n",
      "<Normal Phase> totally train at Epoch [192/200], Loss: 0.274837, Train Acc.: 99.9783\n",
      "**datalen=1737, k=11146.951778\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5100 %\n",
      "<Normal Phase> Epoch [193/200], Step [100/468], Loss: 0.00002623\n",
      "<Normal Phase> Epoch [193/200], Step [200/468], Loss: 0.00006712\n",
      "<Normal Phase> Epoch [193/200], Step [300/468], Loss: 0.00000136\n",
      "<Normal Phase> Epoch [193/200], Step [400/468], Loss: 0.00004255\n",
      "<Normal Phase> totally train at Epoch [193/200], Loss: 0.265420, Train Acc.: 99.9817\n",
      "**datalen=1906, k=11704.299367\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5400 %\n",
      "<Normal Phase> Epoch [194/200], Step [100/468], Loss: 0.00004518\n",
      "<Normal Phase> Epoch [194/200], Step [200/468], Loss: 0.00006751\n",
      "<Normal Phase> Epoch [194/200], Step [300/468], Loss: 0.00000363\n",
      "<Normal Phase> Epoch [194/200], Step [400/468], Loss: 0.00004373\n",
      "<Normal Phase> totally train at Epoch [194/200], Loss: 0.093614, Train Acc.: 99.9950\n",
      "**datalen=1658, k=12289.514335\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5000 %\n",
      "<Normal Phase> Epoch [195/200], Step [100/468], Loss: 0.00000399\n",
      "<Normal Phase> Epoch [195/200], Step [200/468], Loss: 0.00006812\n",
      "<Normal Phase> Epoch [195/200], Step [300/468], Loss: 0.00000000\n",
      "<Normal Phase> Epoch [195/200], Step [400/468], Loss: 0.00005772\n",
      "<Normal Phase> totally train at Epoch [195/200], Loss: 0.026206, Train Acc.: 100.0000\n",
      "**datalen=1463, k=12903.990052\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4700 %\n",
      "<Normal Phase> Epoch [196/200], Step [100/468], Loss: 0.00035661\n",
      "<Normal Phase> Epoch [196/200], Step [200/468], Loss: 0.00002509\n",
      "<Normal Phase> Epoch [196/200], Step [300/468], Loss: 0.00000301\n",
      "<Normal Phase> Epoch [196/200], Step [400/468], Loss: 0.00000824\n",
      "<Normal Phase> totally train at Epoch [196/200], Loss: 0.024963, Train Acc.: 100.0000\n",
      "**datalen=1332, k=13549.189554\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5500 %\n",
      "<Normal Phase> Epoch [197/200], Step [100/468], Loss: 0.00000611\n",
      "<Normal Phase> Epoch [197/200], Step [200/468], Loss: 0.00002391\n",
      "<Normal Phase> Epoch [197/200], Step [300/468], Loss: 0.00000073\n",
      "<Normal Phase> Epoch [197/200], Step [400/468], Loss: 0.00002705\n",
      "<Normal Phase> totally train at Epoch [197/200], Loss: 0.012742, Train Acc.: 100.0000\n",
      "**datalen=1150, k=14226.649032\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5100 %\n",
      "<Normal Phase> Epoch [198/200], Step [100/468], Loss: 0.00004471\n",
      "<Normal Phase> Epoch [198/200], Step [200/468], Loss: 0.00000574\n",
      "<Normal Phase> Epoch [198/200], Step [300/468], Loss: 0.00001486\n",
      "<Normal Phase> Epoch [198/200], Step [400/468], Loss: 0.00001459\n",
      "<Normal Phase> totally train at Epoch [198/200], Loss: 0.315514, Train Acc.: 99.9900\n",
      "**datalen=1398, k=14937.981484\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4600 %\n",
      "<Normal Phase> Epoch [199/200], Step [100/468], Loss: 0.00027961\n",
      "<Normal Phase> Epoch [199/200], Step [200/468], Loss: 0.00005310\n",
      "<Normal Phase> Epoch [199/200], Step [300/468], Loss: 0.00001309\n",
      "<Normal Phase> Epoch [199/200], Step [400/468], Loss: 0.00040613\n",
      "<Normal Phase> totally train at Epoch [199/200], Loss: 0.721146, Train Acc.: 99.9633\n",
      "**datalen=2339, k=15684.880558\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.4600 %\n",
      "<Normal Phase> Epoch [200/200], Step [100/468], Loss: 0.00000732\n",
      "<Normal Phase> Epoch [200/200], Step [200/468], Loss: 0.00000168\n",
      "<Normal Phase> Epoch [200/200], Step [300/468], Loss: 0.00030839\n",
      "<Normal Phase> Epoch [200/200], Step [400/468], Loss: 0.00005162\n",
      "<Normal Phase> totally train at Epoch [200/200], Loss: 0.141017, Train Acc.: 99.9933\n",
      "**datalen=1981, k=16469.124586\n",
      "**datalen=620\n",
      "grad_abs, loss_abs, del_w_abs:\n",
      "<Test Phase> Accuacy of the network on the 10,000 test images:98.5100 %\n",
      "\n",
      " => Best saved model test acc.: 98.7500 %\n"
     ]
    }
   ],
   "source": [
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_acc, train_loss, train_output_entropy, datalen = train(epoch)\n",
    "    test_acc, test_loss, test_output_entropy = test(epoch)\n",
    "    \n",
    "    if best_test_acc < test_acc:\n",
    "        model_save(net)\n",
    "        best_test_acc = test_acc\n",
    "    \n",
    "    train_trace['acc'].append(train_acc)\n",
    "    train_trace['loss'].append(train_loss)\n",
    "    for idx, key in enumerate(train_trace['output_entropy'].keys()):\n",
    "        train_trace['output_entropy'][key].append(train_output_entropy[idx])\n",
    "    train_trace['datalen_total'].append(datalen)\n",
    "    \n",
    "    test_trace['acc'].append(test_acc)\n",
    "    test_trace['loss'].append(test_loss)\n",
    "    for idx, key in enumerate(test_trace['output_entropy'].keys()):\n",
    "        test_trace['output_entropy'][key].append(test_output_entropy[idx])\n",
    "    \n",
    "print(\"\\n\",\"=> Best saved model test acc.: %.4f %%\" %(best_test_acc))\n",
    "train_trace['datalen_class'] = labels_distrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "saved_dir = './results/'\n",
    "net_name = net.__class__.__name__\n",
    "file_name = saved_dir + net_name +'2'+ '.pkl'\n",
    "\n",
    "with open(file_name, 'wb') as f:\n",
    "    pickle.dump(file_trace, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "i = range(10)\n",
    "print(i[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0.], grad_fn=<StackBackward>)\n",
      "tensor([1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n",
      "tensor([0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1.], grad_fn=<StackBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3f391d5cfec2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f9723ecd525d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mnot_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \"\"\"\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensor/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# TODO: make efficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = train_loader\n",
    "for inputs, labels, mask_tensor in i:\n",
    "    print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
       "           -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
       "            2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
       "            2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
       "            0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "            2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
       "           -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
       "           -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
       "            0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
       "            2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
       "            2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
       "            2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
       "            2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
       "            1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
       "            0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
       "            2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
       "            2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 0., 1., 1.], requires_grad=True) tensor(1., grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "b = torch.distributions.bernoulli.Bernoulli(probs=0.5)\n",
    "sample = b.sample(sample_shape=torch.Size([5]))\n",
    "sample.requires_grad_(True)\n",
    "print(sample, sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.,2.], requires_grad=True)\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Long for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-718970e71574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Long for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.,-5.,3.])\n",
    "b = torch.tensor([0,3,1])\n",
    "c = (a > b).to(device=device, dtype=torch.float32)\n",
    "print(c.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3333)\n"
     ]
    }
   ],
   "source": [
    "print(a.mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Test Phase> Accuacy of the network on the 10,000 test images:29.3300 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaeunl/anaconda3/envs/tensor/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_loss, test_output_entropy = test(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_distrib = {str(c):[] for c in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_distrib = dict(zip([str(c) for c in range(10)] ,[0 for _ in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 0,\n",
       " '2': 0,\n",
       " '3': 0,\n",
       " '4': 0,\n",
       " '5': 0,\n",
       " '6': 0,\n",
       " '7': 0,\n",
       " '8': 0,\n",
       " '9': 0}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-132-35607bbfd757>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-132-35607bbfd757>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    labels_distrib[str(x) for x in t] += 1\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2,3,4,5,6])\n",
    "b = torch.tensor([0,0,1,0,1], dtype=torch.uint8)\n",
    "t = a[b].data.cpu().numpy()\n",
    "#for x in t:\n",
    "#    labels_distrib[str(x)] += 1\n",
    "labels_distrib[str(x) for x in t] += 1\n",
    "\n",
    "print(labels_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-123-06d9756bcd0d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-123-06d9756bcd0d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    str(x) for x in range(1)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "str(x) for x in range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0, 0, 0, 0, 0], '1': [0, 0, 0, 0, 0], '2': [0, 0, 0, 0, 0], '3': [0, 0, 0, 0, 0], '4': [0, 0, 0, 0, 0], '5': [0, 0, 0, 0, 0], '6': [0, 0, 0, 0, 0], '7': [0, 0, 0, 0, 0], '8': [0, 0, 0, 0, 0], '9': [0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "labels_distrib = dict(zip([str(c) for c in range(10)] ,[[0 for _ in range(num_epochs)] for _ in range(10)]))\n",
    "print(labels_distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "5316\n",
      "5700\n",
      "5758\n",
      "5790\n",
      "5786\n",
      "5795\n",
      "5806\n",
      "5823\n",
      "5838\n",
      "5861\n",
      "5822\n",
      "5837\n",
      "5862\n",
      "5860\n",
      "5864\n",
      "5865\n",
      "5874\n",
      "5860\n",
      "5869\n",
      "5865\n",
      "5871\n",
      "5868\n",
      "5867\n",
      "5873\n",
      "5883\n",
      "5867\n",
      "5887\n",
      "5876\n",
      "5879\n",
      "5886\n",
      "5881\n",
      "5886\n",
      "5880\n",
      "5899\n",
      "5898\n",
      "5884\n",
      "5896\n",
      "5900\n",
      "5894\n",
      "5882\n",
      "5895\n",
      "5891\n",
      "5869\n",
      "5897\n",
      "5897\n",
      "5893\n",
      "5904\n",
      "5897\n",
      "5891\n",
      "5897\n",
      "5893\n",
      "5901\n",
      "5897\n",
      "5901\n",
      "5899\n",
      "5906\n",
      "5905\n",
      "5904\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5901\n",
      "5903\n",
      "5902\n",
      "5900\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5909\n",
      "5909\n",
      "5908\n",
      "5908\n",
      "5909\n",
      "5911\n",
      "5901\n",
      "5904\n",
      "5910\n",
      "5913\n",
      "5909\n",
      "5910\n",
      "5909\n",
      "5909\n",
      "5912\n",
      "5904\n",
      "5907\n",
      "5916\n",
      "5901\n",
      "5913\n",
      "5901\n",
      "5909\n",
      "5897\n",
      "5908\n",
      "5912\n",
      "5909\n",
      "5905\n",
      "5913\n",
      "5913\n",
      "5907\n",
      "5915\n",
      "5913\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5916\n",
      "5912\n",
      "5916\n",
      "5915\n",
      "5913\n",
      "5904\n",
      "5915\n",
      "5907\n",
      "5916\n",
      "5915\n",
      "5912\n",
      "5915\n",
      "5916\n",
      "5916\n",
      "5914\n",
      "5901\n",
      "5916\n",
      "5909\n",
      "5915\n",
      "5915\n",
      "5914\n",
      "5916\n",
      "5917\n",
      "5915\n",
      "5915\n",
      "5916\n",
      "5917\n",
      "5913\n",
      "5913\n",
      "5917\n",
      "5917\n",
      "5917\n",
      "5917\n",
      "5918\n",
      "5918\n",
      "5917\n",
      "5914\n",
      "5918\n",
      "5909\n",
      "5919\n",
      "5918\n",
      "5918\n",
      "5915\n",
      "5909\n",
      "5918\n",
      "5919\n",
      "5916\n",
      "5919\n",
      "5920\n",
      "5919\n",
      "5919\n",
      "5916\n",
      "5918\n",
      "5918\n",
      "5918\n",
      "5916\n",
      "5918\n",
      "5919\n",
      "5917\n",
      "5919\n",
      "5919\n",
      "5919\n",
      "5918\n",
      "5917\n",
      "5918\n",
      "5917\n",
      "5918\n",
      "5918\n",
      "5919\n",
      "5918\n",
      "5920\n",
      "5919\n",
      "5918\n",
      "5920\n",
      "5920\n",
      "5920\n",
      "5920\n",
      "5921\n",
      "5920\n",
      "5921\n",
      "5920\n",
      "5921\n",
      "5920\n",
      "5921\n",
      "5921\n",
      "5921\n",
      "5920\n",
      "5920\n",
      "5921\n",
      "5921\n",
      "5921\n",
      "5921\n",
      "5921\n",
      "5921\n",
      "5921\n",
      "5921\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    print(labels_distrib['0'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "l = [1,2,3,4,5]\n",
    "\n",
    "for i in l:\n",
    "    i=0\n",
    "    print(i)\n",
    "    \n",
    "    \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
