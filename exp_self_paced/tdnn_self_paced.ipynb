{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.utils.parser import set_train_config\n",
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict(dict(dataset=\"voxc1_fbank_xvector\", \n",
    "                              data_folder=\"/dataset/SV_sets/voxceleb12/feats/fbank64_vad/\",\n",
    "                              input_frames=400, splice_frames=[400, 400], stride_frames=1, \n",
    "                              input_format='fbank', input_dim=65, random_clip=False,\n",
    "                              n_epochs=200, lrs=[0.1, 0.01], lr_schedule=[20], seed=1337,\n",
    "                              no_eer=False, batch_size=128,\n",
    "                              gpu_no=[0], cuda=True, num_workers=4,\n",
    "                              arch=\"tdnn_conv\", loss=\"softmax\",\n",
    "                             ))\n",
    "config = set_train_config(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.feat_dataset import FeatDataset\n",
    "dev_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_si.csv\")\n",
    "dev_df = dev_df.sample(frac=1.0)\n",
    "dev_train_df = dev_df[dev_df.set == 'train']\n",
    "dev_val_df = dev_df[dev_df.set == 'val']\n",
    "eval_df = pd.read_csv(\"/dataset/SV_sets/voxceleb1/dataframes/voxc1_eval.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_train_dataset = FeatDataset.read_df(config, dev_train_df, 'train')\n",
    "dev_val_dataset = FeatDataset.read_df(config, dev_val_df, 'test')\n",
    "eval_dataset = FeatDataset.read_df(config, eval_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sv_system.data.dataloader import init_default_loader \n",
    "dev_train_dataloader = init_default_loader(config, dev_train_dataset, shuffle=True, var_len=False) \n",
    "dev_val_dataloader = init_default_loader(config, dev_val_dataset, shuffle=False, var_len=False) \n",
    "eval_dataloader = init_default_loader(config, eval_dataset, shuffle=False, var_len=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdnn_models import tdnn_xvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = tdnn_xvector(config, 512, n_labels=len(dev_df.label.unique()))\n",
    "# saved_model = torch.load(\"trained_models/vox1_dev_tdnn_xvector_held_out.pt\")\n",
    "# model.load_state_dict(saved_model)\n",
    "\n",
    "if not config['no_cuda']:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter(\"logs/self_paced_voxc1_si\")\n",
    "model_path = \"trained_models/voxc1_si_tdnn_xvector_spl.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/1047\t Loss 0.039616\n",
      "Batch 200/1047\t Loss 0.035465\n",
      "Batch 300/1047\t Loss 0.033290\n",
      "Batch 400/1047\t Loss 0.031453\n",
      "Batch 500/1047\t Loss 0.029823\n",
      "Batch 600/1047\t Loss 0.028586\n",
      "Batch 700/1047\t Loss 0.027485\n",
      "Batch 800/1047\t Loss 0.026600\n",
      "Batch 900/1047\t Loss 0.025819\n",
      "Batch 1000/1047\t Loss 0.025098\n",
      "epoch #16, train loss: 0.0248, train acc: 0.0403\n",
      "sel_samples/total: 24544/134000\n",
      "epoch #16, val loss: 0.0886, val acc: 0.0148\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.015424\n",
      "Batch 200/1047\t Loss 0.015539\n",
      "Batch 300/1047\t Loss 0.015396\n",
      "Batch 400/1047\t Loss 0.015131\n",
      "Batch 500/1047\t Loss 0.014705\n",
      "Batch 600/1047\t Loss 0.014527\n",
      "Batch 700/1047\t Loss 0.014252\n",
      "Batch 800/1047\t Loss 0.014034\n",
      "Batch 900/1047\t Loss 0.013817\n",
      "Batch 1000/1047\t Loss 0.013578\n",
      "epoch #17, train loss: 0.0135, train acc: 0.0933\n",
      "sel_samples/total: 24714/134000\n",
      "epoch #17, val loss: 0.0897, val acc: 0.0269\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.009336\n",
      "Batch 200/1047\t Loss 0.009310\n",
      "Batch 300/1047\t Loss 0.009324\n",
      "Batch 400/1047\t Loss 0.009380\n",
      "Batch 500/1047\t Loss 0.009286\n",
      "Batch 600/1047\t Loss 0.009230\n",
      "Batch 700/1047\t Loss 0.009158\n",
      "Batch 800/1047\t Loss 0.009042\n",
      "Batch 900/1047\t Loss 0.008964\n",
      "Batch 1000/1047\t Loss 0.008895\n",
      "epoch #18, train loss: 0.0088, train acc: 0.1213\n",
      "sel_samples/total: 24680/134000\n",
      "epoch #18, val loss: 0.1011, val acc: 0.0325\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.005959\n",
      "Batch 200/1047\t Loss 0.006225\n",
      "Batch 300/1047\t Loss 0.006395\n",
      "Batch 400/1047\t Loss 0.006414\n",
      "Batch 500/1047\t Loss 0.006457\n",
      "Batch 600/1047\t Loss 0.006468\n",
      "Batch 700/1047\t Loss 0.006389\n",
      "Batch 800/1047\t Loss 0.006367\n",
      "Batch 900/1047\t Loss 0.006367\n",
      "Batch 1000/1047\t Loss 0.006333\n",
      "epoch #19, train loss: 0.0063, train acc: 0.1379\n",
      "sel_samples/total: 24663/134000\n",
      "epoch #19, val loss: 0.1069, val acc: 0.0432\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.004827\n",
      "Batch 200/1047\t Loss 0.004950\n",
      "Batch 300/1047\t Loss 0.004992\n",
      "Batch 400/1047\t Loss 0.004885\n",
      "Batch 500/1047\t Loss 0.004779\n",
      "Batch 600/1047\t Loss 0.004715\n",
      "Batch 700/1047\t Loss 0.004732\n",
      "Batch 800/1047\t Loss 0.004688\n",
      "Batch 900/1047\t Loss 0.004677\n",
      "Batch 1000/1047\t Loss 0.004670\n",
      "epoch #20, train loss: 0.0047, train acc: 0.1478\n",
      "sel_samples/total: 24613/134000\n",
      "epoch #20, val loss: 0.1140, val acc: 0.0447\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.003521\n",
      "Batch 200/1047\t Loss 0.003688\n",
      "Batch 300/1047\t Loss 0.003991\n",
      "Batch 400/1047\t Loss 0.003947\n",
      "Batch 500/1047\t Loss 0.003905\n",
      "Batch 600/1047\t Loss 0.003888\n",
      "Batch 700/1047\t Loss 0.003911\n",
      "Batch 800/1047\t Loss 0.003886\n",
      "Batch 900/1047\t Loss 0.003836\n",
      "Batch 1000/1047\t Loss 0.003824\n",
      "epoch #21, train loss: 0.0038, train acc: 0.1540\n",
      "sel_samples/total: 24605/134000\n",
      "epoch #21, val loss: 0.1080, val acc: 0.0497\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.002806\n",
      "Batch 200/1047\t Loss 0.002803\n",
      "Batch 300/1047\t Loss 0.002790\n",
      "Batch 400/1047\t Loss 0.002954\n",
      "Batch 500/1047\t Loss 0.003052\n",
      "Batch 600/1047\t Loss 0.003110\n",
      "Batch 700/1047\t Loss 0.003110\n",
      "Batch 800/1047\t Loss 0.003139\n",
      "Batch 900/1047\t Loss 0.003154\n",
      "Batch 1000/1047\t Loss 0.003190\n",
      "epoch #22, train loss: 0.0032, train acc: 0.1581\n",
      "sel_samples/total: 24552/134000\n",
      "epoch #22, val loss: 0.1105, val acc: 0.0507\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.002220\n",
      "Batch 200/1047\t Loss 0.002126\n",
      "Batch 300/1047\t Loss 0.002344\n",
      "Batch 400/1047\t Loss 0.002471\n",
      "Batch 500/1047\t Loss 0.002545\n",
      "Batch 600/1047\t Loss 0.002602\n",
      "Batch 700/1047\t Loss 0.002627\n",
      "Batch 800/1047\t Loss 0.002634\n",
      "Batch 900/1047\t Loss 0.002619\n",
      "Batch 1000/1047\t Loss 0.002627\n",
      "epoch #23, train loss: 0.0027, train acc: 0.1619\n",
      "sel_samples/total: 24570/134000\n",
      "epoch #23, val loss: 0.1059, val acc: 0.0509\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.002298\n",
      "Batch 200/1047\t Loss 0.002202\n",
      "Batch 300/1047\t Loss 0.002156\n",
      "Batch 400/1047\t Loss 0.002238\n",
      "Batch 500/1047\t Loss 0.002271\n",
      "Batch 600/1047\t Loss 0.002287\n",
      "Batch 700/1047\t Loss 0.002294\n",
      "Batch 800/1047\t Loss 0.002306\n",
      "Batch 900/1047\t Loss 0.002333\n",
      "Batch 1000/1047\t Loss 0.002359\n",
      "epoch #24, train loss: 0.0024, train acc: 0.1638\n",
      "sel_samples/total: 24501/134000\n",
      "epoch #24, val loss: 0.1190, val acc: 0.0514\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001850\n",
      "Batch 200/1047\t Loss 0.001727\n",
      "Batch 300/1047\t Loss 0.001925\n",
      "Batch 400/1047\t Loss 0.001930\n",
      "Batch 500/1047\t Loss 0.002018\n",
      "Batch 600/1047\t Loss 0.002176\n",
      "Batch 700/1047\t Loss 0.002204\n",
      "Batch 800/1047\t Loss 0.002181\n",
      "Batch 900/1047\t Loss 0.002212\n",
      "Batch 1000/1047\t Loss 0.002253\n",
      "epoch #25, train loss: 0.0023, train acc: 0.1635\n",
      "sel_samples/total: 24401/134000\n",
      "epoch #25, val loss: 0.1142, val acc: 0.0529\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001744\n",
      "Batch 200/1047\t Loss 0.001644\n",
      "Batch 300/1047\t Loss 0.001689\n",
      "Batch 400/1047\t Loss 0.001678\n",
      "Batch 500/1047\t Loss 0.001656\n",
      "Batch 600/1047\t Loss 0.001657\n",
      "Batch 700/1047\t Loss 0.001702\n",
      "Batch 800/1047\t Loss 0.001762\n",
      "Batch 900/1047\t Loss 0.001814\n",
      "Batch 1000/1047\t Loss 0.001826\n",
      "epoch #26, train loss: 0.0018, train acc: 0.1664\n",
      "sel_samples/total: 24395/134000\n",
      "epoch #26, val loss: 0.1158, val acc: 0.0618\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001423\n",
      "Batch 200/1047\t Loss 0.001390\n",
      "Batch 300/1047\t Loss 0.001431\n",
      "Batch 400/1047\t Loss 0.001461\n",
      "Batch 500/1047\t Loss 0.001558\n",
      "Batch 600/1047\t Loss 0.001574\n",
      "Batch 700/1047\t Loss 0.001590\n",
      "Batch 800/1047\t Loss 0.001618\n",
      "Batch 900/1047\t Loss 0.001657\n",
      "Batch 1000/1047\t Loss 0.001680\n",
      "epoch #27, train loss: 0.0017, train acc: 0.1670\n",
      "sel_samples/total: 24347/134000\n",
      "epoch #27, val loss: 0.1217, val acc: 0.0511\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001457\n",
      "Batch 200/1047\t Loss 0.001447\n",
      "Batch 300/1047\t Loss 0.001432\n",
      "Batch 400/1047\t Loss 0.001460\n",
      "Batch 500/1047\t Loss 0.001561\n",
      "Batch 600/1047\t Loss 0.001606\n",
      "Batch 700/1047\t Loss 0.001643\n",
      "Batch 800/1047\t Loss 0.001641\n",
      "Batch 900/1047\t Loss 0.001637\n",
      "Batch 1000/1047\t Loss 0.001682\n",
      "epoch #28, train loss: 0.0017, train acc: 0.1650\n",
      "sel_samples/total: 24131/134000\n",
      "epoch #28, val loss: 0.1238, val acc: 0.0518\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001475\n",
      "Batch 200/1047\t Loss 0.001290\n",
      "Batch 300/1047\t Loss 0.001279\n",
      "Batch 400/1047\t Loss 0.001268\n",
      "Batch 500/1047\t Loss 0.001272\n",
      "Batch 600/1047\t Loss 0.001283\n",
      "Batch 700/1047\t Loss 0.001285\n",
      "Batch 800/1047\t Loss 0.001322\n",
      "Batch 900/1047\t Loss 0.001363\n",
      "Batch 1000/1047\t Loss 0.001404\n",
      "epoch #29, train loss: 0.0014, train acc: 0.1668\n",
      "sel_samples/total: 24089/134000\n",
      "epoch #29, val loss: 0.1215, val acc: 0.0522\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001553\n",
      "Batch 200/1047\t Loss 0.001603\n",
      "Batch 300/1047\t Loss 0.001568\n",
      "Batch 400/1047\t Loss 0.001610\n",
      "Batch 500/1047\t Loss 0.001639\n",
      "Batch 600/1047\t Loss 0.001634\n",
      "Batch 700/1047\t Loss 0.001588\n",
      "Batch 800/1047\t Loss 0.001567\n",
      "Batch 900/1047\t Loss 0.001556\n",
      "Batch 1000/1047\t Loss 0.001545\n",
      "epoch #30, train loss: 0.0015, train acc: 0.1640\n",
      "sel_samples/total: 23843/134000\n",
      "epoch #30, val loss: 0.1271, val acc: 0.0561\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001115\n",
      "Batch 200/1047\t Loss 0.001177\n",
      "Batch 300/1047\t Loss 0.001127\n",
      "Batch 400/1047\t Loss 0.001111\n",
      "Batch 500/1047\t Loss 0.001114\n",
      "Batch 600/1047\t Loss 0.001171\n",
      "Batch 700/1047\t Loss 0.001212\n",
      "Batch 800/1047\t Loss 0.001237\n",
      "Batch 900/1047\t Loss 0.001249\n",
      "Batch 1000/1047\t Loss 0.001266\n",
      "epoch #31, train loss: 0.0013, train acc: 0.1662\n",
      "sel_samples/total: 23862/134000\n",
      "epoch #31, val loss: 0.1295, val acc: 0.0545\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001138\n",
      "Batch 200/1047\t Loss 0.001177\n",
      "Batch 300/1047\t Loss 0.001237\n",
      "Batch 400/1047\t Loss 0.001235\n",
      "Batch 500/1047\t Loss 0.001235\n",
      "Batch 600/1047\t Loss 0.001216\n",
      "Batch 700/1047\t Loss 0.001202\n",
      "Batch 800/1047\t Loss 0.001201\n",
      "Batch 900/1047\t Loss 0.001201\n",
      "Batch 1000/1047\t Loss 0.001218\n",
      "epoch #32, train loss: 0.0012, train acc: 0.1659\n",
      "sel_samples/total: 23787/134000\n",
      "epoch #32, val loss: 0.1327, val acc: 0.0587\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001026\n",
      "Batch 200/1047\t Loss 0.001019\n",
      "Batch 300/1047\t Loss 0.001079\n",
      "Batch 400/1047\t Loss 0.001054\n",
      "Batch 500/1047\t Loss 0.001032\n",
      "Batch 600/1047\t Loss 0.001025\n",
      "Batch 700/1047\t Loss 0.001022\n",
      "Batch 800/1047\t Loss 0.001043\n",
      "Batch 900/1047\t Loss 0.001057\n",
      "Batch 1000/1047\t Loss 0.001066\n",
      "epoch #33, train loss: 0.0011, train acc: 0.1669\n",
      "sel_samples/total: 23738/134000\n",
      "epoch #33, val loss: 0.1360, val acc: 0.0588\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000823\n",
      "Batch 200/1047\t Loss 0.000838\n",
      "Batch 300/1047\t Loss 0.000874\n",
      "Batch 400/1047\t Loss 0.000919\n",
      "Batch 500/1047\t Loss 0.000951\n",
      "Batch 600/1047\t Loss 0.000994\n",
      "Batch 700/1047\t Loss 0.000987\n",
      "Batch 800/1047\t Loss 0.000995\n",
      "Batch 900/1047\t Loss 0.001024\n",
      "Batch 1000/1047\t Loss 0.001043\n",
      "epoch #34, train loss: 0.0011, train acc: 0.1642\n",
      "sel_samples/total: 23359/134000\n",
      "epoch #34, val loss: 0.1440, val acc: 0.0502\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001038\n",
      "Batch 200/1047\t Loss 0.001071\n",
      "Batch 300/1047\t Loss 0.001060\n",
      "Batch 400/1047\t Loss 0.001066\n",
      "Batch 500/1047\t Loss 0.001062\n",
      "Batch 600/1047\t Loss 0.001046\n",
      "Batch 700/1047\t Loss 0.001062\n",
      "Batch 800/1047\t Loss 0.001069\n",
      "Batch 900/1047\t Loss 0.001084\n",
      "Batch 1000/1047\t Loss 0.001097\n",
      "epoch #35, train loss: 0.0011, train acc: 0.1600\n",
      "sel_samples/total: 22890/134000\n",
      "epoch #35, val loss: 0.1456, val acc: 0.0497\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.001161\n",
      "Batch 200/1047\t Loss 0.001127\n",
      "Batch 300/1047\t Loss 0.001065\n",
      "Batch 400/1047\t Loss 0.001035\n",
      "Batch 500/1047\t Loss 0.001009\n",
      "Batch 600/1047\t Loss 0.001026\n",
      "Batch 700/1047\t Loss 0.001029\n",
      "Batch 800/1047\t Loss 0.001004\n",
      "Batch 900/1047\t Loss 0.000992\n",
      "Batch 1000/1047\t Loss 0.000996\n",
      "epoch #36, train loss: 0.0010, train acc: 0.1602\n",
      "sel_samples/total: 22758/134000\n",
      "epoch #36, val loss: 0.1650, val acc: 0.0442\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000893\n",
      "Batch 200/1047\t Loss 0.000885\n",
      "Batch 300/1047\t Loss 0.000850\n",
      "Batch 400/1047\t Loss 0.000831\n",
      "Batch 500/1047\t Loss 0.000867\n",
      "Batch 600/1047\t Loss 0.000866\n",
      "Batch 700/1047\t Loss 0.000863\n",
      "Batch 800/1047\t Loss 0.000873\n",
      "Batch 900/1047\t Loss 0.000883\n",
      "Batch 1000/1047\t Loss 0.000889\n",
      "epoch #37, train loss: 0.0009, train acc: 0.1607\n",
      "sel_samples/total: 22735/134000\n",
      "epoch #37, val loss: 0.1476, val acc: 0.0529\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000989\n",
      "Batch 200/1047\t Loss 0.000945\n",
      "Batch 300/1047\t Loss 0.000864\n",
      "Batch 400/1047\t Loss 0.000842\n",
      "Batch 500/1047\t Loss 0.000825\n",
      "Batch 600/1047\t Loss 0.000809\n",
      "Batch 700/1047\t Loss 0.000828\n",
      "Batch 800/1047\t Loss 0.000837\n",
      "Batch 900/1047\t Loss 0.000822\n",
      "Batch 1000/1047\t Loss 0.000830\n",
      "epoch #38, train loss: 0.0008, train acc: 0.1603\n",
      "sel_samples/total: 22634/134000\n",
      "epoch #38, val loss: 0.1476, val acc: 0.0525\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000793\n",
      "Batch 200/1047\t Loss 0.000804\n",
      "Batch 300/1047\t Loss 0.000798\n",
      "Batch 400/1047\t Loss 0.000771\n",
      "Batch 500/1047\t Loss 0.000772\n",
      "Batch 600/1047\t Loss 0.000795\n",
      "Batch 700/1047\t Loss 0.000794\n",
      "Batch 800/1047\t Loss 0.000779\n",
      "Batch 900/1047\t Loss 0.000779\n",
      "Batch 1000/1047\t Loss 0.000784\n",
      "epoch #39, train loss: 0.0008, train acc: 0.1594\n",
      "sel_samples/total: 22429/134000\n",
      "epoch #39, val loss: 0.1524, val acc: 0.0529\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000747\n",
      "Batch 200/1047\t Loss 0.000740\n",
      "Batch 300/1047\t Loss 0.000724\n",
      "Batch 400/1047\t Loss 0.000716\n",
      "Batch 500/1047\t Loss 0.000713\n",
      "Batch 600/1047\t Loss 0.000709\n",
      "Batch 700/1047\t Loss 0.000692\n",
      "Batch 800/1047\t Loss 0.000688\n",
      "Batch 900/1047\t Loss 0.000703\n",
      "Batch 1000/1047\t Loss 0.000724\n",
      "epoch #40, train loss: 0.0007, train acc: 0.1568\n",
      "sel_samples/total: 21987/134000\n",
      "epoch #40, val loss: 0.1523, val acc: 0.0515\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000864\n",
      "Batch 200/1047\t Loss 0.000836\n",
      "Batch 300/1047\t Loss 0.000753\n",
      "Batch 400/1047\t Loss 0.000719\n",
      "Batch 500/1047\t Loss 0.000686\n",
      "Batch 600/1047\t Loss 0.000660\n",
      "Batch 700/1047\t Loss 0.000658\n",
      "Batch 800/1047\t Loss 0.000654\n",
      "Batch 900/1047\t Loss 0.000656\n",
      "Batch 1000/1047\t Loss 0.000655\n",
      "epoch #41, train loss: 0.0007, train acc: 0.1579\n",
      "sel_samples/total: 22070/134000\n",
      "epoch #41, val loss: 0.1570, val acc: 0.0542\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000607\n",
      "Batch 200/1047\t Loss 0.000617\n",
      "Batch 300/1047\t Loss 0.000592\n",
      "Batch 400/1047\t Loss 0.000565\n",
      "Batch 500/1047\t Loss 0.000586\n",
      "Batch 600/1047\t Loss 0.000599\n",
      "Batch 700/1047\t Loss 0.000588\n",
      "Batch 800/1047\t Loss 0.000601\n",
      "Batch 900/1047\t Loss 0.000612\n",
      "Batch 1000/1047\t Loss 0.000613\n",
      "epoch #42, train loss: 0.0006, train acc: 0.1591\n",
      "sel_samples/total: 22168/134000\n",
      "epoch #42, val loss: 0.1530, val acc: 0.0595\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000610\n",
      "Batch 200/1047\t Loss 0.000572\n",
      "Batch 300/1047\t Loss 0.000559\n",
      "Batch 400/1047\t Loss 0.000550\n",
      "Batch 500/1047\t Loss 0.000549\n",
      "Batch 600/1047\t Loss 0.000568\n",
      "Batch 700/1047\t Loss 0.000575\n",
      "Batch 800/1047\t Loss 0.000578\n",
      "Batch 900/1047\t Loss 0.000579\n",
      "Batch 1000/1047\t Loss 0.000586\n",
      "epoch #43, train loss: 0.0006, train acc: 0.1585\n",
      "sel_samples/total: 22071/134000\n",
      "epoch #43, val loss: 0.1647, val acc: 0.0475\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000467\n",
      "Batch 200/1047\t Loss 0.000487\n",
      "Batch 300/1047\t Loss 0.000484\n",
      "Batch 400/1047\t Loss 0.000479\n",
      "Batch 500/1047\t Loss 0.000469\n",
      "Batch 600/1047\t Loss 0.000478\n",
      "Batch 700/1047\t Loss 0.000496\n",
      "Batch 800/1047\t Loss 0.000503\n",
      "Batch 900/1047\t Loss 0.000515\n",
      "Batch 1000/1047\t Loss 0.000515\n",
      "epoch #44, train loss: 0.0005, train acc: 0.1579\n",
      "sel_samples/total: 21856/134000\n",
      "epoch #44, val loss: 0.1704, val acc: 0.0478\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000525\n",
      "Batch 200/1047\t Loss 0.000479\n",
      "Batch 300/1047\t Loss 0.000450\n",
      "Batch 400/1047\t Loss 0.000443\n",
      "Batch 500/1047\t Loss 0.000437\n",
      "Batch 600/1047\t Loss 0.000444\n",
      "Batch 700/1047\t Loss 0.000438\n",
      "Batch 800/1047\t Loss 0.000432\n",
      "Batch 900/1047\t Loss 0.000438\n",
      "Batch 1000/1047\t Loss 0.000438\n",
      "epoch #45, train loss: 0.0004, train acc: 0.1609\n",
      "sel_samples/total: 22153/134000\n",
      "epoch #45, val loss: 0.1528, val acc: 0.0546\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000466\n",
      "Batch 200/1047\t Loss 0.000463\n",
      "Batch 300/1047\t Loss 0.000467\n",
      "Batch 400/1047\t Loss 0.000461\n",
      "Batch 500/1047\t Loss 0.000454\n",
      "Batch 600/1047\t Loss 0.000460\n",
      "Batch 700/1047\t Loss 0.000458\n",
      "Batch 800/1047\t Loss 0.000450\n",
      "Batch 900/1047\t Loss 0.000451\n",
      "Batch 1000/1047\t Loss 0.000457\n",
      "epoch #46, train loss: 0.0005, train acc: 0.1585\n",
      "sel_samples/total: 21821/134000\n",
      "epoch #46, val loss: 0.1693, val acc: 0.0456\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000540\n",
      "Batch 200/1047\t Loss 0.000508\n",
      "Batch 300/1047\t Loss 0.000475\n",
      "Batch 400/1047\t Loss 0.000475\n",
      "Batch 500/1047\t Loss 0.000484\n",
      "Batch 600/1047\t Loss 0.000498\n",
      "Batch 700/1047\t Loss 0.000492\n",
      "Batch 800/1047\t Loss 0.000487\n",
      "Batch 900/1047\t Loss 0.000478\n",
      "Batch 1000/1047\t Loss 0.000475\n",
      "epoch #47, train loss: 0.0005, train acc: 0.1552\n",
      "sel_samples/total: 21410/134000\n",
      "epoch #47, val loss: 0.1548, val acc: 0.0527\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000344\n",
      "Batch 200/1047\t Loss 0.000368\n",
      "Batch 300/1047\t Loss 0.000390\n",
      "Batch 400/1047\t Loss 0.000385\n",
      "Batch 500/1047\t Loss 0.000387\n",
      "Batch 600/1047\t Loss 0.000388\n",
      "Batch 700/1047\t Loss 0.000390\n",
      "Batch 800/1047\t Loss 0.000393\n",
      "Batch 900/1047\t Loss 0.000398\n",
      "Batch 1000/1047\t Loss 0.000404\n",
      "epoch #48, train loss: 0.0004, train acc: 0.1571\n",
      "sel_samples/total: 21570/134000\n",
      "epoch #48, val loss: 0.1767, val acc: 0.0428\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000387\n",
      "Batch 200/1047\t Loss 0.000390\n",
      "Batch 300/1047\t Loss 0.000432\n",
      "Batch 400/1047\t Loss 0.000429\n",
      "Batch 500/1047\t Loss 0.000436\n",
      "Batch 600/1047\t Loss 0.000426\n",
      "Batch 700/1047\t Loss 0.000420\n",
      "Batch 800/1047\t Loss 0.000406\n",
      "Batch 900/1047\t Loss 0.000402\n",
      "Batch 1000/1047\t Loss 0.000401\n",
      "epoch #49, train loss: 0.0004, train acc: 0.1548\n",
      "sel_samples/total: 21214/134000\n",
      "epoch #49, val loss: 0.1549, val acc: 0.0567\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000373\n",
      "Batch 200/1047\t Loss 0.000395\n",
      "Batch 300/1047\t Loss 0.000378\n",
      "Batch 400/1047\t Loss 0.000366\n",
      "Batch 500/1047\t Loss 0.000372\n",
      "Batch 600/1047\t Loss 0.000359\n",
      "Batch 700/1047\t Loss 0.000362\n",
      "Batch 800/1047\t Loss 0.000363\n",
      "Batch 900/1047\t Loss 0.000353\n",
      "Batch 1000/1047\t Loss 0.000355\n",
      "epoch #50, train loss: 0.0004, train acc: 0.1566\n",
      "sel_samples/total: 21411/134000\n",
      "epoch #50, val loss: 0.1502, val acc: 0.0568\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000294\n",
      "Batch 200/1047\t Loss 0.000284\n",
      "Batch 300/1047\t Loss 0.000288\n",
      "Batch 400/1047\t Loss 0.000295\n",
      "Batch 500/1047\t Loss 0.000301\n",
      "Batch 600/1047\t Loss 0.000316\n",
      "Batch 700/1047\t Loss 0.000328\n",
      "Batch 800/1047\t Loss 0.000339\n",
      "Batch 900/1047\t Loss 0.000343\n",
      "Batch 1000/1047\t Loss 0.000342\n",
      "epoch #51, train loss: 0.0003, train acc: 0.1579\n",
      "sel_samples/total: 21543/134000\n",
      "epoch #51, val loss: 0.1612, val acc: 0.0546\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000312\n",
      "Batch 200/1047\t Loss 0.000299\n",
      "Batch 300/1047\t Loss 0.000302\n",
      "Batch 400/1047\t Loss 0.000297\n",
      "Batch 500/1047\t Loss 0.000297\n",
      "Batch 600/1047\t Loss 0.000304\n",
      "Batch 700/1047\t Loss 0.000310\n",
      "Batch 800/1047\t Loss 0.000306\n",
      "Batch 900/1047\t Loss 0.000305\n",
      "Batch 1000/1047\t Loss 0.000307\n",
      "epoch #52, train loss: 0.0003, train acc: 0.1598\n",
      "sel_samples/total: 21737/134000\n",
      "epoch #52, val loss: 0.1546, val acc: 0.0540\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000256\n",
      "Batch 200/1047\t Loss 0.000250\n",
      "Batch 300/1047\t Loss 0.000249\n",
      "Batch 400/1047\t Loss 0.000258\n",
      "Batch 500/1047\t Loss 0.000266\n",
      "Batch 600/1047\t Loss 0.000275\n",
      "Batch 700/1047\t Loss 0.000286\n",
      "Batch 800/1047\t Loss 0.000294\n",
      "Batch 900/1047\t Loss 0.000300\n",
      "Batch 1000/1047\t Loss 0.000301\n",
      "epoch #53, train loss: 0.0003, train acc: 0.1586\n",
      "sel_samples/total: 21553/134000\n",
      "epoch #53, val loss: 0.1520, val acc: 0.0533\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000250\n",
      "Batch 200/1047\t Loss 0.000243\n",
      "Batch 300/1047\t Loss 0.000231\n",
      "Batch 400/1047\t Loss 0.000229\n",
      "Batch 500/1047\t Loss 0.000237\n",
      "Batch 600/1047\t Loss 0.000246\n",
      "Batch 700/1047\t Loss 0.000256\n",
      "Batch 800/1047\t Loss 0.000262\n",
      "Batch 900/1047\t Loss 0.000270\n",
      "Batch 1000/1047\t Loss 0.000273\n",
      "epoch #54, train loss: 0.0003, train acc: 0.1599\n",
      "sel_samples/total: 21660/134000\n",
      "epoch #54, val loss: 0.1536, val acc: 0.0517\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000284\n",
      "Batch 200/1047\t Loss 0.000267\n",
      "Batch 300/1047\t Loss 0.000279\n",
      "Batch 400/1047\t Loss 0.000287\n",
      "Batch 500/1047\t Loss 0.000280\n",
      "Batch 600/1047\t Loss 0.000278\n",
      "Batch 700/1047\t Loss 0.000285\n",
      "Batch 800/1047\t Loss 0.000290\n",
      "Batch 900/1047\t Loss 0.000291\n",
      "Batch 1000/1047\t Loss 0.000286\n",
      "epoch #55, train loss: 0.0003, train acc: 0.1585\n",
      "sel_samples/total: 21471/134000\n",
      "epoch #55, val loss: 0.1452, val acc: 0.0539\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000223\n",
      "Batch 200/1047\t Loss 0.000241\n",
      "Batch 300/1047\t Loss 0.000254\n",
      "Batch 400/1047\t Loss 0.000258\n",
      "Batch 500/1047\t Loss 0.000247\n",
      "Batch 600/1047\t Loss 0.000238\n",
      "Batch 700/1047\t Loss 0.000240\n",
      "Batch 800/1047\t Loss 0.000244\n",
      "Batch 900/1047\t Loss 0.000245\n",
      "Batch 1000/1047\t Loss 0.000246\n",
      "epoch #56, train loss: 0.0002, train acc: 0.1607\n",
      "sel_samples/total: 21717/134000\n",
      "epoch #56, val loss: 0.1411, val acc: 0.0597\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000194\n",
      "Batch 200/1047\t Loss 0.000184\n",
      "Batch 300/1047\t Loss 0.000187\n",
      "Batch 400/1047\t Loss 0.000199\n",
      "Batch 500/1047\t Loss 0.000203\n",
      "Batch 600/1047\t Loss 0.000210\n",
      "Batch 700/1047\t Loss 0.000211\n",
      "Batch 800/1047\t Loss 0.000220\n",
      "Batch 900/1047\t Loss 0.000234\n",
      "Batch 1000/1047\t Loss 0.000238\n",
      "epoch #57, train loss: 0.0002, train acc: 0.1621\n",
      "sel_samples/total: 21886/134000\n",
      "epoch #57, val loss: 0.1388, val acc: 0.0598\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000235\n",
      "Batch 200/1047\t Loss 0.000245\n",
      "Batch 300/1047\t Loss 0.000234\n",
      "Batch 400/1047\t Loss 0.000226\n",
      "Batch 500/1047\t Loss 0.000216\n",
      "Batch 600/1047\t Loss 0.000209\n",
      "Batch 700/1047\t Loss 0.000219\n",
      "Batch 800/1047\t Loss 0.000219\n",
      "Batch 900/1047\t Loss 0.000219\n",
      "Batch 1000/1047\t Loss 0.000217\n",
      "epoch #58, train loss: 0.0002, train acc: 0.1630\n",
      "sel_samples/total: 21923/134000\n",
      "epoch #58, val loss: 0.1381, val acc: 0.0616\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000191\n",
      "Batch 200/1047\t Loss 0.000195\n",
      "Batch 300/1047\t Loss 0.000202\n",
      "Batch 400/1047\t Loss 0.000191\n",
      "Batch 500/1047\t Loss 0.000187\n",
      "Batch 600/1047\t Loss 0.000187\n",
      "Batch 700/1047\t Loss 0.000202\n",
      "Batch 800/1047\t Loss 0.000203\n",
      "Batch 900/1047\t Loss 0.000205\n",
      "Batch 1000/1047\t Loss 0.000214\n",
      "epoch #59, train loss: 0.0002, train acc: 0.1630\n",
      "sel_samples/total: 21915/134000\n",
      "epoch #59, val loss: 0.1451, val acc: 0.0525\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000229\n",
      "Batch 200/1047\t Loss 0.000234\n",
      "Batch 300/1047\t Loss 0.000223\n",
      "Batch 400/1047\t Loss 0.000209\n",
      "Batch 500/1047\t Loss 0.000216\n",
      "Batch 600/1047\t Loss 0.000219\n",
      "Batch 700/1047\t Loss 0.000217\n",
      "Batch 800/1047\t Loss 0.000213\n",
      "Batch 900/1047\t Loss 0.000210\n",
      "Batch 1000/1047\t Loss 0.000209\n",
      "epoch #60, train loss: 0.0002, train acc: 0.1621\n",
      "sel_samples/total: 21756/134000\n",
      "epoch #60, val loss: 0.1302, val acc: 0.0633\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000134\n",
      "Batch 200/1047\t Loss 0.000154\n",
      "Batch 300/1047\t Loss 0.000162\n",
      "Batch 400/1047\t Loss 0.000166\n",
      "Batch 500/1047\t Loss 0.000176\n",
      "Batch 600/1047\t Loss 0.000181\n",
      "Batch 700/1047\t Loss 0.000188\n",
      "Batch 800/1047\t Loss 0.000192\n",
      "Batch 900/1047\t Loss 0.000193\n",
      "Batch 1000/1047\t Loss 0.000197\n",
      "epoch #61, train loss: 0.0002, train acc: 0.1628\n",
      "sel_samples/total: 21810/134000\n",
      "epoch #61, val loss: 0.1382, val acc: 0.0550\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000216\n",
      "Batch 200/1047\t Loss 0.000200\n",
      "Batch 300/1047\t Loss 0.000206\n",
      "Batch 400/1047\t Loss 0.000216\n",
      "Batch 500/1047\t Loss 0.000204\n",
      "Batch 600/1047\t Loss 0.000202\n",
      "Batch 700/1047\t Loss 0.000199\n",
      "Batch 800/1047\t Loss 0.000201\n",
      "Batch 900/1047\t Loss 0.000207\n",
      "Batch 1000/1047\t Loss 0.000209\n",
      "epoch #62, train loss: 0.0002, train acc: 0.1604\n",
      "sel_samples/total: 21453/134000\n",
      "epoch #62, val loss: 0.1429, val acc: 0.0590\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000190\n",
      "Batch 200/1047\t Loss 0.000183\n",
      "Batch 300/1047\t Loss 0.000174\n",
      "Batch 400/1047\t Loss 0.000176\n",
      "Batch 500/1047\t Loss 0.000185\n",
      "Batch 600/1047\t Loss 0.000189\n",
      "Batch 700/1047\t Loss 0.000189\n",
      "Batch 800/1047\t Loss 0.000194\n",
      "Batch 900/1047\t Loss 0.000192\n",
      "Batch 1000/1047\t Loss 0.000192\n",
      "epoch #63, train loss: 0.0002, train acc: 0.1605\n",
      "sel_samples/total: 21425/134000\n",
      "epoch #63, val loss: 0.1450, val acc: 0.0509\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000160\n",
      "Batch 200/1047\t Loss 0.000176\n",
      "Batch 300/1047\t Loss 0.000169\n",
      "Batch 400/1047\t Loss 0.000165\n",
      "Batch 500/1047\t Loss 0.000168\n",
      "Batch 600/1047\t Loss 0.000167\n",
      "Batch 700/1047\t Loss 0.000166\n",
      "Batch 800/1047\t Loss 0.000166\n",
      "Batch 900/1047\t Loss 0.000166\n",
      "Batch 1000/1047\t Loss 0.000169\n",
      "epoch #64, train loss: 0.0002, train acc: 0.1617\n",
      "sel_samples/total: 21536/134000\n",
      "epoch #64, val loss: 0.1304, val acc: 0.0587\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000139\n",
      "Batch 200/1047\t Loss 0.000133\n",
      "Batch 300/1047\t Loss 0.000133\n",
      "Batch 400/1047\t Loss 0.000133\n",
      "Batch 500/1047\t Loss 0.000139\n",
      "Batch 600/1047\t Loss 0.000140\n",
      "Batch 700/1047\t Loss 0.000144\n",
      "Batch 800/1047\t Loss 0.000144\n",
      "Batch 900/1047\t Loss 0.000148\n",
      "Batch 1000/1047\t Loss 0.000148\n",
      "epoch #65, train loss: 0.0001, train acc: 0.1638\n",
      "sel_samples/total: 21795/134000\n",
      "epoch #65, val loss: 0.1496, val acc: 0.0518\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000161\n",
      "Batch 200/1047\t Loss 0.000153\n",
      "Batch 300/1047\t Loss 0.000147\n",
      "Batch 400/1047\t Loss 0.000148\n",
      "Batch 500/1047\t Loss 0.000147\n",
      "Batch 600/1047\t Loss 0.000148\n",
      "Batch 700/1047\t Loss 0.000151\n",
      "Batch 800/1047\t Loss 0.000154\n",
      "Batch 900/1047\t Loss 0.000157\n",
      "Batch 1000/1047\t Loss 0.000161\n",
      "epoch #66, train loss: 0.0002, train acc: 0.1606\n",
      "sel_samples/total: 21330/134000\n",
      "epoch #66, val loss: 0.1245, val acc: 0.0596\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000146\n",
      "Batch 200/1047\t Loss 0.000128\n",
      "Batch 300/1047\t Loss 0.000127\n",
      "Batch 400/1047\t Loss 0.000131\n",
      "Batch 500/1047\t Loss 0.000137\n",
      "Batch 600/1047\t Loss 0.000139\n",
      "Batch 700/1047\t Loss 0.000135\n",
      "Batch 800/1047\t Loss 0.000133\n",
      "Batch 900/1047\t Loss 0.000132\n",
      "Batch 1000/1047\t Loss 0.000133\n",
      "epoch #67, train loss: 0.0001, train acc: 0.1661\n",
      "sel_samples/total: 22072/134000\n",
      "epoch #67, val loss: 0.1239, val acc: 0.0625\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000109\n",
      "Batch 200/1047\t Loss 0.000123\n",
      "Batch 300/1047\t Loss 0.000130\n",
      "Batch 400/1047\t Loss 0.000137\n",
      "Batch 500/1047\t Loss 0.000140\n",
      "Batch 600/1047\t Loss 0.000139\n",
      "Batch 700/1047\t Loss 0.000137\n",
      "Batch 800/1047\t Loss 0.000138\n",
      "Batch 900/1047\t Loss 0.000140\n",
      "Batch 1000/1047\t Loss 0.000143\n",
      "epoch #68, train loss: 0.0001, train acc: 0.1637\n",
      "sel_samples/total: 21669/134000\n",
      "epoch #68, val loss: 0.1289, val acc: 0.0588\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000143\n",
      "Batch 200/1047\t Loss 0.000158\n",
      "Batch 300/1047\t Loss 0.000163\n",
      "Batch 400/1047\t Loss 0.000159\n",
      "Batch 500/1047\t Loss 0.000153\n",
      "Batch 600/1047\t Loss 0.000151\n",
      "Batch 700/1047\t Loss 0.000153\n",
      "Batch 800/1047\t Loss 0.000149\n",
      "Batch 900/1047\t Loss 0.000149\n",
      "Batch 1000/1047\t Loss 0.000147\n",
      "epoch #69, train loss: 0.0001, train acc: 0.1605\n",
      "sel_samples/total: 21194/134000\n",
      "epoch #69, val loss: 0.1269, val acc: 0.0576\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000144\n",
      "Batch 200/1047\t Loss 0.000133\n",
      "Batch 300/1047\t Loss 0.000128\n",
      "Batch 400/1047\t Loss 0.000122\n",
      "Batch 500/1047\t Loss 0.000119\n",
      "Batch 600/1047\t Loss 0.000123\n",
      "Batch 700/1047\t Loss 0.000124\n",
      "Batch 800/1047\t Loss 0.000124\n",
      "Batch 900/1047\t Loss 0.000125\n",
      "Batch 1000/1047\t Loss 0.000128\n",
      "epoch #70, train loss: 0.0001, train acc: 0.1633\n",
      "sel_samples/total: 21554/134000\n",
      "epoch #70, val loss: 0.1251, val acc: 0.0615\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000108\n",
      "Batch 200/1047\t Loss 0.000107\n",
      "Batch 300/1047\t Loss 0.000111\n",
      "Batch 400/1047\t Loss 0.000112\n",
      "Batch 500/1047\t Loss 0.000112\n",
      "Batch 600/1047\t Loss 0.000111\n",
      "Batch 700/1047\t Loss 0.000110\n",
      "Batch 800/1047\t Loss 0.000114\n",
      "Batch 900/1047\t Loss 0.000117\n",
      "Batch 1000/1047\t Loss 0.000115\n",
      "epoch #71, train loss: 0.0001, train acc: 0.1657\n",
      "sel_samples/total: 21895/134000\n",
      "epoch #71, val loss: 0.1154, val acc: 0.0656\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000095\n",
      "Batch 200/1047\t Loss 0.000105\n",
      "Batch 300/1047\t Loss 0.000106\n",
      "Batch 400/1047\t Loss 0.000106\n",
      "Batch 500/1047\t Loss 0.000107\n",
      "Batch 600/1047\t Loss 0.000108\n",
      "Batch 700/1047\t Loss 0.000110\n",
      "Batch 800/1047\t Loss 0.000109\n",
      "Batch 900/1047\t Loss 0.000112\n",
      "Batch 1000/1047\t Loss 0.000113\n",
      "epoch #72, train loss: 0.0001, train acc: 0.1667\n",
      "sel_samples/total: 21954/134000\n",
      "epoch #72, val loss: 0.1170, val acc: 0.0579\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000127\n",
      "Batch 200/1047\t Loss 0.000113\n",
      "Batch 300/1047\t Loss 0.000106\n",
      "Batch 400/1047\t Loss 0.000105\n",
      "Batch 500/1047\t Loss 0.000100\n",
      "Batch 600/1047\t Loss 0.000100\n",
      "Batch 700/1047\t Loss 0.000104\n",
      "Batch 800/1047\t Loss 0.000104\n",
      "Batch 900/1047\t Loss 0.000107\n",
      "Batch 1000/1047\t Loss 0.000107\n",
      "epoch #73, train loss: 0.0001, train acc: 0.1670\n",
      "sel_samples/total: 21958/134000\n",
      "epoch #73, val loss: 0.1258, val acc: 0.0583\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000109\n",
      "Batch 200/1047\t Loss 0.000109\n",
      "Batch 300/1047\t Loss 0.000108\n",
      "Batch 400/1047\t Loss 0.000105\n",
      "Batch 500/1047\t Loss 0.000104\n",
      "Batch 600/1047\t Loss 0.000102\n",
      "Batch 700/1047\t Loss 0.000103\n",
      "Batch 800/1047\t Loss 0.000105\n",
      "Batch 900/1047\t Loss 0.000105\n",
      "Batch 1000/1047\t Loss 0.000106\n",
      "epoch #74, train loss: 0.0001, train acc: 0.1670\n",
      "sel_samples/total: 21923/134000\n",
      "epoch #74, val loss: 0.1190, val acc: 0.0603\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000119\n",
      "Batch 200/1047\t Loss 0.000115\n",
      "Batch 300/1047\t Loss 0.000108\n",
      "Batch 400/1047\t Loss 0.000098\n",
      "Batch 500/1047\t Loss 0.000093\n",
      "Batch 600/1047\t Loss 0.000095\n",
      "Batch 700/1047\t Loss 0.000095\n",
      "Batch 800/1047\t Loss 0.000096\n",
      "Batch 900/1047\t Loss 0.000095\n",
      "Batch 1000/1047\t Loss 0.000098\n",
      "epoch #75, train loss: 0.0001, train acc: 0.1661\n",
      "sel_samples/total: 21758/134000\n",
      "epoch #75, val loss: 0.1184, val acc: 0.0578\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000112\n",
      "Batch 200/1047\t Loss 0.000109\n",
      "Batch 300/1047\t Loss 0.000108\n",
      "Batch 400/1047\t Loss 0.000115\n",
      "Batch 500/1047\t Loss 0.000112\n",
      "Batch 600/1047\t Loss 0.000112\n",
      "Batch 700/1047\t Loss 0.000110\n",
      "Batch 800/1047\t Loss 0.000106\n",
      "Batch 900/1047\t Loss 0.000106\n",
      "Batch 1000/1047\t Loss 0.000107\n",
      "epoch #76, train loss: 0.0001, train acc: 0.1649\n",
      "sel_samples/total: 21461/134000\n",
      "epoch #76, val loss: 0.1225, val acc: 0.0576\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000103\n",
      "Batch 200/1047\t Loss 0.000100\n",
      "Batch 300/1047\t Loss 0.000098\n",
      "Batch 400/1047\t Loss 0.000094\n",
      "Batch 500/1047\t Loss 0.000094\n",
      "Batch 600/1047\t Loss 0.000092\n",
      "Batch 700/1047\t Loss 0.000094\n",
      "Batch 800/1047\t Loss 0.000094\n",
      "Batch 900/1047\t Loss 0.000094\n",
      "Batch 1000/1047\t Loss 0.000095\n",
      "epoch #77, train loss: 0.0001, train acc: 0.1671\n",
      "sel_samples/total: 21840/134000\n",
      "epoch #77, val loss: 0.1171, val acc: 0.0589\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000084\n",
      "Batch 200/1047\t Loss 0.000081\n",
      "Batch 300/1047\t Loss 0.000075\n",
      "Batch 400/1047\t Loss 0.000071\n",
      "Batch 500/1047\t Loss 0.000071\n",
      "Batch 600/1047\t Loss 0.000070\n",
      "Batch 700/1047\t Loss 0.000074\n",
      "Batch 800/1047\t Loss 0.000078\n",
      "Batch 900/1047\t Loss 0.000079\n",
      "Batch 1000/1047\t Loss 0.000080\n",
      "epoch #78, train loss: 0.0001, train acc: 0.1692\n",
      "sel_samples/total: 22141/134000\n",
      "epoch #78, val loss: 0.1069, val acc: 0.0610\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000085\n",
      "Batch 200/1047\t Loss 0.000079\n",
      "Batch 300/1047\t Loss 0.000079\n",
      "Batch 400/1047\t Loss 0.000075\n",
      "Batch 500/1047\t Loss 0.000073\n",
      "Batch 600/1047\t Loss 0.000075\n",
      "Batch 700/1047\t Loss 0.000074\n",
      "Batch 800/1047\t Loss 0.000071\n",
      "Batch 900/1047\t Loss 0.000071\n",
      "Batch 1000/1047\t Loss 0.000072\n",
      "epoch #79, train loss: 0.0001, train acc: 0.1702\n",
      "sel_samples/total: 22242/134000\n",
      "epoch #79, val loss: 0.1052, val acc: 0.0667\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000081\n",
      "Batch 200/1047\t Loss 0.000083\n",
      "Batch 300/1047\t Loss 0.000080\n",
      "Batch 400/1047\t Loss 0.000076\n",
      "Batch 500/1047\t Loss 0.000076\n",
      "Batch 600/1047\t Loss 0.000076\n",
      "Batch 700/1047\t Loss 0.000074\n",
      "Batch 800/1047\t Loss 0.000077\n",
      "Batch 900/1047\t Loss 0.000078\n",
      "Batch 1000/1047\t Loss 0.000078\n",
      "epoch #80, train loss: 0.0001, train acc: 0.1699\n",
      "sel_samples/total: 22147/134000\n",
      "epoch #80, val loss: 0.1099, val acc: 0.0625\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000070\n",
      "Batch 200/1047\t Loss 0.000083\n",
      "Batch 300/1047\t Loss 0.000084\n",
      "Batch 400/1047\t Loss 0.000081\n",
      "Batch 500/1047\t Loss 0.000078\n",
      "Batch 600/1047\t Loss 0.000079\n",
      "Batch 700/1047\t Loss 0.000078\n",
      "Batch 800/1047\t Loss 0.000080\n",
      "Batch 900/1047\t Loss 0.000081\n",
      "Batch 1000/1047\t Loss 0.000081\n",
      "epoch #81, train loss: 0.0001, train acc: 0.1695\n",
      "sel_samples/total: 22011/134000\n",
      "epoch #81, val loss: 0.1119, val acc: 0.0597\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000066\n",
      "Batch 200/1047\t Loss 0.000058\n",
      "Batch 300/1047\t Loss 0.000056\n",
      "Batch 400/1047\t Loss 0.000058\n",
      "Batch 500/1047\t Loss 0.000061\n",
      "Batch 600/1047\t Loss 0.000062\n",
      "Batch 700/1047\t Loss 0.000061\n",
      "Batch 800/1047\t Loss 0.000061\n",
      "Batch 900/1047\t Loss 0.000061\n",
      "Batch 1000/1047\t Loss 0.000061\n",
      "epoch #82, train loss: 0.0001, train acc: 0.1724\n",
      "sel_samples/total: 22442/134000\n",
      "epoch #82, val loss: 0.1005, val acc: 0.0669\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000049\n",
      "Batch 200/1047\t Loss 0.000054\n",
      "Batch 300/1047\t Loss 0.000055\n",
      "Batch 400/1047\t Loss 0.000056\n",
      "Batch 500/1047\t Loss 0.000056\n",
      "Batch 600/1047\t Loss 0.000056\n",
      "Batch 700/1047\t Loss 0.000057\n",
      "Batch 800/1047\t Loss 0.000059\n",
      "Batch 900/1047\t Loss 0.000060\n",
      "Batch 1000/1047\t Loss 0.000062\n",
      "epoch #83, train loss: 0.0001, train acc: 0.1724\n",
      "sel_samples/total: 22428/134000\n",
      "epoch #83, val loss: 0.0995, val acc: 0.0629\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000084\n",
      "Batch 200/1047\t Loss 0.000076\n",
      "Batch 300/1047\t Loss 0.000071\n",
      "Batch 400/1047\t Loss 0.000066\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000068\n",
      "Batch 700/1047\t Loss 0.000069\n",
      "Batch 800/1047\t Loss 0.000070\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000070\n",
      "epoch #84, train loss: 0.0001, train acc: 0.1699\n",
      "sel_samples/total: 21963/134000\n",
      "epoch #84, val loss: 0.0981, val acc: 0.0664\n",
      "------------------------------\n",
      "curr_lr: 0.1\n",
      "Batch 100/1047\t Loss 0.000067\n",
      "Batch 200/1047\t Loss 0.000064\n",
      "Batch 300/1047\t Loss 0.000068\n",
      "Batch 400/1047\t Loss 0.000067\n",
      "Batch 500/1047\t Loss 0.000066\n",
      "Batch 600/1047\t Loss 0.000062\n",
      "Batch 700/1047\t Loss 0.000065\n",
      "Batch 800/1047\t Loss 0.000067\n",
      "Batch 900/1047\t Loss 0.000069\n",
      "Batch 1000/1047\t Loss 0.000068\n",
      "epoch #85, train loss: 0.0001, train acc: 0.1706\n",
      "sel_samples/total: 22032/134000\n",
      "epoch #85, val loss: 0.1029, val acc: 0.0632\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000051\n",
      "Batch 200/1047\t Loss 0.000045\n",
      "Batch 300/1047\t Loss 0.000043\n",
      "Batch 400/1047\t Loss 0.000040\n",
      "Batch 500/1047\t Loss 0.000038\n",
      "Batch 600/1047\t Loss 0.000037\n",
      "Batch 700/1047\t Loss 0.000036\n",
      "Batch 800/1047\t Loss 0.000035\n",
      "Batch 900/1047\t Loss 0.000034\n",
      "Batch 1000/1047\t Loss 0.000033\n",
      "epoch #86, train loss: 0.0000, train acc: 0.1789\n",
      "sel_samples/total: 23536/134000\n",
      "epoch #86, val loss: 0.0924, val acc: 0.0797\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000012\n",
      "Batch 200/1047\t Loss 0.000014\n",
      "Batch 300/1047\t Loss 0.000016\n",
      "Batch 400/1047\t Loss 0.000017\n",
      "Batch 500/1047\t Loss 0.000017\n",
      "Batch 600/1047\t Loss 0.000017\n",
      "Batch 700/1047\t Loss 0.000017\n",
      "Batch 800/1047\t Loss 0.000016\n",
      "Batch 900/1047\t Loss 0.000016\n",
      "Batch 1000/1047\t Loss 0.000017\n",
      "epoch #87, train loss: 0.0000, train acc: 0.1818\n",
      "sel_samples/total: 24114/134000\n",
      "epoch #87, val loss: 0.0904, val acc: 0.0818\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000010\n",
      "Batch 200/1047\t Loss 0.000009\n",
      "Batch 300/1047\t Loss 0.000010\n",
      "Batch 400/1047\t Loss 0.000011\n",
      "Batch 500/1047\t Loss 0.000011\n",
      "Batch 600/1047\t Loss 0.000011\n",
      "Batch 700/1047\t Loss 0.000011\n",
      "Batch 800/1047\t Loss 0.000011\n",
      "Batch 900/1047\t Loss 0.000011\n",
      "Batch 1000/1047\t Loss 0.000011\n",
      "epoch #88, train loss: 0.0000, train acc: 0.1825\n",
      "sel_samples/total: 24256/134000\n",
      "epoch #88, val loss: 0.0870, val acc: 0.0821\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000008\n",
      "Batch 200/1047\t Loss 0.000010\n",
      "Batch 300/1047\t Loss 0.000009\n",
      "Batch 400/1047\t Loss 0.000009\n",
      "Batch 500/1047\t Loss 0.000010\n",
      "Batch 600/1047\t Loss 0.000009\n",
      "Batch 700/1047\t Loss 0.000010\n",
      "Batch 800/1047\t Loss 0.000010\n",
      "Batch 900/1047\t Loss 0.000010\n",
      "Batch 1000/1047\t Loss 0.000010\n",
      "epoch #89, train loss: 0.0000, train acc: 0.1829\n",
      "sel_samples/total: 24347/134000\n",
      "epoch #89, val loss: 0.0850, val acc: 0.0826\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000007\n",
      "Batch 300/1047\t Loss 0.000007\n",
      "Batch 400/1047\t Loss 0.000007\n",
      "Batch 500/1047\t Loss 0.000007\n",
      "Batch 600/1047\t Loss 0.000007\n",
      "Batch 700/1047\t Loss 0.000007\n",
      "Batch 800/1047\t Loss 0.000008\n",
      "Batch 900/1047\t Loss 0.000007\n",
      "Batch 1000/1047\t Loss 0.000008\n",
      "epoch #90, train loss: 0.0000, train acc: 0.1831\n",
      "sel_samples/total: 24398/134000\n",
      "epoch #90, val loss: 0.0827, val acc: 0.0835\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000007\n",
      "Batch 400/1047\t Loss 0.000007\n",
      "Batch 500/1047\t Loss 0.000007\n",
      "Batch 600/1047\t Loss 0.000007\n",
      "Batch 700/1047\t Loss 0.000007\n",
      "Batch 800/1047\t Loss 0.000007\n",
      "Batch 900/1047\t Loss 0.000007\n",
      "Batch 1000/1047\t Loss 0.000007\n",
      "epoch #91, train loss: 0.0000, train acc: 0.1834\n",
      "sel_samples/total: 24439/134000\n",
      "epoch #91, val loss: 0.0811, val acc: 0.0835\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000007\n",
      "Batch 600/1047\t Loss 0.000007\n",
      "Batch 700/1047\t Loss 0.000007\n",
      "Batch 800/1047\t Loss 0.000007\n",
      "Batch 900/1047\t Loss 0.000007\n",
      "Batch 1000/1047\t Loss 0.000007\n",
      "epoch #92, train loss: 0.0000, train acc: 0.1835\n",
      "sel_samples/total: 24476/134000\n",
      "epoch #92, val loss: 0.0788, val acc: 0.0835\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000008\n",
      "Batch 300/1047\t Loss 0.000007\n",
      "Batch 400/1047\t Loss 0.000007\n",
      "Batch 500/1047\t Loss 0.000007\n",
      "Batch 600/1047\t Loss 0.000007\n",
      "Batch 700/1047\t Loss 0.000007\n",
      "Batch 800/1047\t Loss 0.000007\n",
      "Batch 900/1047\t Loss 0.000007\n",
      "Batch 1000/1047\t Loss 0.000007\n",
      "epoch #93, train loss: 0.0000, train acc: 0.1836\n",
      "sel_samples/total: 24506/134000\n",
      "epoch #93, val loss: 0.0763, val acc: 0.0839\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #94, train loss: 0.0000, train acc: 0.1837\n",
      "sel_samples/total: 24504/134000\n",
      "epoch #94, val loss: 0.0744, val acc: 0.0836\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #95, train loss: 0.0000, train acc: 0.1837\n",
      "sel_samples/total: 24516/134000\n",
      "epoch #95, val loss: 0.0730, val acc: 0.0839\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000007\n",
      "Batch 900/1047\t Loss 0.000007\n",
      "Batch 1000/1047\t Loss 0.000007\n",
      "epoch #96, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24532/134000\n",
      "epoch #96, val loss: 0.0713, val acc: 0.0843\n",
      "------------------------------\n",
      "curr_lr: 0.020000000000000004\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #97, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24523/134000\n",
      "epoch #97, val loss: 0.0701, val acc: 0.0843\n",
      "------------------------------\n",
      "curr_lr: 0.004000000000000001\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #98, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24527/134000\n",
      "epoch #98, val loss: 0.0697, val acc: 0.0846\n",
      "------------------------------\n",
      "curr_lr: 0.004000000000000001\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #99, train loss: 0.0000, train acc: 0.1837\n",
      "sel_samples/total: 24532/134000\n",
      "epoch #99, val loss: 0.0697, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 0.004000000000000001\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #100, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24530/134000\n",
      "epoch #100, val loss: 0.0693, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 0.004000000000000001\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #101, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24533/134000\n",
      "epoch #101, val loss: 0.0691, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 0.004000000000000001\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #102, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24536/134000\n",
      "epoch #102, val loss: 0.0683, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #103, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24533/134000\n",
      "epoch #103, val loss: 0.0685, val acc: 0.0846\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000007\n",
      "Batch 300/1047\t Loss 0.000007\n",
      "Batch 400/1047\t Loss 0.000007\n",
      "Batch 500/1047\t Loss 0.000007\n",
      "Batch 600/1047\t Loss 0.000007\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #104, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24534/134000\n",
      "epoch #104, val loss: 0.0681, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000007\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #105, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24527/134000\n",
      "epoch #105, val loss: 0.0687, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #106, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24536/134000\n",
      "epoch #106, val loss: 0.0685, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000008\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #107, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24526/134000\n",
      "epoch #107, val loss: 0.0686, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #108, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24524/134000\n",
      "epoch #108, val loss: 0.0678, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 0.0008000000000000003\n",
      "Batch 100/1047\t Loss 0.000007\n",
      "Batch 200/1047\t Loss 0.000007\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #109, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24527/134000\n",
      "epoch #109, val loss: 0.0681, val acc: 0.0846\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #110, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24522/134000\n",
      "epoch #110, val loss: 0.0686, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #111, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24519/134000\n",
      "epoch #111, val loss: 0.0685, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #112, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24517/134000\n",
      "epoch #112, val loss: 0.0681, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #113, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24514/134000\n",
      "epoch #113, val loss: 0.0687, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #114, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24512/134000\n",
      "epoch #114, val loss: 0.0684, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #115, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24515/134000\n",
      "epoch #115, val loss: 0.0682, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #116, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24511/134000\n",
      "epoch #116, val loss: 0.0682, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 0.00016000000000000007\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #117, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24501/134000\n",
      "epoch #117, val loss: 0.0686, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000006\n",
      "Batch 400/1047\t Loss 0.000006\n",
      "Batch 500/1047\t Loss 0.000006\n",
      "Batch 600/1047\t Loss 0.000006\n",
      "Batch 700/1047\t Loss 0.000006\n",
      "Batch 800/1047\t Loss 0.000006\n",
      "Batch 900/1047\t Loss 0.000006\n",
      "Batch 1000/1047\t Loss 0.000006\n",
      "epoch #118, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24507/134000\n",
      "epoch #118, val loss: 0.0678, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #119, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24488/134000\n",
      "epoch #119, val loss: 0.0680, val acc: 0.0854\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #120, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24487/134000\n",
      "epoch #120, val loss: 0.0686, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #121, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24486/134000\n",
      "epoch #121, val loss: 0.0678, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #122, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24491/134000\n",
      "epoch #122, val loss: 0.0681, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #123, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24481/134000\n",
      "epoch #123, val loss: 0.0681, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #124, train loss: 0.0000, train acc: 0.1840\n",
      "sel_samples/total: 24457/134000\n",
      "epoch #124, val loss: 0.0684, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #125, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24477/134000\n",
      "epoch #125, val loss: 0.0682, val acc: 0.0854\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000006\n",
      "Batch 200/1047\t Loss 0.000006\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #126, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24467/134000\n",
      "epoch #126, val loss: 0.0681, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 3.200000000000001e-05\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #127, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24459/134000\n",
      "epoch #127, val loss: 0.0678, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #128, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24455/134000\n",
      "epoch #128, val loss: 0.0686, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #129, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24437/134000\n",
      "epoch #129, val loss: 0.0681, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #130, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24432/134000\n",
      "epoch #130, val loss: 0.0685, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #131, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24424/134000\n",
      "epoch #131, val loss: 0.0679, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000005\n",
      "Batch 300/1047\t Loss 0.000005\n",
      "Batch 400/1047\t Loss 0.000005\n",
      "Batch 500/1047\t Loss 0.000005\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000005\n",
      "Batch 800/1047\t Loss 0.000005\n",
      "Batch 900/1047\t Loss 0.000005\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #132, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24437/134000\n",
      "epoch #132, val loss: 0.0684, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000005\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #133, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24381/134000\n",
      "epoch #133, val loss: 0.0682, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000005\n",
      "epoch #134, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24412/134000\n",
      "epoch #134, val loss: 0.0687, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000005\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #135, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24382/134000\n",
      "epoch #135, val loss: 0.0681, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #136, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24376/134000\n",
      "epoch #136, val loss: 0.0681, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #137, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24355/134000\n",
      "epoch #137, val loss: 0.0688, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #138, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24339/134000\n",
      "epoch #138, val loss: 0.0683, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #139, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24354/134000\n",
      "epoch #139, val loss: 0.0682, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #140, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24317/134000\n",
      "epoch #140, val loss: 0.0682, val acc: 0.0847\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #141, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24319/134000\n",
      "epoch #141, val loss: 0.0680, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #142, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24299/134000\n",
      "epoch #142, val loss: 0.0682, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #143, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24278/134000\n",
      "epoch #143, val loss: 0.0685, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #144, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24266/134000\n",
      "epoch #144, val loss: 0.0682, val acc: 0.0846\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #145, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24223/134000\n",
      "epoch #145, val loss: 0.0684, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000004\n",
      "Batch 300/1047\t Loss 0.000004\n",
      "Batch 400/1047\t Loss 0.000004\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000004\n",
      "Batch 800/1047\t Loss 0.000004\n",
      "Batch 900/1047\t Loss 0.000004\n",
      "Batch 1000/1047\t Loss 0.000004\n",
      "epoch #146, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24205/134000\n",
      "epoch #146, val loss: 0.0684, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #147, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24195/134000\n",
      "epoch #147, val loss: 0.0680, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000004\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #148, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24193/134000\n",
      "epoch #148, val loss: 0.0678, val acc: 0.0854\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000004\n",
      "Batch 600/1047\t Loss 0.000004\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #149, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24146/134000\n",
      "epoch #149, val loss: 0.0683, val acc: 0.0847\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #150, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24107/134000\n",
      "epoch #150, val loss: 0.0678, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #151, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24121/134000\n",
      "epoch #151, val loss: 0.0680, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #152, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 24068/134000\n",
      "epoch #152, val loss: 0.0680, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #153, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24037/134000\n",
      "epoch #153, val loss: 0.0684, val acc: 0.0847\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #154, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 24023/134000\n",
      "epoch #154, val loss: 0.0685, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #155, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23982/134000\n",
      "epoch #155, val loss: 0.0681, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #156, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 23966/134000\n",
      "epoch #156, val loss: 0.0679, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #157, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23932/134000\n",
      "epoch #157, val loss: 0.0677, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #158, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23898/134000\n",
      "epoch #158, val loss: 0.0682, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000003\n",
      "Batch 200/1047\t Loss 0.000003\n",
      "Batch 300/1047\t Loss 0.000003\n",
      "Batch 400/1047\t Loss 0.000003\n",
      "Batch 500/1047\t Loss 0.000003\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #159, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23854/134000\n",
      "epoch #159, val loss: 0.0684, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000003\n",
      "Batch 700/1047\t Loss 0.000003\n",
      "Batch 800/1047\t Loss 0.000003\n",
      "Batch 900/1047\t Loss 0.000003\n",
      "Batch 1000/1047\t Loss 0.000003\n",
      "epoch #160, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23817/134000\n",
      "epoch #160, val loss: 0.0681, val acc: 0.0853\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #161, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23812/134000\n",
      "epoch #161, val loss: 0.0682, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #162, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23742/134000\n",
      "epoch #162, val loss: 0.0681, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #163, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23690/134000\n",
      "epoch #163, val loss: 0.0680, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #164, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 23661/134000\n",
      "epoch #164, val loss: 0.0684, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #165, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 23644/134000\n",
      "epoch #165, val loss: 0.0683, val acc: 0.0853\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #166, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23600/134000\n",
      "epoch #166, val loss: 0.0677, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #167, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23541/134000\n",
      "epoch #167, val loss: 0.0682, val acc: 0.0846\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #168, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23517/134000\n",
      "epoch #168, val loss: 0.0685, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #169, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23481/134000\n",
      "epoch #169, val loss: 0.0685, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #170, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23350/134000\n",
      "epoch #170, val loss: 0.0682, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #171, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23388/134000\n",
      "epoch #171, val loss: 0.0685, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #172, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 23309/134000\n",
      "epoch #172, val loss: 0.0683, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #173, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 23275/134000\n",
      "epoch #173, val loss: 0.0682, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #174, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23285/134000\n",
      "epoch #174, val loss: 0.0685, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #175, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 23136/134000\n",
      "epoch #175, val loss: 0.0678, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #176, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23082/134000\n",
      "epoch #176, val loss: 0.0683, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000002\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000002\n",
      "Batch 400/1047\t Loss 0.000002\n",
      "Batch 500/1047\t Loss 0.000002\n",
      "Batch 600/1047\t Loss 0.000002\n",
      "Batch 700/1047\t Loss 0.000002\n",
      "Batch 800/1047\t Loss 0.000002\n",
      "Batch 900/1047\t Loss 0.000002\n",
      "Batch 1000/1047\t Loss 0.000002\n",
      "epoch #177, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 23027/134000\n",
      "epoch #177, val loss: 0.0681, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #178, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22954/134000\n",
      "epoch #178, val loss: 0.0686, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000002\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #179, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22946/134000\n",
      "epoch #179, val loss: 0.0681, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #180, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22840/134000\n",
      "epoch #180, val loss: 0.0679, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #181, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22840/134000\n",
      "epoch #181, val loss: 0.0679, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #182, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22711/134000\n",
      "epoch #182, val loss: 0.0684, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #183, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 22635/134000\n",
      "epoch #183, val loss: 0.0683, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #184, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22669/134000\n",
      "epoch #184, val loss: 0.0685, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #185, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 22546/134000\n",
      "epoch #185, val loss: 0.0683, val acc: 0.0854\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #186, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22460/134000\n",
      "epoch #186, val loss: 0.0684, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #187, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22436/134000\n",
      "epoch #187, val loss: 0.0683, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #188, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22320/134000\n",
      "epoch #188, val loss: 0.0682, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #189, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 22204/134000\n",
      "epoch #189, val loss: 0.0683, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #190, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22199/134000\n",
      "epoch #190, val loss: 0.0686, val acc: 0.0851\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #191, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 22043/134000\n",
      "epoch #191, val loss: 0.0681, val acc: 0.0853\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #192, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 21978/134000\n",
      "epoch #192, val loss: 0.0685, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #193, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 21940/134000\n",
      "epoch #193, val loss: 0.0683, val acc: 0.0847\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #194, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 21813/134000\n",
      "epoch #194, val loss: 0.0685, val acc: 0.0849\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #195, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 21773/134000\n",
      "epoch #195, val loss: 0.0682, val acc: 0.0848\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #196, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 21727/134000\n",
      "epoch #196, val loss: 0.0681, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #197, train loss: 0.0000, train acc: 0.1839\n",
      "sel_samples/total: 21607/134000\n",
      "epoch #197, val loss: 0.0681, val acc: 0.0850\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #198, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 21594/134000\n",
      "epoch #198, val loss: 0.0683, val acc: 0.0852\n",
      "------------------------------\n",
      "curr_lr: 6.400000000000003e-06\n",
      "Batch 100/1047\t Loss 0.000001\n",
      "Batch 200/1047\t Loss 0.000001\n",
      "Batch 300/1047\t Loss 0.000001\n",
      "Batch 400/1047\t Loss 0.000001\n",
      "Batch 500/1047\t Loss 0.000001\n",
      "Batch 600/1047\t Loss 0.000001\n",
      "Batch 700/1047\t Loss 0.000001\n",
      "Batch 800/1047\t Loss 0.000001\n",
      "Batch 900/1047\t Loss 0.000001\n",
      "Batch 1000/1047\t Loss 0.000001\n",
      "epoch #199, train loss: 0.0000, train acc: 0.1838\n",
      "sel_samples/total: 21476/134000\n",
      "epoch #199, val loss: 0.0680, val acc: 0.0850\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, MultiStepLR\n",
    "from sklearn.metrics import roc_curve\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "plateau_scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=2)\n",
    "step_scheduler = MultiStepLR(optimizer, [30], 0.1)\n",
    "\n",
    "k = 7.2\n",
    "\n",
    "for epoch_idx in range(16, config['n_epochs']):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    \n",
    "    n_sel_samples = 0\n",
    "# =============== train code #===============\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        ### spl masking\n",
    "        spl_mask = loss.lt(k)\n",
    "        masked_loss = torch.masked_select(loss, spl_mask).mean()\n",
    "        n_sel_samples += spl_mask.sum().item()\n",
    "        masked_loss.backward()\n",
    "        optimizer.step()\n",
    "                        \n",
    "        loss_sum += masked_loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "        if (batch_idx+1) % 100 == 0:\n",
    "            print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "                  .format(batch_idx+1, len(dev_train_dataloader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "    plateau_scheduler.step(train_loss)\n",
    "    \n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))\n",
    "    writer.add_scalar(\"train/loss\", train_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"train/acc\", train_acc, epoch_idx+1)\n",
    "    \n",
    "    k = k/1.05\n",
    "    print(\"sel_samples/total: {}/{}\".format(n_sel_samples, total))\n",
    "\n",
    "#=============== dev_val code #===============\n",
    "    model.eval()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss_sum += loss.mean().item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    val_loss = loss_sum / total\n",
    "    val_acc = n_corrects / total\n",
    "    \n",
    "    \n",
    "    print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))\n",
    "    writer.add_scalar(\"val/loss\", val_loss, epoch_idx+1)\n",
    "    writer.add_scalar(\"val/acc\", val_acc, epoch_idx+1)\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See, Fr features\n",
    "fr_feats = []\n",
    "model.eval()\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    fr_feat = model.fr_feat(X).cpu().detach()\n",
    "    fr_feats.append(fr_feat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Freeze Model & FineTurne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "freezed_params = []\n",
    "for param_name, param in model.named_parameters():\n",
    "    if param_name not in ['classifier.2.weight', 'classifier.2.bias']:\n",
    "        freezed_params.append(param)\n",
    "        \n",
    "for param in freezed_params:\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #0, train loss: 0.0219, train acc: 0.9818\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #1, train loss: 0.0217, train acc: 0.9818\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #2, train loss: 0.0214, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #3, train loss: 0.0210, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #4, train loss: 0.0206, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #5, train loss: 0.0202, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #6, train loss: 0.0198, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #7, train loss: 0.0194, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #8, train loss: 0.0190, train acc: 0.9848\n",
      "------------------------------\n",
      "curr_lr: 0.01\n",
      "epoch #9, train loss: 0.0186, train acc: 0.9848\n"
     ]
    }
   ],
   "source": [
    "# =============== fine_tune code #===============\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
    "for epoch_idx in range(0, 10):\n",
    "    print(\"-\"*30)\n",
    "    curr_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(\"curr_lr: {}\".format(curr_lr))\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    n_corrects = 0\n",
    "    total = 0\n",
    "    for batch_idx, (X, y) in enumerate(held_out_train_dataloader):\n",
    "        if not config['no_cuda']:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = criterion(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "    #         if (batch_idx+1) % 1000 == 0:\n",
    "    #             print(\"Batch {}/{}\\t Loss {:.6f}\" \\\n",
    "    #                   .format(batch_idx+1, len(si_loader), loss_sum / total))\n",
    "    train_loss = loss_sum / total\n",
    "    train_acc = n_corrects / total\n",
    "\n",
    "    print(\"epoch #{}, train loss: {:.4f}, train acc: {:.4f}\".format(epoch_idx, train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73438e0851f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#=============== dev_val code #===============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#=============== dev_val code #===============\n",
    "model.eval()\n",
    "loss_sum = 0\n",
    "n_corrects = 0\n",
    "total = 0\n",
    "for batch_idx, (X, y) in enumerate(dev_val_dataloader):\n",
    "    if not config['no_cuda']:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "    logit = model(X)\n",
    "    loss = criterion(logit, y)\n",
    "    loss_sum += loss.item()\n",
    "    n_corrects += logit.max(1)[1].eq(y).sum().item()\n",
    "    total += y.size(0)\n",
    "val_loss = loss_sum / total\n",
    "val_acc = n_corrects / total\n",
    "print(\"epoch #{}, val loss: {:.4f}, val acc: {:.4f}\".format(epoch_idx, val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SV Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "global_mean = si_embeds.mean(0)\n",
    "clf = LDA(solver='svd', n_components=200)\n",
    "clf.fit(si_embeds - global_mean, si_key_df.label)\n",
    "\n",
    "si_embeds = clf.transform(si_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "sv_embeds = clf.transform(sv_embeds - global_mean).astype(np.float32)\n",
    "\n",
    "si_dataset, embed_dim, n_labels = embedToDataset(si_embeds.reshape(-1,200), si_key_df)\n",
    "sv_dataset, _, _ = embedToDataset(sv_embeds, sv_key_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
